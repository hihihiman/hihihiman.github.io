[{"categories":["建站记录"],"content":"方案 省流版 主题：Hugo + FixIt 部署：Git + GitHub Workflows 图床：Picgo + Aliyun OSS 域名：GitHub Pages + Netlify + Aliyun 评论：Giscus 完整版 使用 Git 进行版本控制，并结合 GitHub Workflows 实现了自动化部署和贪食蛇效果。 使用了 Picgo 作为图床工具，并将图片上传至阿里云 OSS（对象存储服务）。 起初使用 GitHub Pages 作为博客的托管平台，并将自定义域名绑定到 GitHub Pages；后来使用 Netlify 作为 CDN，博客页面的平均加载时间缩短了约 50%，提高了用户的访问速度。 集成了 Giscus 评论系统，基于GitHub Issues，提供了一个简洁且易于使用的评论功能。 ","date":"2023-08-09","objectID":"/blog_beautify/:0:1","tags":["hugo"],"title":"FixIt主题美化","uri":"/blog_beautify/"},{"categories":["建站记录"],"content":"分享几个链接吧 针对 LoveIt 主题（FixIt 的前身）：https://lewky.cn/posts/hugo-3.html/ 基于 Hexos Fluid 的很好看的博客：https://nyimac.gitee.io/ 我的 FixIt 主题博客很多地方都借鉴了毕少侠：http://geekswg.js.cool/ Github 贪食蛇制作指南：https://cloud.tencent.com/developer/article/1935739 markdown 写作中的emoji表情包：https://hugoloveit.com/zh-cn/emoji-support/ 通过Netlify配置阿里云域名：https://blog.csdn.net/mqdxiaoxiao/article/details/96365253 通过 giscus 配置评论区：https://www.jianshu.com/p/a8a4dc99c0c7 最终还是取消了，为什么评论系统都需要科学上网？！ 贪食蛇效果 我肯定不是没怎么写代码，而是被贪食蛇吃掉了! ","date":"2023-08-09","objectID":"/blog_beautify/:0:2","tags":["hugo"],"title":"FixIt主题美化","uri":"/blog_beautify/"},{"categories":null,"content":"都将会's friends","date":"2023-08-10","objectID":"/friends/","tags":null,"title":"Index","uri":"/friends/"},{"categories":null,"content":"Base info - nickname: Lruihao avatar: https://lruihao.cn/images/avatar.jpg url: https://lruihao.cn description: Lruihao's Note ","date":"2023-08-10","objectID":"/friends/:1:0","tags":null,"title":"Index","uri":"/friends/"},{"categories":null,"content":"Friendly Reminder Notice If you want to exchange link, please leave a comment in the above format. (personal non-commercial blogs / websites only)  Website failure, stop maintenance and improper content may be unlinked! Those websites that do not respect other people’s labor achievements, reprint without source, or malicious acts, please do not come to exchange. ","date":"2023-08-10","objectID":"/friends/:2:0","tags":null,"title":"Index","uri":"/friends/"},{"categories":["interview"],"content":"红黑树 什么是红黑树？ 自平衡的二叉查找树，在进行插入和删除等可能会破坏树的平衡的操作时，需要重新自处理达到平衡状态。 根和叶子节点是黑色的，插入的节点是红色的 叶子节点是空节点。 每个红色节点必有黑色子节点。 任意一结点到每个叶子结点的路径都包含数量相同的黑结点。 通过左旋、右旋和变色来实现自平衡： 左旋只影响旋转结点和其右子树的结构，把右子树的结点往左子树挪了。右旋只影响旋转结点和其左子树的结构，把左子树的结点往右子树挪了。 红黑树有什么好处？ 红黑树是效率相对较高的当我们插入和删除数据相对频繁的时候 查询性能略微逊色于AVL树 红黑树是自我平衡的所有操作的复杂度最多是O(logn) 不管怎么变化，只有红黑两个常数，任何不平衡都会在三次旋转之内解决 红黑树场景 TreeSet、TreeMap、HashMap ","date":"2023-08-09","objectID":"/ds/:0:1","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"比较不同的树 二叉查找树(BST) ：解决了排序的基本问题，但是由于无法保证平衡，可能退化为链表； 平衡二叉树(AVL) ：通过旋转解决了平衡的问题，但是旋转操作效率太低； 红黑树 ：通过舍弃严格的平衡和引入红黑节点，解决了 AVL 旋转效率过低的问题，但是在磁盘等场景下，树仍然太高，IO 次数太多； B 树 ：通过将二叉树改为多路平衡查找树，解决了树过高的问题； B+树 ：在 B 树的基础上 将非叶节点改造为不存储数据的纯索引节点，适合放入内存中； 进一步降低了树的高度，查询性能好； 叶节点使用指针连接成链表，范围查询更加高效。 跳表：用于 Redis 的 zset，解决了链表查询效率低下的问题。 ","date":"2023-08-09","objectID":"/ds/:0:2","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"Queue 有哪些？ BlockingQueue：提供了线程安全的队列访问方式 PriorityQueue：优先级队列是一个可以排序的队列 Deque：一个线性 collection，支持在两端插入和移除元素。名称 deque 是“double ended queue（双端队列）”的缩写，通常读为“deck”。 ","date":"2023-08-09","objectID":"/ds/:0:3","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"Set 有哪些结构？ HashSet：哈希表，无序 LinkedHashSet：底层数据结构是双向链表和哈希表，有序。由链表保证元素有序，由哈希表保证元素唯一。 TreeSet：底层数据结构是红黑树，内部实现排序，也可以自定义排序规则。 ","date":"2023-08-09","objectID":"/ds/:0:4","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"🌟ArrayList 源码分析 默认大小为 10（初始为 0，第一次添加变为 10） 每次扩容 1.5 倍：0 -\u003e 10 -\u003e 15 -\u003e 22 -\u003e 33 -\u003e 49 … 数组 -\u003e list：Arrays.asList() 是浅拷贝，修改数组会影响 list，因为指向同一个内存地址 list -\u003e 数组：list.toArray() 是深拷贝，调用 Arrays.copyOf(data,size) ArrayList 是动态数组，LinkedList 是双向链表，都不是线程安全的，除非 Collections.synchronizedList() ","date":"2023-08-09","objectID":"/ds/:0:5","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"🌟HashMap 源码分析 JDK8 起， 链表 -\u003e 红黑树：链表长度 \u003e= 8 \u0026\u0026 数组长度 \u003e= 64 时 红黑树 -\u003e 链表：resize（）扩容时，树节点\u003c=6 put 方法： table 为空或 ++size \u003e *threshold(默认 12 = 数组长度（默认 16） 0.75) **时触发扩容机制 resize() 每次扩容 «1 （即*2） jdk8 之前头插法会导致死循环，jdk8 采用尾插法，避免扩容 resize() 迁移时的死循环 hash(key) 得到数组索引 扰动算法 (hash^hash»\u003e16)异或操作使 hash 值更均匀，减少 hash 冲突 hash\u0026(n-1), n 为 2n ，等价于 hash%n ，位运算性能更好 key 存在则修改，不存在则添加 ","date":"2023-08-09","objectID":"/ds/:0:6","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"ConcurrentHashMap 源码分析 锁分段技术？ 原理：把数据分成很多段储存，每一段都对应一把锁，读取数据不加锁，写操作时能够将锁的粒度保持尽量的小，不用对整个ConcurrentHashMap加锁。 JDK7：Segment + HashEntry 来保证线程安全 【不可扩容】 Segment extends ReentrantLock（继承可重入锁） HashEntry则用于存储键值对数据 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组中的数据进行修改时，必须先获得它对应的Segment锁 JDK8：CAS + synchronized 来保证线程安全，减少了锁的粒度 数组+链表：类似 HashMap，长度\u003e8 变为红黑树【处理哈希冲突】 transient volatile int count：volatile的语义可以保证写操作在读操作之前，也就保证了写操作对后续的读操作都是可见的，这样后面get的后续操作就可以拿到完整的元素内容 对每个数组元素Node加锁，锁的粒度更细，并发度更高（只要 hash 不冲突就无并发问题）。 【get】为什么读操作不加锁？ 因为Node的元素和指针是用volatile修饰的，在多线程环境下线程A修改结点的val或者新增节点的时候是对线程B可见的。此外，volatile修饰数组对get操作没有效果，但可以在扩容的时候对其他线程具有可见性。 volatile关键字：通过禁止指令重排，保证可见性、有序性。但不保证原子性。使用volatile关键字会强制将修改的值立即写入主存，并导致缓存变量的缓存行无效。 但是，当节点是红黑树的时候，如果树正在变色旋转并且错查询的值不是红黑树的头节点，会加读写锁。 【put】如何保证插入数据线程安全？ 首先判空，非空情况下使用**扰动函数（高16位与低16位做异或，再和Hash_bit做与运算）**计算key的hash值， 初始化数组采用懒汉式（第一次put时才初始化），线程安全通过一个volatile修饰的变量sizeCtl结合CAS来实现 当寻址后的数组位置没有被占用时，直接基于主内存地址，通过CAS将创建的Node节点插入到当前位置 当寻址到的位置正在进行扩容/迁移操作时（hash=MOVED=-1），执行helpTransfer来协助其他线程进行扩容/迁移 当 sizeCtl 为负值(-(1+n))时，表示有 n 个线程正在扩容 迁移：先计算当前线程需要转移对节点范围，再进行数据迁移 当出现hash冲突时，对单个数组元素加 synchronized 锁，确保线程安全地将节点插入到链表或红黑树（链表长\u003e8时调用treeifyBin(tab,i）转红黑树)中，若key存在替换旧值，若key不存在将kv追加到链表尾部 ConcurrentHashMap迭代器是强一致性还是弱一致性？HashMap呢？ 迭代器iterator是弱一致性的，因为在迭代过程中可以向map中添加元素；而HashMap是强一致性的。 ","date":"2023-08-09","objectID":"/ds/:0:7","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"排序算法比较 稳定排序算法的优点在于可以保持相等元素的相对顺序，这在某些应用场景下非常重要，例如按照年龄和姓名对学生名单排序时，如果年龄相同，则应该按照姓名的字母顺序排序，这就需要使用稳定排序算法。 ","date":"2023-08-09","objectID":"/ds/:0:8","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"贪心算法 vs 动态规划？ 站在全局角度来看，对于每个可选项，贪心算法是只保留一种决策（要么最优，要么非最优） ，动态规划保留了它的一个决策集，从中取最优。 当然，算法有优势就有劣势，贪心算法虽然适用条件苛刻，但复杂度低，当可选集太大时，动态规划这种穷举策略会比较耗时，而贪心算法虽然并不一定能找到最优解，但也可以找到一个接近最优解的解。 综上，贪心算法选的是当前最优解，而动态规划是通过子问题的最优解推出当前的最优解。 ","date":"2023-08-09","objectID":"/ds/:0:9","tags":["DS"],"title":"🚩DS - 数据结构","uri":"/ds/"},{"categories":["interview"],"content":"OSI 七层模型？ 应用层：为计算机用户提供服务 HTTP 七层负载均衡：【Nginx 的 server/udp】读取报文数据部分，根据数据内容做出负载均衡决策，更灵活 表示层：数据加密解密 会话层：管理应用程序之间的会话 传输层：通用的数据传输服务 TCP/UDP 四层负载均衡：【Nginx 的 upstream】通过 ip+port，转发到真实服务器，性能好 网络层：路由和寻址 数据链路层：帧编码和差错校验 VLAN：逻辑划分数据，将一个物理局域网划分为多个虚拟局域网，每个 VLAN 都是一个独立的广播域。 物理层：透明传输比特流 ","date":"2023-08-09","objectID":"/network/:0:1","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"TCP/IP 四层模型？ 应用层（Application Layer）：提供应用程序之间的通信和数据交换，包括DNS（基于 UDP）、HTTP、FTP、SMTP等协议。 传输层（Transport Layer）：负责提供端到端的数据传输，包括TCP（传输控制协议）和UDP（用户数据报协议）。 网络层（Network Layer）：处理网络间的数据传输和路径选择，包括IP（网际协议）和ICMP（Internet控制消息协议）。 链路层（Link Layer）：负责物理网络之间的数据传输，包括以太网、Wi-Fi等。 ","date":"2023-08-09","objectID":"/network/:0:2","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"请求转发（Forward）和重定向（Redirect）的区别？ 转发（Forward）： 服务器行为 地址栏路径不变 服务器收到请求后，直接访问目标地址的URL，把那个URL的响应内容返回给浏览器 事情未做完，数据共享 如：有些网页转发天气预报信息 重定向（Redirect）： 客户端行为 地址栏路径改变 服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址，所以地址栏显示的是新的URL 事情已做完，数据不共享 如：用户未登录，重定向到登录页面。 ","date":"2023-08-09","objectID":"/network/:0:3","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"为什么要三次握手？ 为什么不能两次？ 为了防止已失效的连接请求报文段突然又传送到了服务端，导致资源建立无意义的连接。 假设不采用“三次握手”，那么只要server对一个早已失效的报文段发出确认，新的连接就建立了。但此时的client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样server的很多资源就白白浪费掉了。采用三次握手的话，server由于收不到确认的时候，就知道client并没有要求建立连接，这就有效的防止了服务器端的一直等待而浪费资源。 为什么不用四次？ 因为握手的目的是为了证明服务端和客户端都具有接收和发送数据的能力，在第二次握手时，服务端发送的ack=seq+1证明了客户端的网络联通性，并且发送了自己的seq，只需要等待第三次握手的ack=seq+1就能证明服务端的网络联通性，可以三次做到的，就不用第四次了，节约资源。 ","date":"2023-08-09","objectID":"/network/:0:4","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"HTTPS 的原理？ HTTPS和HTTP的区别？ HTTPS 增加了传输层安全协议（Transport Layer Security，TLS） 端口不同：http 是 80 端口，https 是 443 端口 使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，可以防止”中间人“攻击。一般免费证书较少，因而需要一定费用。 两个阶段？ 分成证书验证阶段（非对称加密）和数据传输阶段（对称加密，保证效率） 使用 HTTPS 会被抓包吗？ 会，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。 为什么 HTTPS 协议需要 9 倍时延才能完成通信？ HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。 TCP 协议需要通过三次握手建立 TCP 连接保证通信的可靠性（1.5-RTT）； TLS 协议会在 TCP 协议之上通过四次握手建立 TLS 连接保证通信的安全性（2-RTT）； HTTP 协议会在 TCP 和 TLS 上通过一次往返发送请求并接收响应（1-RTT）； ","date":"2023-08-09","objectID":"/network/:0:5","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"浏览器输入url之后发生了什么？ 通过 DNS 将域名解析为ip地址 通过ip和端口号向服务器发送GET请求 如果配置了Nginx，可能会发生负载均衡，转发到其他ip地址 传输层进行TCP三次握手 获得响应数据，加上js代码渲染页面 传输完成后四次挥手断开连接 ","date":"2023-08-09","objectID":"/network/:0:6","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"get请求和post请求的区别？ GET和POST本质上没区别都是依赖TCP/IP链接。HTTP之所以划分出GET, POST, PUT, DELETE等请求类型，是为了满足不同的需求。那你肯定又有疑问了：底层都是TCP，那怎么实现以上不同的呢？比如GET请求的最大2KB是在哪验证的呢？ GET, POST, PUT, DELETE等不同请求类型的不同处理策略是通过数据包控制的。 比如：GET产生一个TCP数据包；POST产生两个TCP数据包。 对于GET方式的请求，浏览器会把header和data一并发送出去，服务器响应200（返回数据）；而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。 为什么删除不能用get请求 通过历史记录或书签，很容易在没有意识到的情况下重新输入GET请求。如果GET是破坏性的，可能会导致意外的数据丢失。 ","date":"2023-08-09","objectID":"/network/:0:7","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"你了解Java应用开发中的注入攻击吗？ SQL注入：or 1=1 操作系统命令注入：Java 语言提供了类似 Runtime.exec(…) 的 API XML 注入攻击：Java 核心类库提供了全面的 XML 处理、转换等各种 API，而 XML 自身是可以包含动态内容的 如何保证安全： 运行时安全机制。可以简单认为，就是限制 Java 运行时的行为，不要做越权或者不靠谱的事情。从原则上来说，Java 的 GC 等资源回收管理机制，都可以看作是运行时安全的一部分，如果相应机制失效，就会导致 JVM 出现 OOM 等错误，可看作是另类的拒绝服务。 Java 提供的安全框架 API，这是构建安全通信等应用的基础。加密算法、HTTPS JDK 集成的各种安全工具。尽量使用较新版本的 JDK，并使用推荐的安全机制和标准 ","date":"2023-08-09","objectID":"/network/:0:8","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"公钥和私钥？ 假设 (public key,private key) 代表一个账户 通信：拿对方的公钥加密，发送信息，对方收到后拿自己的密钥解密。 签名：拿自己的密钥加密，其他人拿自己的公钥验证。 ","date":"2023-08-09","objectID":"/network/:0:9","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"TCP 协议 TCP 头部一般包含以下字段： 源端口号（Source Port）：16 位，用于标识发送端口号。 目的端口号（Destination Port）：16 位，用于标识接收端口号。 序列号（Sequence Number）：32 位，用于标识该数据段的序列号，用于保证数据的有序传输。 确认号（Acknowledgment Number）：32 位，用于标识期望接收的下一个数据段的序列号。 数据偏移（Data Offset）：4 位，用于标识 TCP 头部的长度，以 4 字节为单位。 保留（Reserved）：6 位，保留未使用。 标志位（Flags）：6 位，用于标识 TCP 的状态，包括 URG、ACK、PSH、RST、SYN、FIN 等。 窗口大小（Window Size）：16 位，用于标识接收方能够接收的数据量大小。 校验和（Checksum）：16 位，用于检验 TCP 头部和数据的正确性。 紧急指针（Urgent Pointer）：16 位，用于标识 TCP 数据中的紧急数据的位置。 选项（Options）：可选，用于传输一些额外的信息，例如最大段大小（MSS）、时间戳（Timestamp）等。 流量控制和拥塞控制的区别？ TCP 流量控制和拥塞控制的最大区别在于它们的控制对象和目的不同。 TCP 流量控制的控制对象是接收方，其目的是控制发送方向接收方发送数据的速率，以避免接收方缓冲区溢出。TCP 流量控制是通过滑动窗口机制，动态调整窗口大小来控制发送方的发送速率，以保证接收方可以顺利接收数据。 TCP 拥塞控制的控制对象是网络，其目的是控制发送方发送数据的速率，以避免网络拥塞和数据丢失。TCP 拥塞控制是通过检测网络拥塞情况，动态调整拥塞窗口大小来控制发送方的发送速率，以保证网络的可靠性和稳定性。 因此，TCP 流量控制和拥塞控制的最大区别在于它们的控制对象和目的不同。TCP 流量控制是为了保证接收方的数据接收效率和可靠性，而 TCP 拥塞控制是为了保证网络的可靠性和稳定性。 ","date":"2023-08-09","objectID":"/network/:0:10","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"服务端出现大量close_wait状态，可能的情况？ 服务器端应用程序未正确关闭套接字连接：CLOSE_WAIT状态表示服务器已经关闭了连接，但是仍然在等待最后一方（通常是客户端）发送关闭连接的请求。这可能是因为服务器端应用程序没有正确关闭套接字连接，导致连接处于未正常关闭的状态。 客户端未发送关闭连接请求：CLOSE_WAIT状态也可能是由于客户端没有发送关闭连接请求，导致服务器端一直等待着。这可能是客户端应用程序中的bug或者网络通信问题导致的。 服务器资源不足或处理速度较慢：如果服务器端的资源（如文件描述符、线程等）不足或者处理速度较慢，导致无法及时处理关闭连接的请求，可能会导致大量的CLOSE_WAIT状态积累。 网络中的问题：在网络中存在问题，如网络延迟、丢包、拥塞等，可能导致连接关闭的请求未能及时到达服务器端，从而导致CLOSE_WAIT状态的积累。 ","date":"2023-08-09","objectID":"/network/:0:11","tags":["NetWork"],"title":"🚩NetWork - 计算机网络","uri":"/network/"},{"categories":["interview"],"content":"进程间通信的方式有哪几种？ 无名管道( pipe )：父子进程通信 高级管道(popen)：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程 有名管道 (named pipe)：它允许无亲缘关系进程间的通信 消息队列( message queue )：克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号量( semophore )：作为一种锁机制 信号 ( sinal )：用于通知接收进程某个事件已经发生 共享内存( shared memory )：映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问 套接字( socket )：可用于不同机器间的进程通信 ","date":"2023-08-09","objectID":"/os/:0:1","tags":["OS"],"title":"🚩OS - 操作系统","uri":"/os/"},{"categories":["interview"],"content":"用户态和核心态的区别？ Linux系统仅采用ring 0 和 ring 3 这2个权限: ring 0 权限最高，可以使用所有 C P U 指令集 ring 3 权限最低，仅能使用常规 C P U 指令集，不能使用操作硬件资源的 C P U 指令集，比如 I O 读写、网卡访问、申请内存都不行。 用户态与内核态的概念就是C P U 指令集权限的区别，进程中要读写 I O，必然会用到 ring 0 级别的 C P U 指令集，而此时 C P U 的指令集操作权限只有 ring 3，为了可以操作ring 0 级别的 C P U 指令集， C P U 切换指令集操作权限级别为 ring 0，C P U再执行相应的ring 0 级别的 C P U 指令集（内核代码），执行的内核代码会使用当前进程的内核栈。 ","date":"2023-08-09","objectID":"/os/:0:2","tags":["OS"],"title":"🚩OS - 操作系统","uri":"/os/"},{"categories":["draft"],"content":"容器的本质是进程。","date":"2023-08-08","objectID":"/docker_draft/","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"基础知识 ","date":"2023-08-08","objectID":"/docker_draft/:1:0","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"docker的优点 轻量级和可移植性：Docker 容器非常轻量级，相比于虚拟机，它们只需要少量的系统资源和运行时间，因此可以更快地启动和停止。Docker 容器还具有高度的可移植性，可以在任何运行 Docker 的环境中运行，无需担心依赖项或环境配置的问题。 简化应用程序部署：Docker 使得应用程序的部署和管理变得更加简单和可靠。通过将应用程序和其依赖项打包成容器，可以将其轻松地部署到任何环境中，从而简化了整个部署过程。 高度可定制性：Docker 允许用户构建自己的 Docker 镜像，并根据自己的需求进行自定义配置。这使得用户可以根据自己的应用程序需求创建定制化的容器镜像，提高了应用程序的可定制性和可扩展性。 容器化应用程序隔离：Docker 容器提供了隔离的环境，可以避免应用程序之间的冲突和干扰。每个容器都具有自己的文件系统、网络、内存和 CPU 资源，可以有效地隔离应用程序之间的资源。 更好的资源利用率：Docker 容器可以共享主机操作系统的内核和其他资源，从而提高了资源利用率。与虚拟机相比，Docker 容器可以更高效地使用系统资源，从而提高了系统的性能和稳定性。 ","date":"2023-08-08","objectID":"/docker_draft/:1:1","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"docker 的缺点 容器只是一种特殊的进程，通过Linux Namespace机制进行隔离，但是隔离得不彻底，多个容器使用的还是同一个宿主机的操作系统内核： 在容器里执行 top 指令,显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据 很多资源和对象无法被 Namespace 化，如“时间” ","date":"2023-08-08","objectID":"/docker_draft/:1:2","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"Docker Compose 和 Dockerfile 的区别？ Dockerfile：是一种文本文件格式，用于定义 Docker 镜像的构建过程。Dockerfile 中包含了一系列的指令，用于指定基础镜像、安装软件、拷贝文件、配置环境变量等操作，最终生成一个新的 Docker 镜像。Dockerfile 可以通过 docker build 命令执行，将 Dockerfile 转换为 Docker 镜像。 FROM，该命令指定基于哪个基础镜像，因为你要指定一个基础镜像才能基于这个镜像之上进行其他操作，DockerFile第一条必须为From指令。如果同一个DockerFile创建多个镜像时，可使用多个From指令（每个镜像一次） MAINTAINER，指定作者信息 CMD EXPOSE ，这个是用来暴露端口的 ENV ，是用于定义环境变量 VOLUME，这个是用来指定挂载点（也可以在idea配置） Docker Compose：用于定义和运行多个容器之间的依赖关系。Docker Compose 中使用 YAML 文件格式，定义了应用程序中的服务、网络、卷等资源，以及它们之间的关系和依赖关系。Docker Compose 可以通过 docker-compose 命令集成管理多个容器。 需要注意的是，Docker Compose 和 Dockerfile 并不是互斥的概念，它们可以一起使用，Dockerfile 定义容器镜像的构建过程，Docker Compose 定义容器之间的关系和依赖关系，可以一起协同工作，实现更加灵活和高效的容器化应用部署和管理。 ","date":"2023-08-08","objectID":"/docker_draft/:1:3","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"使用教程 首先给服务器上的防火墙打开端口： ","date":"2023-08-08","objectID":"/docker_draft/:2:0","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"1. portainer（可视化工具） # docker搜索 docker search portainer # docker拉取镜像 docker pull portainer/portainer:latest # 创建数据卷 docker volume create portainer_data # 运行容器 docker run --name portainer -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer #-d #容器在后台运行 #-p 9000:9000 # 宿主机9000端口映射容器中的9000端口 #-v /var/run/docker.sock:/var/run/docker.sock # 把宿主机的Docker守护进程(docker daemon)默认监听的Unix域套接字挂载到容器中 #-v # 把宿主机目录 挂载到 容器目录； #–-name portainer # 指定运行容器的名称 cd /usr/libexec/docker/ sudo ln -s docker-runc-current docker-runc 参考：https://cloud.tencent.com/developer/article/1867994 ","date":"2023-08-08","objectID":"/docker_draft/:2:1","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"2. 生成ca证书 # 创建ca证书 mkdir -p /usr/local/ca cd /usr/local/ca/ # 创建密码（输入两次） openssl genrsa -aes256 -out ca-key.pem 4096 # 依次输入密码、国家、省、市、组织名称等,输入‘.’略过 # common name 填服务器ip openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem # 生成服务端的 server-key.pem openssl genrsa -out server-key.pem 4096 # ip地址改成自己的ip或域名 openssl req -subj \"/CN=124.221.157.240\" -sha256 -new -key server-key.pem -out server.csr # 配置任何ip+证书可以访问 echo subjectAltName = IP:124.221.157.240,IP:0.0.0.0 \u003e\u003e extfile.cnf # 密钥仅用于服务器身份验证 echo extendedKeyUsage = serverAuth \u003e\u003e extfile.cnf # 生成签名证书 openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf # 生成客户端的 key.pem openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr # 密钥适用于客户端身份验证 echo extendedKeyUsage = clientAuth \u003e extfile-client.cnf # 生成签名证书 以下表示365天，可以改成更长 openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf # 删除过程中产生的配置文件 rm -v client.csr server.csr extfile.cnf extfile-client.cnf # 密钥读写权限改为读取权限 # from 0600 (rw-------) to 0400 (r--------) chmod -v 0400 ca-key.pem key.pem server-key.pem # 证书对外可读，也关闭写权限 # from 0644 (rw-r--r--) to 0444 (r--r--r--) chmod -v 0444 ca.pem server-cert.pem cert.pem # 复制证书 cp server-*.pem /etc/docker/ cp ca.pem /etc/docker/ # docker 关联 ca vim /usr/lib/systemd/system/docker.service # 修改ExecStart这一行（添加没有的部分，不删除原有的） ExecStart=/usr/bin/dockerd --tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server-cert.pem --tlskey=/etc/docker/server-key.pem -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock # 重新加载 daemon systemctl daemon-reload # 重启 docker systemctl restart docker # 开放2376端口 /sbin/iptables -I INPUT -p tcp --dport 2376 -j ACCEPT # 回到主机，把ca文件拷贝出来，-r 表示拷贝文件夹 scp -r ubuntu@124.221.157.240:/usr/local/ca ~/Desktop # 这里遇到了 key.pem 无法拷贝的问题，临时修改了权限 chmod -v 0444 key.pem # 单独拷贝 scp ubuntu@124.221.157.240:/usr/local/ca/key.pem ~/Desktop/ca # 再改回来 chmod -v 0400 key.pem # 测试证书接口 curl https://124.221.157.240:2376/info --cert ./cert.pem --key ./key.pem --cacert ./ca.pem 证书生效了。 参考：https://blog.csdn.net/qq_46126559/article/details/118373855 ","date":"2023-08-08","objectID":"/docker_draft/:2:2","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"3. idea部署https resources/docker/dockerFile #FROM openjdk:8-jdk-alpine FROM java:8 RUN mkdir -p /apps # app改成项目名称 ADD *.jar /apps/app.jar #对外暴露的端口 EXPOSE 8801 # 环境、jar包路径 # app改成项目名称 ENTRYPOINT [\"java\",\"-Xmx512m\",\"-Xms512m\",\"-Dspring.profiles.active=test\",\"-jar\",\"/apps/app.jar\"] RUN echo \"Asia/shanghai\" \u003e /etc/timezone; ENV LANG C.UTF-8 https://124.221.157.240:2376 选择包含cert.pem、key.pem、ca.pem 的文件夹 /etc/localtime /usr/share/zoneinfo/Asia/Shanghai /apps ","date":"2023-08-08","objectID":"/docker_draft/:2:3","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"4. Redis 安装 # 拉镜像 docker pull redis:latest # 挂载目录 mkdir /docker-data/redis cd /docker-data/redis # 下载redis.conf wget http://download.redis.io/redis-stable/redis.conf # 权限 chmod 777 redis.conf # 修改配置信息 vim /docker-data/redis/redis.conf bind 127.0.0.1 # 这行要注释掉，解除本地连接限制 protected-mode no # 默认yes，如果设置为yes，则只允许在本机的回环连接，其他机器无法连接。 daemonize no # 默认no 为不守护进程模式，docker部署不需要改为yes，docker run -d本身就是后台启动，不然会冲突 requirepass 123456 # 设置密码 appendonly yes # 持久化 #启动 docker run --name redis \\ -p 6379:6379 \\ -v /docker-data/redis/redis.conf:/etc/redis/redis.conf \\ -v /docker-data/redis:/data \\ -d redis redis-server /etc/redis/redis.conf --appendonly yes 参考：https://cloud.tencent.com/developer/article/1997596 ","date":"2023-08-08","objectID":"/docker_draft/:2:4","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"5. minio 配置 docker run -p 9090:9090 -p 9091:9091 \\ --name minio \\ -d --restart=always \\ -e \"MINIO_ACCESS_KEY=minioadmin\" \\ -e \"MINIO_SECRET_KEY=Josh@123\" \\ -v /home/minio/data:/data \\ -v /home/minio/config:/root/.minio \\ minio/minio server \\ /data --console-address \":9091\" -address \":9090\" 9090是服务端口，9091是可视化界面 ","date":"2023-08-08","objectID":"/docker_draft/:2:5","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"监控系统 ","date":"2023-08-08","objectID":"/docker_draft/:3:0","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"如何配置监控系统？ 安装：编写一个 docker-compose.yml 文件，指定 Prometheus 和 Grafana 的镜像、端口等配置，然后使用 docker-compose up 命令启动 Prometheus 和 Grafana 。 version: '3' services: prometheus: image: prom/prometheus ports: - \"9090:9090\" volumes: - ./prometheus:/etc/prometheus/ command: - --config.file=/etc/prometheus/prometheus.yml - --storage.tsdb.path=/prometheus restart: always grafana: image: grafana/grafana ports: - \"3000:3000\" volumes: - ./grafana:/var/lib/grafana restart: always 配置 Prometheus：在 Prometheus 的配置文件中，指定需要监控的目标（例如 Java 应用程序），以及需要采集的指标（例如 JVM 内存使用情况、线程池情况等）。可以使用 Prometheus 的查询语言（PromQL）查询指标，在Grafana面板中，点击“Apply”或“Refresh”按钮，即可执行查询语句并显示结果 # 获取实例的 CPU 使用率： 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) # 获取实例的内存使用率： 100 - ((node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) / node_memory_MemTotal_bytes) * 100 # 获取实例的磁盘使用率： 100 - (avg by (instance) (node_filesystem_free_bytes{fstype=\"ext4\"} / node_filesystem_size_bytes{fstype=\"ext4\"}) * 100) # 获取 HTTP 请求数量： sum by (job) (http_requests_total) # 获取 HTTP 请求错误数量： sum by (job, status) (http_requests_total{status=~\"5..\"}) # 获取 HTTP 请求响应时间的平均值： avg by (job) (http_request_duration_seconds) # 获取 HTTP 请求响应时间的 90% 百分位数： histogram_quantile(0.9, sum by (le) (rate(http_request_duration_seconds_bucket[5m]))) # 获取容器 CPU 使用率： sum by (container) (rate(container_cpu_usage_seconds_total{container!=\"POD\"}[5m])) * 100 # 获取容器内存使用率： sum by (container) (container_memory_usage_bytes{container!=\"POD\"}) / sum by (container) (container_spec_memory_limit_bytes{container!=\"POD\"}) * 100 配置 Grafana：在 Grafana 中，选择“Prometheus”作为数据源类型，创建仪表盘和面板，使用 Grafana 的查询语言（GQL）查询指标，并将查询结果可视化展示。 # 获取指定时间范围内的监控数据 SELECT mean(\"metric_name\") FROM \"measurement_name\" WHERE time \u003e= now() - 1h GROUP BY time(1m) fill(null) # 数据聚合和重采样 SELECT sum(\"metric_name\") FROM \"measurement_name\" WHERE time \u003e= now() - 1d GROUP BY time(1h) fill(0) # 根据标签过滤数据 SELECT mean(\"metric_name\") FROM \"measurement_name\" WHERE \"tag_name\" =~ /tag_value/ AND time \u003e= now() - 1h GROUP BY time(1m) fill(null) # 获取最新的监控数据 SELECT last(\"metric_name\") FROM \"measurement_name\" WHERE time \u003e= now() - 1h GROUP BY \"tag_name\" fill(null) # 数据去重 SELECT distinct(\"tag_name\") FROM \"measurement_name\" # 计算两个指标之间的关系 SELECT derivative(\"metric1_name\") / derivative(\"metric2_name\") * 100 FROM \"measurement_name\" WHERE time \u003e= now() - 1h GROUP BY time(1m) fill(null) # 计算指标的百分位数 SELECT percentile(\"metric_name\", 95) FROM \"measurement_name\" WHERE time \u003e= now() - 1h GROUP BY time(1m) fill(null) ","date":"2023-08-08","objectID":"/docker_draft/:3:1","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"Springboot 集成 grafana + Prometheus 获得 99 线和 999 线？ 添加依赖 在 Spring Boot 项目的 pom.xml 文件中添加以下依赖： \u003cdependency\u003e \u003cgroupId\u003eio.micrometer\u003c/groupId\u003e \u003cartifactId\u003emicrometer-core\u003c/artifactId\u003e \u003cversion\u003e1.5.9\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.micrometer\u003c/groupId\u003e \u003cartifactId\u003emicrometer-registry-prometheus\u003c/artifactId\u003e \u003cversion\u003e1.5.9\u003c/version\u003e \u003c/dependency\u003e 其中，micrometer-core 是 Micrometer 库的核心依赖，micrometer-registry-prometheus 是 Micrometer 的 Prometheus 注册表依赖。 配置 Prometheus 注册表 在 Spring Boot 项目的配置文件 application.yml 中添加 Prometheus 注册表配置： management: endpoints: web: exposure: include: \"*\" metrics: tags: application: ${spring.application.name} distribution: percentiles-histogram: http.server.requests: true export: prometheus: enabled: true step: 10s 其中，management.endpoints.web.exposure.include 配置开启所有的监控端点；management.metrics.tags.application 配置应用程序名称；management.metrics.export.prometheus.enabled 配置开启 Prometheus 注册表；management.metrics.export.prometheus.step 配置采集数据的时间间隔。 配置 Grafana 数据源 在 Grafana 中添加 Prometheus 数据源，并配置数据源 URL。例如，Prometheus 数据源 URL 可以配置为 http://localhost:9090。 创建 Dashboard 在 Grafana 中创建 Dashboard，并配置 Panel。例如，可以添加一个新的 Graph Panel，然后在 Metrics 选项卡中选择 http.server.requests 指标并设置 percentiles 为 0.99 和 0.999。 查看 99 线和 999 线 在创建好的 Dashboard 中，可以查看 http.server.requests 指标的 99 线和 999 线。这些线条将显示在 Graph Panel 中，并显示当前时间范围内的 99% 和 99.9% 的请求响应时间。 以上是 Spring Boot 集成 Grafana 和 Prometheus 并获取 99 线和 999 线的步骤。需要注意的是，具体的配置和实现可能因应用程序的具体情况而异，需要根据具体情况进行调整和修改。 ","date":"2023-08-08","objectID":"/docker_draft/:3:2","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["draft"],"content":"动态配置限流阈值 要根据 Grafana 监控数据动态更改限流阈值，可以使用 Grafana 的 Alerting 功能和 Spring Boot 的 Actuator 端点，具体步骤如下： 创建 Grafana Alerting 规则 在 Grafana 中创建 Alerting 规则，以监控请求响应时间超过 999 线的情况。具体步骤如下： a. 在 Grafana 中创建一个 Dashboard，并添加一个 Graph Panel。 b. 在 Metrics 选项卡中选择 http.server.requests 指标，并设置 percentiles 为 0.999。 c. 在 Alert 选项卡中创建一个 Alerting 规则，当请求响应时间超过 999 线时触发。 d. 在 Notifications 选项卡中配置 Alert 的通知方式，例如邮件通知。 创建 Actuator 端点 在 Spring Boot 应用程序中创建 Actuator 端点，用于接收来自 Grafana 的 Alerting 通知，并动态更改限流阈值。具体步骤如下： a. 创建一个实现 Actuator Endpoint 接口的类，用于接收来自 Grafana 的 Alerting 通知。 @Component @Endpoint(id = \"my-endpoint\") public class MyEndpoint { @WriteOperation public void setRateLimit(@Selector String route, int rateLimit) { // 设置限流阈值，例如使用 Redis 缓存保存阈值。 } } 其中，setRateLimit 方法用于设置限流阈值，例如使用 Redis 缓存保存阈值。 b. 在 Spring Boot 应用程序的配置文件 application.yml 中开启 Actuator 端点。 management: endpoints: web: exposure: include: \"*\" endpoint: my-endpoint: enabled: true 实现限流器 在 Spring Boot 应用程序中实现限流器，用于根据 Actuator 端点中保存的阈值进行限流。例如，可以使用 Spring Cloud Gateway 的 RequestRateLimiter 过滤器进行限流。 @Component public class RateLimiterGatewayFilterFactory extends AbstractGatewayFilterFactory\u003cRateLimiterGatewayFilterFactory.Config\u003e { // 从 Redis 缓存中获取限流阈值 private int getRateLimit(String route) { // 例如使用 Redis 缓存保存阈值。 } public RateLimiterGatewayFilterFactory() { super(Config.class); } @Override public GatewayFilter apply(Config config) { return (exchange, chain) -\u003e { String route = exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_ROUTE_ATTR); int rateLimit = getRateLimit(route); // 使用 RequestRateLimiter 过滤器进行限流 return new RequestRateLimiterGatewayFilterFactory().apply(new RequestRateLimiterGatewayFilterFactory.Config().setRateLimiter(new RedisRateLimiter(rateLimit, rateLimit, Duration.ofSeconds(1)))); }; } public static class Config { // 配置 } } 其中，RateLimiterGatewayFilterFactory 是自定义的限流器，使用 Redis 缓存保存限流阈值；getRateLimit 方法用于从 Redis 缓存中获取限流阈值；使用 RequestRateLimiter 过滤器进行限流。 测试 启动 Spring Boot 应用程序，然后在 Grafana 中创建一个 Alerting 规则，当请求响应时间超过 999 线时触发。当 Alert 触发时，Grafana 会向 Actuator 端点发送通知，Actuator 端点会动态更改限流阈值。限流器会根据新的限流阈值进行限流，从而有效控制请求的并发量。 以上是根据 Grafana 监控数据动态更改限流阈值的步骤。需要注意的是，具体的实现和配置可能因应用程序的具体情况而异，需要根据具体情况进行调整和修改。 ","date":"2023-08-08","objectID":"/docker_draft/:3:3","tags":["Docker"],"title":"Docker 杂记","uri":"/docker_draft/"},{"categories":["practice"],"content":"一步一个脚印","date":"2023-08-08","objectID":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/","tags":["LeetCode","周赛与笔试"],"title":"往期周赛与笔试题解","uri":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/"},{"categories":["practice"],"content":"20230709 class Solution { public static int maximumJumps(int[] nums, int target) { int len = nums.length; int[] dp = new int[len]; for (int i = 1; i \u003c len; i++) { for (int j = 0; j \u003c i; j++) { if (Math.abs(nums[j] - nums[i]) \u003c= target){ if (dp[j] == 0 \u0026\u0026 j != 0) continue; dp[i] = Math.max(dp[i], dp[j] + 1); } } } return dp[len - 1] == 0 ? -1 : dp[len - 1]; } } class Solution { public int maxNonDecreasingLength(int[] nums1, int[] nums2) { int n = nums1.length; // dp1[i] 表示以 nums1[i] 结尾的最长非递减子数组长度 // dp2[i] 表示以 nums2[i] 结尾的最长非递减子数组长度 int[] dp1 = new int[n], dp2 = new int[n]; Arrays.fill(dp1, 1); Arrays.fill(dp2, 1); for (int i = 1; i \u003c n; i++) { if (nums1[i] \u003e= nums1[i - 1]) { dp1[i] = Math.max(dp1[i], dp1[i - 1] + 1); } if (nums1[i] \u003e= nums2[i - 1]) { dp1[i] = Math.max(dp1[i], dp2[i - 1] + 1); } if (nums2[i] \u003e= nums2[i - 1]) { dp2[i] = Math.max(dp2[i], dp2[i - 1] + 1); } if (nums2[i] \u003e= nums1[i - 1]) { dp2[i] = Math.max(dp2[i], dp1[i - 1] + 1); } } int max1 = IntStream.of(dp1).max().getAsInt(); int max2 = IntStream.of(dp2).max().getAsInt(); return Math.max(max1, max2); } } ","date":"2023-08-08","objectID":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/:0:1","tags":["LeetCode","周赛与笔试"],"title":"往期周赛与笔试题解","uri":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/"},{"categories":["practice"],"content":"20230716 class Solution { public int maximumBeauty(int[] nums, int k) { Arrays.sort(nums); int res = 0; for (int l = 0, r = 0; r \u003c nums.length; r++) { while (nums[r] \u003e nums[l] + 2 * k) { l++; } res = Math.max(res, r - l + 1); } return res; } } public static int minimumIndex(List\u003cInteger\u003e nums) { Map\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e(); int max = nums.get(0); int count = 1; //初始化map,以及找出出现最多的元素max，以及它出现的次数count for (int i = 0; i \u003c nums.size(); ++i) { map.put(nums.get(i), map.get(nums.get(i)) != null ? map.get(nums.get(i)) + 1 : 1); } for (int i : map.keySet()) { if (map.get(i) \u003e count) { max = i; count = map.get(i); } } //不在需要记录右边的，因为count_r=count-countL int countL = 0; for (int i = 0; i \u003c nums.size() - 1; ++i) { //如果变动了max if (nums.get(i) == max) { countL++; } //判断两边是否有支配元素且相同 if (2 * countL \u003e i + 1 \u0026\u0026 2 * (count - countL) \u003e nums.size() - i - 1) { return i; } } return -1; } ","date":"2023-08-08","objectID":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/:0:2","tags":["LeetCode","周赛与笔试"],"title":"往期周赛与笔试题解","uri":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/"},{"categories":["practice"],"content":"20230722 class Solution { public long maxScore(int[] nums, int x) { int n = nums.length; long even = -x, odd = -x; if (nums[0] % 2 == 0) even = nums[0]; else odd = nums[0]; for (int i = 1; i \u003c n; ++i) { if (nums[i] % 2 == 0) even = Math.max(even, odd - x) + nums[i]; else odd = Math.max(odd, even - x) + nums[i]; } return Math.max(even, odd); } } public static final int MOD = (int) (1e9 + 7); public static int numberOfWays(int n, int x) { List\u003cInteger\u003e nums = new ArrayList\u003c\u003e();// 所有可能的x的幂 for (int i = 1; Math.pow(i, x) \u003c= n; i++) { nums.add((int) Math.pow(i, x)); } // 01背包问题，不可重复 int[] dp = new int[n + 1]; // 选择某个数本身 dp[0] = 1; for (int i = 0; i \u003c nums.size(); i++) { for (int j = n; j \u003e= nums.get(i); j--) { // 加到 dp[0] 为止 dp[j] = (dp[j] + dp[j - nums.get(i)]) % MOD; } } return dp[n]; } ","date":"2023-08-08","objectID":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/:0:3","tags":["LeetCode","周赛与笔试"],"title":"往期周赛与笔试题解","uri":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/"},{"categories":["practice"],"content":"xhs：统计词频 \u003e 3的词语，按照词频倒序，相同词频按字典序输出词汇。 import java.util.*; public class TreeMapWordFrequencyExample { public static void main(String[] args) { // 创建一个TreeMap对象，并使用自定义的Comparator进行排序 TreeMap\u003cString, Integer\u003e wordFrequencyMap = new TreeMap\u003c\u003e(new Comparator\u003cString\u003e() { @Override public int compare(String word1, String word2) { // 词频倒序 int frequencyCompare = wordFrequencyMap.get(word2).compareTo(wordFrequencyMap.get(word1)); if (frequencyCompare == 0) { // 字典序升序 return word1.compareTo(word2); } else { return frequencyCompare; } } }); // 假设有一些单词 List\u003cString\u003e words = Arrays.asList(\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\", \"grape\", \"banana\", \"grape\", \"grape\"); // 统计单词的词频 for (String word : words) { wordFrequencyMap.put(word, wordFrequencyMap.getOrDefault(word, 0) + 1); } // 输出词频大于3的单词，并按照词频倒序输出 for (Map.Entry\u003cString, Integer\u003e entry : wordFrequencyMap.entrySet()) { if (entry.getValue() \u003e 3) { System.out.println(entry.getKey() + \": \" + entry.getValue()); } } } } 以后的题解会单独发在 TAG = 周赛与笔试 中。 ","date":"2023-08-08","objectID":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/:0:4","tags":["LeetCode","周赛与笔试"],"title":"往期周赛与笔试题解","uri":"/%E5%BE%80%E6%9C%9F%E5%91%A8%E8%B5%9B%E9%A2%98%E8%A7%A3/"},{"categories":["practice","interview"],"content":"给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 lv1. 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。 class Solution { public int maxProfit(int[] prices) { int res = 0; int min = Integer.MAX_VALUE; for (int price : prices) { if (price \u003c= min) { min = Math.min(min, price); } else { res = Math.max(res, price - min); } } return res; } } lv2. 在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。 class Solution { public int maxProfit(int[] prices) { if (prices.length \u003c 2) { return 0; } int res = 0; int pre = prices[0]; for (int i = 1; i \u003c prices.length; i++) { int cur = prices[i]; if (cur \u003e pre) { res += cur - pre; } pre = cur; } return res; } } lv3. 每次交易（买入+卖出）需要交手续费 class Solution { public int maxProfit(int[] prices, int fee) { int n = prices.length; int[][] dp = new int[n][2]; // 0 不持有 1 持有 dp[0][0] = 0; dp[0][1] = -prices[0]; for (int i = 1; i \u003c n; i++) { // i-1 昨天 dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][1] + prices[i] - fee); dp[i][1] = Math.max(dp[i - 1][1], dp[i - 1][0] - prices[i]); } return dp[n - 1][0]; } } lv4. 不限次数，但含冷冻期：卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 class Solution { public int maxProfit(int[] prices) { int n = prices.length; if (n \u003c= 1) return 0; // 0 不持有，不在冷冻期 // 1 持有 // 2 不持有，在冷冻期 - 昨天一定持有，且今天要卖 int[][] dp = new int[n][3]; dp[0][0] = 0; dp[0][1] = -prices[0]; dp[0][2] = 0; for (int i = 1; i \u003c n; i++) {//从[1]...[n-1] dp[i][0] = Math.max(dp[i - 1][0], dp[i - 1][2]); dp[i][1] = Math.max(dp[i - 1][1], dp[i - 1][0] - prices[i]); dp[i][2] = dp[i - 1][1] + prices[i]; } return Math.max(dp[n - 1][0], dp[n - 1][2]); } } lv5. 最多可以完成 两笔 交易，不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票） class Solution { public int maxProfit(int[] prices) { if (prices.length \u003c 2) { return 0; } int n = prices.length; int[][] dp = new int[n][5]; // 0 没有任何买卖 // 1 买入第一次 dp[0][1] = -prices[0]; // 2 卖出第一次 // 3 买入第二次 dp[0][3] = -prices[0]; // 4 卖出第二次 for (int i = 1; i \u003c n; i++) { // i-1 表示昨天 // 买入花钱，所以是 -price[i] dp[i][1] = Math.max(dp[i - 1][1], dp[i - 1][0] - prices[i]); // 卖出得钱，所以是 +price[i] dp[i][2] = Math.max(dp[i - 1][2], dp[i - 1][1] + prices[i]); dp[i][3] = Math.max(dp[i - 1][3], dp[i - 1][2] - prices[i]); dp[i][4] = Math.max(dp[i - 1][4], dp[i - 1][3] + prices[i]); } // 最后一天的第二次卖出 return dp[n - 1][4]; } } lv6. 你最多可以完成 k 笔交易。也就是说，你最多可以买 k 次，卖 k 次，你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 class Solution { public int maxProfit(int k, int[] prices) { int len = prices.length; if (len \u003c 2) { return 0; } // 0 未买入 // k+1 第 k 次买入 - 奇数 // k+2 第 k 次卖出 - 偶数 int[][] dp = new int[len][2 * k + 1]; // 每个时刻第一次买入的初始情况 for (int i = 0; i \u003c k; i++) { dp[0][2 * i + 1] = -prices[0]; } for (int i = 1; i \u003c len; i++) { // i-1 表示昨天 for (int j = 1; j \u003c= 2 * k; j++) { if ((j \u0026 1) == 1) { // 当天考虑买入 // 不买，延续昨天的 // 买，要在昨天不持有的基础上买 dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - 1] - prices[i]); } else { // 当天考虑卖出 // 不卖，延续昨天的 // 卖，要在昨天持有的基础上卖 dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - 1] + prices[i]); } } } return dp[len - 1][2 * k]; } } lv999. 限制K笔交易，冷冻期1天，有手续费？","date":"2023-08-08","objectID":"/%E7%BB%8F%E5%85%B8%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%9D%E8%B7%AF/:0:0","tags":["DS","LeetCode"],"title":"🚩经典股票问题 - 动态规划思路","uri":"/%E7%BB%8F%E5%85%B8%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%9D%E8%B7%AF/"},{"categories":["practice","interview"],"content":"这么经典，岂不默写？","date":"2023-08-08","objectID":"/leetcode-%E5%85%B8%E4%B8%AD%E5%85%B8/","tags":["DS","LeetCode"],"title":"🚩LeetCode 典中典","uri":"/leetcode-%E5%85%B8%E4%B8%AD%E5%85%B8/"},{"categories":["practice","interview"],"content":" public class QuickSort { public static void quickSort(int[] nums, int left, int right) { if (left \u003c right) { int pivotIndex = partition(nums, left, right); quickSort(nums, left, pivotIndex - 1); quickSort(nums, pivotIndex + 1, right); } } private static int partition(int[] nums, int left, int right) { int pivot = nums[right]; int i = left - 1; for (int j = left; j \u003c right; j++) { if (nums[j] \u003c pivot) { i++; swap(nums, i, j); } } swap(nums, i + 1, right); return i + 1; } private static void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } } public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new LinkedList\u003c\u003e(); Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); for (TreeNode cur = root; cur != null || !stack.isEmpty(); cur = cur.right) { // 一直下到最左 while (cur != null) { stack.push(cur); cur = cur.left; } cur = stack.pop();// 每次出栈的就是中序遍历顺序的节点，然后在以下进行对应操作 res.add(cur.val); // 最后向右一步 } return res; } public class BinaryTreePreorderTraversal { public List\u003cInteger\u003e preorderTraversal(TreeNode root) { List\u003cInteger\u003e result = new ArrayList\u003c\u003e(); if (root == null) { return result; } Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); result.add(node.val); if (node.right != null) { stack.push(node.right); } if (node.left != null) { stack.push(node.left); } } return result; } } public List\u003cInteger\u003e postorderTraversal(TreeNode root) { List\u003cInteger\u003e result = new ArrayList\u003c\u003e(); if (root == null) { return result; } Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); result.add(0, node.val); // 插入到结果列表的首部 if (node.left != null) { stack.push(node.left); } if (node.right != null) { stack.push(node.right); } } return result; } ","date":"2023-08-08","objectID":"/leetcode-%E5%85%B8%E4%B8%AD%E5%85%B8/:0:0","tags":["DS","LeetCode"],"title":"🚩LeetCode 典中典","uri":"/leetcode-%E5%85%B8%E4%B8%AD%E5%85%B8/"},{"categories":["practice","interview"],"content":"跟着 debug 调试一下","date":"2023-08-08","objectID":"/lfu/","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice","interview"],"content":"[460]LFU 缓存.java 请你为 最不经常使用（LFU）缓存算法设计并实现数据结构。 实现 LFUCache 类： LFUCache(int capacity) - 用数据结构的容量 capacity 初始化对象 int get(int key) - 如果键存在于缓存中，则获取键的值，否则返回 -1。 void put(int key, int value) - 如果键已存在，则变更其值；如果键不存在，请插入键值对。当缓存达到其容量时，则应该在插入新项之前，使最不经常使用的项无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，应该去除 最近最久未使用 的键。 注意「项的使用次数」就是自插入该项以来对其调用 get 和 put 函数的次数之和。使用次数会在对应项被移除后置为 0 。 为了确定最不常使用的键，可以为缓存中的每个键维护一个 使用计数器 。使用计数最小的键是最久未使用的键。 当一个键首次插入到缓存中时，它的使用计数器被设置为 1 (由于 put 操作)。对缓存中的键执行 get 或 put 操作，使用计数器的值将会递增。 ","date":"2023-08-08","objectID":"/lfu/:1:0","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice","interview"],"content":"思路 需要 3 个 Map 来实现三对映射关系其中，一个 key 可能对应多个频率，在 Java 语言中可以使用 LinkedHashSet； key -\u003e value 键映射值， key -\u003e freq 键映射频率， freq -\u003e key 频率映射键， LFU 的核心在于要同步更新三个 map，所以细节的把握非常重要； get 方法根据 key 去 key-value 的 Map 中找，并增加该 key 的频率； put 方法需要考虑缓存大小超出给定容量的情况，若 key 是第一次出现，则需要淘汰一个频率最小的 key，并添加新的 key-value 对；若原本就存在 key，则更新它的 value，并自增它的频率。 ","date":"2023-08-08","objectID":"/lfu/:1:1","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice","interview"],"content":"代码 class LFUCache { // key 到 val 的映射，我们后文称为 KV 表 HashMap\u003cInteger, Integer\u003e keyToVal; // key 到 freq 的映射，我们后文称为 KF 表 HashMap\u003cInteger, Integer\u003e keyToFreq; // freq 到 key 列表的映射，我们后文称为 FK 表 HashMap\u003cInteger, LinkedHashSet\u003cInteger\u003e\u003e freqToKeys; // 记录最小的频次 int minFreq; // 记录 LFU 缓存的最大容量 int capacity; public LFUCache(int capacity) { keyToVal = new HashMap\u003c\u003e(); keyToFreq = new HashMap\u003c\u003e(); freqToKeys = new HashMap\u003c\u003e(); this.capacity = capacity; this.minFreq = 0; } public int get(int key) { if (!keyToVal.containsKey(key)) { return -1; } increaseFreq(key); return keyToVal.get(key); } public void put(int key, int value) { if (this.capacity \u003c= 0) { return; } /* 若 key 已存在，修改对应的 val 即可 */ if (keyToVal.containsKey(key)) { keyToVal.put(key, value); // key 对应的 freq 加一 increaseFreq(key); return; } /* key 不存在，需要插入 */ /* 容量已满的话需要淘汰一个 freq 最小的 key */ if (this.capacity \u003c= keyToVal.size()) { removeMinFreqKey(); } /* 插入 key 和 val，对应的 freq 为 1 */ // 插入 KV 表 keyToVal.put(key, value); // 插入 KF 表 keyToFreq.put(key, 1); // 插入 FK 表 freqToKeys.putIfAbsent(1, new LinkedHashSet\u003c\u003e()); freqToKeys.get(1).add(key); // 插入新 key 后最小的 freq 肯定是 1 this.minFreq = 1; } private void increaseFreq(int key) { int freq = keyToFreq.get(key); /* 更新 KF 表 */ keyToFreq.put(key, freq + 1); /* 更新 FK 表 */ // 将 key 从 freq 对应的列表中删除 freqToKeys.get(freq).remove(key); // 将 key 加入 freq + 1 对应的列表中 freqToKeys.putIfAbsent(freq + 1, new LinkedHashSet\u003c\u003e()); freqToKeys.get(freq + 1).add(key); // 如果 freq 对应的列表空了，移除这个 freq if (freqToKeys.get(freq).isEmpty()) { freqToKeys.remove(freq); // 如果这个 freq 恰好是 minFreq，更新 minFreq if (freq == this.minFreq) { this.minFreq++; } } } private void removeMinFreqKey() { // freq 最小的 key 列表 LinkedHashSet\u003cInteger\u003e keyList = freqToKeys.get(this.minFreq); // 其中最先被插入的那个 key 就是该被淘汰的 key int deletedKey = keyList.iterator().next(); /* 更新 FK 表 */ keyList.remove(deletedKey); if (keyList.isEmpty()) { freqToKeys.remove(this.minFreq); // 问：这里需要更新 minFreq 的值吗？ } /* 更新 KV 表 */ keyToVal.remove(deletedKey); /* 更新 KF 表 */ keyToFreq.remove(deletedKey); } } ","date":"2023-08-08","objectID":"/lfu/:1:2","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice","interview"],"content":"复杂度 时间复杂度：O(1) 空间复杂度：O(capacity) ","date":"2023-08-08","objectID":"/lfu/:1:3","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice","interview"],"content":"调试代码 import org.apache.commons.lang.StringUtils; import java.util.*; import java.util.concurrent.TimeUnit; public class Demo { public static void main(String[] args) throws InterruptedException { Scanner scanner = new Scanner(System.in); LFUCache lfuCache = new LFUCache(5) {{ put(1); put(2); put(3); put(4); put(0); }}; for (int i = 0; i \u003c 10; i++) { int nextInt = new Random().nextInt(5); System.out.println(\"random visit = \" + nextInt); TimeUnit.SECONDS.sleep(1); lfuCache.get(nextInt); lfuCache.look(); } for (int i = 5; i \u003c 10; i++) { System.out.println(\"new key = \" + i); lfuCache.put(i); } } static class LFUCache { // key 到 val 的映射，我们后文称为 KV 表 HashMap\u003cInteger, Integer\u003e keyToVal; // key 到 freq 的映射，我们后文称为 KF 表 HashMap\u003cInteger, Integer\u003e keyToFreq; // freq 到 key 列表的映射，我们后文称为 FK 表 HashMap\u003cInteger, LinkedHashSet\u003cInteger\u003e\u003e freqToKeys; // 记录最小的频次 int minFreq; // 记录 LFU 缓存的最大容量 int capacity; public LFUCache(int capacity) { keyToVal = new HashMap\u003c\u003e(); keyToFreq = new HashMap\u003c\u003e(); freqToKeys = new HashMap\u003c\u003e(); this.capacity = capacity; this.minFreq = 0; } public int get(int key) { if (!keyToVal.containsKey(key)) { return -1; } increaseFreq(key); return keyToVal.get(key); } public void put(int key) { put(key, key * 10); } public void put(int key, int value) { if (this.capacity \u003c= 0) { return; } /* 若 key 已存在，修改对应的 val 即可 */ if (keyToVal.containsKey(key)) { keyToVal.put(key, value); // key 对应的 freq 加一 increaseFreq(key); return; } /* key 不存在，需要插入 */ /* 容量已满的话需要淘汰一个 freq 最小的 key */ if (this.capacity \u003c= keyToVal.size()) { removeMinFreqKey(); } /* 插入 key 和 val，对应的 freq 为 1 */ // 插入 KV 表 keyToVal.put(key, value); // 插入 KF 表 keyToFreq.put(key, 1); // 插入 FK 表 freqToKeys.putIfAbsent(1, new LinkedHashSet\u003c\u003e()); freqToKeys.get(1).add(key); // 插入新 key 后最小的 freq 肯定是 1 this.minFreq = 1; look(); } // kf,fk 的更新 private void increaseFreq(int key) { int freq = keyToFreq.get(key); /* 更新 KF 表 */ keyToFreq.put(key, freq + 1); /* 更新 FK 表 */ // 将 key 从 freq 对应的列表中删除 freqToKeys.get(freq).remove(key); // 将 key 加入 freq + 1 对应的列表中 freqToKeys.putIfAbsent(freq + 1, new LinkedHashSet\u003c\u003e()); freqToKeys.get(freq + 1).add(key); // 如果 freq 对应的列表空了，移除这个 freq if (freqToKeys.get(freq).isEmpty()) { freqToKeys.remove(freq); // 如果这个 freq 恰好是 minFreq，更新 minFreq if (freq == this.minFreq) { this.minFreq++; } } } private void removeMinFreqKey() { // freq 最小的 key 列表 LinkedHashSet\u003cInteger\u003e keyList = freqToKeys.get(this.minFreq); // 其中最先被插入的那个 key 就是该被淘汰的 key int deletedKey = keyList.iterator().next(); /* 更新 FK 表 */ keyList.remove(deletedKey); if (keyList.isEmpty()) { freqToKeys.remove(this.minFreq); System.out.println(\"问：这里需要更新 minFreq 的值吗？\"); // Q：这里需要更新 minFreq 的值吗？ // A: 不需要，因为执行完该方法后，minFreq 会被赋值为1 } /* 更新 KV 表 */ keyToVal.remove(deletedKey); /* 更新 KF 表 */ keyToFreq.remove(deletedKey); } public void look() { System.out.println(\"capacity = \" + capacity); System.out.println(\"minFreq = \" + minFreq); System.out.print(\"-----kv------\"); Iterator\u003cInteger\u003e iterator = keyToVal.keySet().iterator(); while (iterator.hasNext()) { Integer k = iterator.next(); System.out.print(k + \",\" + keyToVal.get(k) + \"; \"); } System.out.println(); System.out.print(\"-----kf------\"); Iterator\u003cInteger\u003e iterator2 = keyToFreq.keySet().iterator(); while (iterator2.hasNext()) { Integer k = iterator2.next(); System.out.print(k + \",\" + keyToFreq.get(k) + \"; \"); } System.out.println(); System.out.print(\"-----fk------\"); Iterator\u003cInteger\u003e iterator3 = freqToKeys.keySet().iterator(); while (iterator3.hasNext()) { Integer k = iterator3.next(); System.out.print(k + \",[\" + StringUtils.join(freqToKeys.get(k), \",\") + \"]; \"); } System.out.println(); System.out.println(\"==================\"); } } } ","date":"2023-08-08","objectID":"/lfu/:1:4","tags":["DS","LeetCode"],"title":"LFU","uri":"/lfu/"},{"categories":["practice"],"content":"常回来看看","date":"2023-08-08","objectID":"/leetcode/","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"数组 ","date":"2023-08-08","objectID":"/leetcode/:1:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[560]和为K的子数组.java 给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。 思路 采用空间换时间策略，用 map 记录前序和。 代码 class Solution { public int subarraySum(int[] nums, int k) { int count = 0, sum = 0; Map\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e() {{ put(0, 1); }}; for (int num : nums) { sum += num; count += map.getOrDefault(sum - k, 0); map.put(sum, map.getOrDefault(sum, 0) + 1); } return count; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:1:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[304]二维区域和检索 - 矩阵不可变.java 给定一个二维矩阵 matrix，以下类型的多个请求： 计算其子矩形范围内元素的总和，该子矩阵的左上角为 (row1, col1) ，右下角为 (row2, col2) 。 实现 NumMatrix 类： NumMatrix(int matrix) 给定整数矩阵 matrix 进行初始化 int sumRegion(int row1, int col1, int row2, int col2) 返回左上角 (row1, col1) 、右下角(row2, col2) 的子矩阵的元素总和。 思路 代码 class NumMatrix { // 存储左上角区域和 private int[][] preSum; public NumMatrix(int[][] matrix) { int m = matrix.length; int n = matrix[0].length; preSum = new int[m][n]; preSum[0][0] = matrix[0][0]; for (int i = 1; i \u003c m; i++) { preSum[i][0] = preSum[i - 1][0] + matrix[i][0]; } for (int i = 1; i \u003c n; i++) { preSum[0][i] = preSum[0][i - 1] + matrix[0][i]; } for (int i = 1; i \u003c m; i++) { for (int j = 1; j \u003c n; j++) { preSum[i][j] = matrix[i][j] + preSum[i - 1][j] + preSum[i][j - 1] - preSum[i - 1][j - 1]; } } } public int sumRegion(int row1, int col1, int row2, int col2) { int leftCol = (col1 == 0) ? 0 : preSum[row2][col1 - 1]; int upRow = (row1 == 0) ? 0 : preSum[row1 - 1][col2]; int leftUpArea = (row1 == 0 || col1 == 0) ? 0 : preSum[row1 - 1][col1 - 1]; return preSum[row2][col2] - leftCol - upRow + leftUpArea; } } 复杂度 时间复杂度：O(n^2) 空间复杂度：O(n^2) ","date":"2023-08-08","objectID":"/leetcode/:1:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[1094]拼车.java 假设你是一位顺风车司机，车上最初有 capacity 个空座位可以用来载客。由于道路的限制，车 只能 向一个方向行驶（也就是说，不允许掉头或改变方向，你可以将其想象为一个向量）。 这儿有一份乘客行程计划表 trips，其中 trips[i] = [num_passengers, start_location, end_location] 包含了第 i 组乘客的行程信息： 必须接送的乘客数量； 乘客的上车地点； 以及乘客的下车地点。 这些给出的地点位置是从你的 初始 出发位置向前行驶到这些地点所需的距离（它们一定在你的行驶方向上）。 请你根据给出的行程计划表和车子的座位数，来判断你的车是否可以顺利完成接送所有乘客的任务（当且仅当你可以在所有给定的行程中接送所有乘客时，返回 true，否则请返回 false）。 思路 利用差分数组，上车加乘客数，下车减乘客数，最后遍历一次检验是否有超载情况。 代码 class Solution { public boolean carPooling(int[][] trips, int capacity) { // 差分数组 int[] diff = new int[1001]; // 统计每一站净上车人数 for (int[] trip : trips) { int passengers = trip[0]; int start = trip[1]; int end = trip[2]; diff[start] += passengers; diff[end] -= passengers; } // 排查是否有超载情况 int cur = 0; for (int i : diff) { cur += i; if (cur \u003e capacity) { return false; } } return true; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:1:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[1109]航班预订统计.java 这里有 n 个航班，它们分别从 1 到 n 进行编号。 有一份航班预订表 bookings ，表中第 i 条预订记录 bookings[i] = [firsti, lasti, seatsi] 意味着在从 firsti 到 lasti （包含 firsti 和 lasti ）的 每个航班 上预订了 seatsi 个座位。 请你返回一个长度为 n 的数组 answer，其中 answer[i] 是航班 i 上预订的座位总数。 思路 利用差分数组，根据每一条数据，在 first 处加座位数，在 last +1 处减座位数，最后遍历累加得到 answer 数组。 代码 class Solution { public int[] corpFlightBookings(int[][] bookings, int n) { int[] diff = new int[n]; int[] answer = new int[n]; for (int[] booking : bookings) { int first = booking[0]; int last = booking[1]; int seats = booking[2]; diff[first - 1] += seats; if (last \u003c n) { // 实际上是 last+1-1 // +1 是因为座位在下一站才腾出来 // -1 是因为航班从 1 开始编号，而数组从 0 开始编号 diff[last] -= seats; } } answer[0] = diff[0]; for (int i = 1; i \u003c n; i++) { answer[i] = answer[i - 1] + diff[i]; } return answer; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:1:4","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"二分搜索模板 int binary_search(int[] nums, int target) { int left = 0, right = nums.length - 1; while(left \u003c= right) { int mid = left + (right - left) / 2; if (nums[mid] \u003c target) { left = mid + 1; } else if (nums[mid] \u003e target) { right = mid - 1; } else if(nums[mid] == target) { // 直接返回 return mid; } } // 直接返回 return -1; } 寻找左边界 int left_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u003c= right) { int mid = left + (right - left) / 2; if (nums[mid] \u003c target) { left = mid + 1; } else if (nums[mid] \u003e target) { right = mid - 1; } else if (nums[mid] == target) { // 别返回，锁定左侧边界 right = mid - 1; } } // 最后要检查 left 越界的情况 if (left \u003e= nums.length || nums[left] != target) return -1; return left; } 寻找右边界 int right_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left \u003c= right) { int mid = left + (right - left) / 2; if (nums[mid] \u003c target) { left = mid + 1; } else if (nums[mid] \u003e target) { right = mid - 1; } else if (nums[mid] == target) { // 别返回，锁定右侧边界 left = mid + 1; } } // 最后要检查 right 越界的情况 if (right \u003c 0 || nums[right] != target) return -1; return right; } ","date":"2023-08-08","objectID":"/leetcode/:1:5","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[875]爱吃香蕉的珂珂.java 珂珂喜欢吃香蕉。这里有 N 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 H 小时后回来。 珂珂可以决定她吃香蕉的速度 K （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 K 根。如果这堆香蕉少于 K 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。 珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。 返回她可以在 H 小时内吃掉所有香蕉的最小速度 K（K 为整数）。 思路 以速度为自变量，花费时间为应变量，构造函数，定义域在 [1,10^9] 所求为最小速度，转换为寻找左边界 花费时间过长，应当加大速度，扩大 left 边界；花费时间相等或足够小，不直接返回，而是缩小 right 边界 退出循环前，left==right \u0026\u0026 f(piles,mid)==h ， 此时位于左边界，right = mid -1 后退出循环，因此最终返回 left 代码 class Solution { public int minEatingSpeed(int[] piles, int h) { int left = 1, right = 1000000000; while (left \u003c= right) { int mid = left + (right - left) / 2; if (f(piles, mid) \u003e h) { left = mid + 1; } else { // 别返回，锁定左侧边界 right = mid - 1; } } return left; } private int f(int[] piles, int v) { int hours = 0; for (int pile : piles) { hours += pile / v; if (pile % v \u003e 0) { hours++; } } return hours; } } 复杂度 时间复杂度：O(nlogn) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:1:6","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[1011]在 D 天内送达包裹的能力.java 传送带上的包裹必须在 D 天内从一个港口运送到另一个港口。 传送带上的第 i 个包裹的重量为 weights[i]。每一天，我们都会按给出重量的顺序往传送带上装载包裹。我们装载的重量不会超过船的最大运载重量。 返回能在 D 天内将传送带上的所有包裹送达的船的最低运载能力。 思路 以承载量为自变量，花费天数为应变量，构造函数，承载量下限为最大的那个包裹的重量，上限为所有包裹加起来的重量。 所求为最小承载量，转换为寻找左边界 花费时间过长，应当加大承载量，扩大 left 边界；花费时间相等或足够小，不直接返回，而是缩小 right 边界 退出循环前，left==right \u0026\u0026 f(weights,mid)==days ， 此时位于左边界，right = mid -1 后退出循环，因此最终返回 left 代码 class Solution { public int shipWithinDays(int[] weights, int days) { int left = 1, right = 1; for (int weight : weights) { if (weight \u003e left) { left = weight; } right += weight; } while (left \u003c= right) { int mid = left + (right - left) / 2; if (f(weights, mid) \u003e days) { left = mid + 1; } else { right = mid - 1; } } return left; } private int f(int[] weights, int capacity) { int days = 1; int todayCapacity = capacity; for (int weight : weights) { if (weight \u003e todayCapacity) { // 今天不够装了，明天一来就装这个包裹 days++; todayCapacity = capacity - weight; } else { // 今天装下这个包裹没问题 todayCapacity -= weight; } } return days; } } 复杂度 时间复杂度：O(nlogn) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:1:7","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"下一个更大元素（循环搜索） class Solution { public int[] nextGreaterElements(int[] nums) { int[] res = new int[nums.length]; Stack\u003cInteger\u003e stack = new Stack\u003c\u003e(); // 从后往前遍历 for (int i = nums.length - 1; i \u003e= 0; i--) { while (!stack.isEmpty() \u0026\u0026 nums[i] \u003e= stack.peek()) { // 栈顶元素即将被 nums[i] 挡住 stack.pop(); } if (stack.isEmpty()) { res[i] = -1; // 后面找不到了，去前面找找看 for (int j = 0; j \u003c i; j++) { if (nums[j] \u003e nums[i]) { res[i] = nums[j]; break; } } } else { res[i] = stack.peek(); } stack.push(nums[i]); } return res; } } ","date":"2023-08-08","objectID":"/leetcode/:1:8","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"搜索二维矩阵 编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性： 每行中的整数从左到右按升序排列。每行的第一个整数大于前一行的最后一个整数。 class Solution { public boolean searchMatrix(int[][] matrix, int target) { int m = matrix.length, n = matrix[0].length; int low = 0, high = m * n - 1; while (low \u003c= high) { int mid = (high - low) / 2 + low; int x = matrix[mid / n][mid % n]; if (x \u003c target) { low = mid + 1; } else if (x \u003e target) { high = mid - 1; } else { return true; } } return false; } } ","date":"2023-08-08","objectID":"/leetcode/:1:9","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"折线图的最少线段数 给你一个二维整数数组 stockPrices ，其中 stockPrices[i] = [dayi, pricei] 表示股票在 dayi 的价格为 pricei 。折线图 是一个二维平面上的若干个点组成的图，横坐标表示日期，纵坐标表示价格，折线图由相邻的点连接而成。比方说下图是一个例子： 请你返回要表示一个折线图所需要的 最少线段数 。 class Solution { public int minimumLines(int[][] stockPrices) { if (stockPrices.length \u003c= 1) { return 0; } Arrays.sort(stockPrices, (a, b) -\u003e { if (a[0] == b[0]) { return a[1] - b[1]; } else { return a[0] - b[0]; } }); int count = 1; String k = getK(stockPrices[1][1] - stockPrices[0][1], stockPrices[1][0] - stockPrices[0][0]); for (int i = 2; i \u003c stockPrices.length; i++) { int dy = stockPrices[i][1] - stockPrices[i - 1][1]; int dx = stockPrices[i][0] - stockPrices[i - 1][0]; if (dy == 0 \u0026\u0026 dx == 0) { continue; } String tempK = getK(dy, dx); if (!k.equals(tempK)) { count++; k = tempK; } } return count; } private String getK(int dy, int dx) { if (dx == 0) { return \"wx\"; } //二者最大公约数，将两数化为最简，方便比较 int gcd = gcd(Math.abs(dy), Math.abs(dx)); dy /= gcd; dx /= gcd; return dy + \"/\" + dx; } /** * 求最大公约数 * * @param a * @param b * @return */ private int gcd(int a, int b) { if (b != 0) { return gcd(b, a % b); } return a; } } ","date":"2023-08-08","objectID":"/leetcode/:1:10","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。 代码（双指针） class Solution { public List\u003cList\u003cInteger\u003e\u003e threeSum(int[] nums) { List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); if (nums == null || nums.length \u003c 3) { return res; } Arrays.sort(nums); for (int i = 0; i \u003c nums.length - 2; i++) { //减枝 if (nums[i] \u003e 0) { break; } //去重 if (i \u003e 0 \u0026\u0026 nums[i] == nums[i - 1]) { continue; } //后两数之和等于第一个数的相反数 int target = -nums[i]; int left = i + 1, right = nums.length - 1; while (left \u003c right) { if (nums[left] + nums[right] == target) { res.add(Arrays.asList(nums[i], nums[left++], nums[right--])); while (left \u003c right \u0026\u0026 nums[left] == nums[left - 1]) { left++; } while (left \u003c right \u0026\u0026 nums[right] == nums[right + 1]) { right--; } } else if (nums[left] + nums[right] \u003c target) { left++; } else { right--; } } } return res; } } 复杂度 时间复杂度：O(n^2) 空间复杂度：$$ \\begin{cases} O(logn)，不允许改变参数\\ O(1)，允许改变参数 \\end{cases} $$ ","date":"2023-08-08","objectID":"/leetcode/:1:11","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"颜色分类（荷兰国旗问题） 给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。 此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。 代码 class Solution { public void sortColors(int[] nums) { int left = 0, right = nums.length - 1; for (int i = 0; i \u003c= right; i++) { if (nums[i] == 0) { //i把0给了left，left只可能把0给i swap(nums, left++, i); } else if (nums[i] == 2) { //i把2给了right，right可能把0或1给i，所以i不能立刻自增 swap(nums, right--, i--); } } } private void swap(int[] nums, int index1, int index2) { int a = nums[index1]; nums[index1] = nums[index2]; nums[index2] = a; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:1:12","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[31]下一个排列.java //整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。 //// // 例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。 // //// 整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。更正式地，如果数组的所有排列根据其字典顺序从小到大排列在一个容器中，那么数组的 下一个排列 就是在这个有序容器中排在它后面的那个排列。如果不存在下一个更大的排列，那么这个数组必须重排为字典序最小的排列（即，其元素按升序排列）。 //// // 例如，arr = [1,2,3] 的下一个排列是 [1,3,2] 。 // 类似地，arr = [2,3,1] 的下一个排列是 [3,1,2] 。 // 而 arr = [3,2,1] 的下一个排列是 [1,2,3] ，因为 [3,2,1] 不存在一个字典序更大的排列。 // //// 给你一个整数数组 nums ，找出 nums 的下一个排列。 //// 必须 原地 修改，只允许使用额外常数空间。 class Solution { public void nextPermutation(int[] nums) { int len = nums.length; int i = len - 2; // 从后往前找到第一个升序的数字 while (i \u003e= 0 \u0026\u0026 nums[i] \u003e= nums[i + 1]) { i--; } // 如果找不到，则说明是最小的排列，需要重排 if (i \u003e= 0) { int j = len - 1; // 从后往前找到第一个比i大的数字 while (nums[i] \u003e= nums[j]) { j--; } // 交换i和j swap(nums, i, j); } // 反转后面的数字:从降序变成升序 reverse(nums, i + 1); } public void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } public void reverse(int[] nums, int start) { int len = nums.length; int i = start, j = len - 1; while (i \u003c j) { swap(nums, i++, j--); } } } ","date":"2023-08-08","objectID":"/leetcode/:1:13","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[200]岛屿数量.java 给你一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，请你计算网格中岛屿的数量。 岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。 此外，你可以假设该网格的四条边均被水包围。 class Solution { public int numIslands(char[][] grid) { int count = 0; for (int i = 0; i \u003c grid.length; i++) { for (int j = 0; j \u003c grid[i].length; j++) { if (grid[i][j] == '1') { count++; callBFS(grid, i, j); } } } return count; } private void callBFS(char[][] grid, int i, int j) { if (i \u003c 0 || i \u003e= grid.length || j \u003c 0 || j \u003e= grid[i].length || grid[i][j] == '0') { return; } grid[i][j] = '0'; callBFS(grid, i + 1, j); callBFS(grid, i - 1, j); callBFS(grid, i, j + 1); callBFS(grid, i, j - 1); } } ","date":"2023-08-08","objectID":"/leetcode/:1:14","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[695]岛屿的最大面积.java //给你一个大小为 m x n 的二进制矩阵 grid 。 //// 岛屿 是由一些相邻的 1 (代表土地) 构成的组合，这里的「相邻」要求两个 1 必须在 水平或者竖直的四个方向上 相邻。你可以假设 grid 的四个边缘都//被 0（代表水）包围着。 //// 岛屿的面积是岛上值为 1 的单元格的数目。 //// 计算并返回 grid 中最大的岛屿面积。如果没有岛屿，则返回面积为 0 。 class Solution { boolean visited[][]; public int maxAreaOfIsland(int[][] grid) { int res = 0; int rows = grid.length; int cols = grid[0].length; visited = new boolean[rows][cols]; for (int i = 0; i \u003c rows; i++) { for (int j = 0; j \u003c cols; j++) { res = Math.max(res, area(i, j, grid)); } } return res; } public int area(int row, int column, int[][] grid) { if (row \u003c 0 || row \u003e= grid.length || column \u003c 0 || column \u003e= grid[row].length || visited[row][column] || grid[row][column] == 0) { return 0; } visited[row][column] = true; return 1 + area(row + 1, column, grid) + area(row - 1, column, grid) + area(row, column + 1, grid) + area(row, column - 1, grid); } } ","date":"2023-08-08","objectID":"/leetcode/:1:15","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[79]单词搜索.java //给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。 //// 单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。 class Solution { boolean[][] visited; public boolean exist(char[][] board, String word) { final int rows = board.length; final int cols = board[0].length; visited = new boolean[rows][cols]; for (int i = 0; i \u003c rows; i++) { for (int j = 0; j \u003c cols; j++) { if (dfs(board, word, 0, i, j)) { return true; } } } return false; } private boolean dfs(char[][] board, String word, int index, int i, int j) { if (index == word.length()) { return true; } if (i \u003c 0 || i \u003e= board.length || j \u003c 0 || j \u003e= board[0].length || visited[i][j] || board[i][j] != word.charAt(index)) { return false; } visited[i][j] = true; boolean res = dfs(board, word, index + 1, i + 1, j) || dfs(board, word, index + 1, i - 1, j) || dfs(board, word, index + 1, i, j + 1) || dfs(board, word, index + 1, i, j - 1); visited[i][j] = false; return res; } } ","date":"2023-08-08","objectID":"/leetcode/:1:16","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"链表 ","date":"2023-08-08","objectID":"/leetcode/:2:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[21]合并两个有序链表.java class Solution { // 非递归 public ListNode mergeTwoLists(ListNode l1, ListNode l2) { // 伪头结点 ListNode dummyHead = new ListNode(-1); ListNode cur = dummyHead; while (l1 != null \u0026\u0026 l2 != null) { if (l1.val \u003c l2.val) { cur.next = l1; l1 = l1.next; } else { cur.next = l2; l2 = l2.next; } cur = cur.next; } // 剩余部分 cur.next = l1 == null ? l2 : l1; return dummyHead.next; } } ","date":"2023-08-08","objectID":"/leetcode/:2:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"反转链表 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 代码 class Solution { public ListNode reverseList(ListNode head) { ListNode pre = null, cur = head, next; while (cur != null) { next = cur.next; cur.next = pre; pre = cur; cur = next; } //cur==null 退出循环，pre此时指向之前的尾节点，现在的头节点。 return pre; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:2:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[92]反转链表 II.java 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u003c= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。 class Solution { public ListNode reverseBetween(ListNode head, int left, int right) { // 因为头节点有可能发生变化，使用虚拟头节点可以避免复杂的分类讨论 ListNode dummyNode = new ListNode(-1, head); ListNode pre = dummyNode; // 第 1 步：从虚拟头节点走 left - 1 步，来到 left 节点的前一个节点 // 建议写在 for 循环里，语义清晰 for (int i = 0; i \u003c left - 1; i++) { pre = pre.next; } // 第 2 步：从 pre 再走 right - left + 1 步，来到 right 节点 ListNode rightNode = pre; for (int i = 0; i \u003c right - left + 1; i++) { rightNode = rightNode.next; } // 第 3 步：切断出一个子链表（截取链表） ListNode leftNode = pre.next; ListNode curr = rightNode.next; // 注意：切断链接 pre.next = null; rightNode.next = null; // 第 4 步：同第 206 题，反转链表的子区间 reverseLinkedList(leftNode); // 第 5 步：接回到原来的链表中 pre.next = rightNode; leftNode.next = curr; return dummyNode.next; } private void reverseLinkedList(ListNode head) { // 也可以使用递归反转一个链表 ListNode pre = null; ListNode cur = head; while (cur != null) { ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } } } ","date":"2023-08-08","objectID":"/leetcode/:2:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[142]环形链表 II.java 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意，pos 仅仅是用于标识环的情况，并不会作为参数传递到函数中。 说明：不允许修改给定的链表。 思路 如图所示，先分析第一次相遇： slow 走过距离：a+b fast 走过距离：a+b+n(b+c) 又因为 fast 速度是 slow 的 2 倍，得：2(a+b) = a+b+n(b+c) 化简得：a=(n-1)(b+c)+c 这时，让 fast 指针回到起点，slow 指针留在原地，它们同时以相同速度运动 c 的距离，慢指针到达交点处，快指针距离交点距离是环长的整数倍，由于我们不知道 c 的大小，只能让快慢指针继续保持相同速度向前走，直到第二次相遇，相遇点必是交点。 代码 public class Solution { public ListNode detectCycle(ListNode head) { ListNode slow = head, fast = head; while (fast != null \u0026\u0026 fast.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) { break; } } //无环 if (fast == null || fast.next == null) { return null; } fast = head; while (fast != slow) { slow = slow.next; fast = fast.next; } return fast; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:2:4","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[86]分隔链表.java //给你一个链表的头节点 head 和一个特定值 x ，请你对链表进行分隔，使得所有 小于 x 的节点都出现在 大于或等于 x 的节点之前。 //// 你应当 保留 两个分区中每个节点的初始相对位置。 //// //// 示例 1： //// //输入：head = [1,4,3,2,5,2], x = 3//输出：[1,2,2,4,3,5] class Solution { public ListNode partition(ListNode head, int x) { ListNode left = new ListNode(0); ListNode right = new ListNode(0); ListNode l = left; ListNode r = right; while (head != null) { if (head.val \u003c x) { l.next = head; l = l.next; } else { r.next = head; r = r.next; } head = head.next; } l.next = right.next; r.next = null; return left.next; } } ","date":"2023-08-08","objectID":"/leetcode/:2:5","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[148]排序链表.java 给你链表的头结点 head ，请将其按 升序 排列并返回 排序后的链表 。 class Solution { public ListNode sortList(ListNode head) { if (head == null || head.next == null) return head; ListNode slow = head, fast = head.next; while (fast != null \u0026\u0026 fast.next != null) { slow = slow.next; fast = fast.next.next; } ListNode right = sortList(slow.next); slow.next = null; ListNode left = sortList(head); return merge(left, right); } private ListNode merge(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(0); ListNode cur = dummy; while (l1 != null \u0026\u0026 l2 != null) { if (l1.val \u003c l2.val) { cur.next = l1; l1 = l1.next; } else { cur.next = l2; l2 = l2.next; } cur = cur.next; } cur.next = l1 == null ? l2 : l1; return dummy.next; } } ","date":"2023-08-08","objectID":"/leetcode/:2:6","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[2]两数相加.java //给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 //// 请你将两个数相加，并以相同形式返回一个表示和的链表。 //// 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 //// //// 示例 1： //// //输入：l1 = [2,4,3], l2 = [5,6,4]//输出：[7,0,8]//解释：342 + 465 = 807. class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummyHead = new ListNode(0); ListNode cur = dummyHead; int carry = 0; while (l1 != null || l2 != null) { int x = l1 == null ? 0 : l1.val; int y = l2 == null ? 0 : l2.val; int sum = x + y + carry; carry = sum / 10; cur.next = new ListNode(sum % 10); cur = cur.next; if (l1 != null) l1 = l1.next; if (l2 != null) l2 = l2.next; } if (carry \u003e 0) { cur.next = new ListNode(carry); } return dummyHead.next; } } ","date":"2023-08-08","objectID":"/leetcode/:2:7","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[143]重排链表.java //给定一个单链表 L 的头节点 head ，单链表 L 表示为： //// //L0 → L1 → … → Ln - 1 → Ln// //// 请将其重新排列后变为： //// //L0 → Ln → L1 → Ln - 1 → L2 → Ln - 2 → … //// 不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。 class Solution { public void reorderList(ListNode head) { if (head == null || head.next == null) { return; } ListNode l1 = head; ListNode slow = head, fast = head, prev = null; while (fast != null \u0026\u0026 fast.next != null) { prev = slow; slow = slow.next; fast = fast.next.next; } prev.next = null; ListNode l2 = slow; l2 = reverse(l2); l1 = merge(l1, l2); } private ListNode reverse(ListNode head) { ListNode prev = null; ListNode curNode = head; while (curNode != null) { ListNode nextNode = curNode.next; curNode.next = prev; prev = curNode; curNode = nextNode; } return prev; } private ListNode merge(ListNode l1, ListNode l2) { ListNode dummyHead = new ListNode(-1); ListNode curNode = dummyHead; while (l1 != null \u0026\u0026 l2 != null) { curNode.next = l1; curNode = curNode.next; l1 = l1.next; curNode.next = l2; curNode = curNode.next; l2 = l2.next; } curNode.next = l1 == null ? l2 : l1; return dummyHead.next; } } ","date":"2023-08-08","objectID":"/leetcode/:2:8","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"二叉树 快速排序就是个二叉树的前序遍历，归并排序就是个二叉树的后序遍历。 ","date":"2023-08-08","objectID":"/leetcode/:3:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"递归框架 /* 二叉树遍历框架 */ void traverse(TreeNode root) { // 前序遍历 traverse(root.left) // 中序遍历 traverse(root.right) // 后序遍历 } ","date":"2023-08-08","objectID":"/leetcode/:3:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"快速排序 void sort(int[] nums, int lo, int hi) { /****** 前序遍历位置 ******/ // 通过交换元素构建分界点 p int p = partition(nums, lo, hi); /************************/ sort(nums, lo, p - 1); sort(nums, p + 1, hi); } ","date":"2023-08-08","objectID":"/leetcode/:3:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"归并排序 void sort(int[] nums, int lo, int hi) { int mid = (lo + hi) / 2; sort(nums, lo, mid); sort(nums, mid + 1, hi); /****** 后序遍历位置 ******/ // 合并两个排好序的子数组 merge(nums, lo, mid, hi); /************************/ } 可以说，只要涉及递归，都可以抽象成二叉树的问题。 ","date":"2023-08-08","objectID":"/leetcode/:3:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"中序遍历二叉树 class Solution { public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new LinkedList\u003c\u003e(); Deque\u003cTreeNode\u003e stack = new LinkedList\u003c\u003e(); for (TreeNode cur = root; cur != null || !stack.isEmpty(); cur = cur.right) { // 一直下到最左 while (cur != null) { stack.push(cur); cur = cur.left; } cur = stack.pop();// 每次出栈的就是中序遍历顺序的节点，然后在以下进行对应操作 res.add(cur.val); // 最后向右一步 } return res; } } ","date":"2023-08-08","objectID":"/leetcode/:3:4","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[105]从前序与中序遍历序列构造二叉树.java class Solution { public TreeNode buildTree(int[] preorder, int[] inorder) { HashMap\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e(); for (int i = 0; i \u003c inorder.length; i++) { map.put(inorder[i], i); } return buildTreeHelper(preorder, 0, preorder.length, inorder, 0, inorder.length, map); } /** * 指针左闭右开 * * @param preorder * @param p_start * @param p_end * @param inorder * @param i_start * @param i_end * @param map * @return */ private TreeNode buildTreeHelper(int[] preorder, int p_start, int p_end, int[] inorder, int i_start, int i_end, HashMap\u003cInteger, Integer\u003e map) { if (p_start == p_end) { return null; } int root_val = preorder[p_start]; TreeNode root = new TreeNode(root_val); int i_root_index = map.get(root_val); int leftNum = i_root_index - i_start; // preorder 分为 [p_start + 1,p_start + leftNum + 1) 和 [p_start + leftNum + 1, p_end]，前序根结点在左边 // inorder 分为 [i_start,i_root_index) 和 [i_root_index + 1,i_end)，少一个根结点 root.left = buildTreeHelper(preorder, p_start + 1, p_start + leftNum + 1, inorder, i_start, i_root_index, map); root.right = buildTreeHelper(preorder, p_start + 1 + leftNum, p_end, inorder, i_root_index + 1, i_end, map); return root; } ","date":"2023-08-08","objectID":"/leetcode/:3:5","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[114]二叉树展开为链表.java 给你二叉树的根结点 root ，请你将它展开为一个单链表： 展开后的单链表应该同样使用 TreeNode ，其中 right 子指针指向链表中下一个结点，而左子指针始终为 null 。 展开后的单链表应该与二叉树 先序遍历 顺序相同。 思路 把每个节点的左子树插入到该节点与右子树之间； 应用后序递归框架（左、右、根）。 代码 class Solution { public void flatten(TreeNode root) { // 1. 叶子节点 if (root == null) return; // 2. 递归 flatten TreeNode left = root.left; TreeNode right = root.right; flatten(left); flatten(right); // 3. 拼左 root.left = null; root.right = left; // 4. 拼右 TreeNode p = root; while (p.right != null) { p = p.right; } p.right = right; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:3:6","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[116]填充每个节点的下一个右侧节点指针.java 给定一个 完美二叉树 ，其所有叶子节点都在同一层，每个父节点都有两个子节点。 思路 把每个节点的左子树指向右子树； 值得注意的是，左子树的右子树也要指向右子树的左子树； 应用前序递归框架（根、左、右）。 代码 class Solution { public Node connect(Node root) { if (root == null) { return null; } connect(root.left, root.right); return root; } private void connect(Node left, Node right) { if (left == null || right == null) { return; } left.next = right; // 四个点，三个指向 connect(left.left, left.right); connect(left.right, right.left); connect(right.left, right.right); } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:3:7","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[701]二叉搜索树中的插入操作.java 给定二叉搜索树（BST）的根节点和要插入树中的值，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据 保证 ，新值和原始二叉搜索树中的任意节点值都不同 思路 这题的关键在于重置 搜索路径 上节点的左或右子树，如果不为空，则递归；如果为空，直接 new 一个新节点，这个节点不需要做任何处理，它是会被上一次递归所指向的。 代码 class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) { return new TreeNode(val); } if (val \u003e root.val) { root.right = insertIntoBST(root.right, val); } else { root.left = insertIntoBST(root.left, val); } return root; } } 复杂度 时间复杂度：平均情况 O(logn)，最坏情况 O(n) 空间复杂度：平均情况 O(logn)，最坏情况 O(n) ","date":"2023-08-08","objectID":"/leetcode/:3:8","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[450]删除二叉搜索树中的节点.java 给定一个二叉搜索树的根节点 root 和一个值 key，删除二叉搜索树中的 key 对应的节点，并保证二叉搜索树的性质不变。返回二叉搜索树（有可能被更新）的根节点的引用。 思路 首先，查找和删除这两步不能分开，因为删除还需要用到删除节点的上一个节点。 找到元素后，如果删除节点左右子树有空着的，只需要把另一棵子树移到当前位置即可，直接返回，这个节点不需要做任何处理，它是会被上一次递归所指向的。 最复杂的是要删除的节点左右子树都不为空的情况，可以用一个小技巧：找到要删除节点的下一个更大节点，它的位置在其右子树的最左叶子节点，设为 p 节点，用 p 节点的值覆盖要删除的节点的值，这时 p 节点的值出现了 2 次，然后转而去右子树删除 p 节点的值。 代码 class Solution { public TreeNode deleteNode(TreeNode root, int key) { if (root == null) { return null; } else if (root.val == key) { if (root.left == null || root.right == null) { return root.left == null ? root.right : root.left; } else { // 把右子树最左结点移到当前位置 TreeNode p = root.right; while (p.left != null) { p = p.left; } // 把要删除的值先覆盖，此时这个值出现 2 次， 去右子树删除多余的节点 root.val = p.val; root.right = deleteNode(root.right, p.val); } } else if (root.val \u003c key) { root.right = deleteNode(root.right, key); } else if (root.val \u003e key) { root.left = deleteNode(root.left, key); } return root; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:3:9","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[102]二叉树的层序遍历.java 给你一个二叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。 思路 构建一个队列，初始化时将根结点入队； 每个节点出队时，将它的子节点按先左后右的顺序入队； 只要队列非空就重复以上步骤。 代码 class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { List\u003cList\u003cInteger\u003e\u003e res = new LinkedList\u003c\u003e(); if (root == null) { return res; } Queue\u003cTreeNode\u003e q = new LinkedList\u003c\u003e(); // 核心数据结构 q.offer(root); // 将起点加入队列 while (!q.isEmpty()) { int size = q.size(); List\u003cInteger\u003e row = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c size; i++) { TreeNode cur = q.poll(); row.add(cur.val); if (cur.left != null) { q.offer(cur.left); } if (cur.right != null) { q.offer(cur.right); } } res.add(row); } return res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:3:10","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[752]打开转盘锁.java 你有一个带有四个圆形拨轮的转盘锁。每个拨轮都有10个数字： ‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’, ‘6’, ‘7’, ‘8’, ‘9’。每个拨轮可以自由旋转：例如把 ‘9’ 变为 ‘0’，‘0’ 变为 ‘9’ 。每次旋转都只能旋转一个拨轮的一位数字。 锁的初始数字为 ‘0000’ ，一个代表四个拨轮的数字的字符串。 列表 deadends 包含了一组死亡数字，一旦拨轮的数字和列表里的任何一个元素相同，这个锁将会被永久锁定，无法再被旋转。 字符串 target 代表可以解锁的数字，你需要给出解锁需要的最小旋转次数，如果无论如何不能解锁，返回 -1 。 思路 运用 BFS 框架，构建一个队列，每当一个字符串出队，就将改变 4 个密码位的 8 种情况入队，每判断一层，计数自增，当出队值等于目标值时返回计数，否则最终返回 -1； 针对字符串中的某一位操作，可以抽一个方法出来； 题目还给了一组死亡数字，当出队字符串包含其中时，就跳过，说明走到了死胡同； 当出队的字符串不是最终目标时，就把它加入死亡数字，这样可以防止重复访问，走入死循环。 代码 class Solution { public int openLock(String[] deadends, String target) { Queue\u003cString\u003e q = new LinkedList\u003c\u003e(); Set\u003cString\u003e set = new HashSet\u003c\u003e(); for (String deadend : deadends) { set.add(deadend); } q.offer(\"0000\"); int count = 0; while (!q.isEmpty()) { int size = q.size(); for (int i = 0; i \u003c size; i++) { String cur = q.poll(); if (cur.equals(target)) { return count; } if (set.contains(cur)) { continue; } set.add(cur); for (int j = 0; j \u003c 4; j++) { q.offer(operateOne(cur, j, 1)); q.offer(operateOne(cur, j, -1)); } } count++; } return -1; } /** * 改变字符串中的某一位 * * @param s 字符串 * @param index 哪一位 * @param change 改变多少 （对于转盘锁只能传入 +1 或 -1） * @return */ private String operateOne(String s, int index, int change) { char[] ch = s.toCharArray(); if (ch[index] == '9' \u0026\u0026 change == 1) { ch[index] = '0'; } else if (ch[index] == '0' \u0026\u0026 change == -1) { ch[index] = '9'; } else { ch[index] += change; } return new String(ch); } } 复杂度 因为数字的进制、转盘的位数都是常数， 时间复杂度：O(deadends.length) 空间复杂度：O(deadends.length) ","date":"2023-08-08","objectID":"/leetcode/:3:11","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[662]二叉树最大宽度.java //给定一个二叉树，编写一个函数来获取这个树的最大宽度。树的宽度是所有层中的最大宽度。这个二叉树与满二叉树（full binary tree）结构相同，但一些节点为空。 //// 每一层的宽度被定义为两个端点（该层最左和最右的非空节点，两端点间的null节点也计入长度）之间的长度。 //// 示例 1: //// //输入: //// 1// / // 3 2// / \\ // 5 3 9 ////输出: 4//解释: 最大值出现在树的第 3 层，宽度为 4 (5,3,null,9)。 class Solution { int maxWidth; Map\u003cInteger, Integer\u003e leftmostPositions; public int widthOfBinaryTree(TreeNode root) { maxWidth = 0; leftmostPositions = new HashMap\u003c\u003e(); getWidth(root, 0, 0); return maxWidth; } private void getWidth(TreeNode node, int depth, int position) { if (node == null) return; // 不存在该层，则初始化 leftmostPositions.computeIfAbsent(depth, key -\u003e position); maxWidth = Math.max(maxWidth, position - leftmostPositions.get(depth) + 1); // 当前节点的左右边界 getWidth(node.left, depth + 1, position * 2); getWidth(node.right, depth + 1, position * 2 + 1); } } ","date":"2023-08-08","objectID":"/leetcode/:3:12","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[199]二叉树的右视图.java 给定一个二叉树的 根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 class Solution { public List\u003cInteger\u003e rightSideView(TreeNode root) { List\u003cInteger\u003e result = new LinkedList\u003c\u003e(); if (root == null) return result; Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { int size = queue.size(); for (int i = 0; i \u003c size; i++) { TreeNode currentNode = queue.poll(); if (i == 0) result.add(currentNode.val); // 入队：先右后左；否则就取 i == size -1 也可以 if (currentNode.right != null) queue.offer(currentNode.right); if (currentNode.left != null) queue.offer(currentNode.left); } } return result; } } ","date":"2023-08-08","objectID":"/leetcode/:3:13","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[236]二叉树的最近公共祖先.java class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) { return root; } TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if (left == null) { return right; } if (right == null) { return left; } return root; } } ","date":"2023-08-08","objectID":"/leetcode/:3:14","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"滑动窗口 ","date":"2023-08-08","objectID":"/leetcode/:4:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"模版 /* 滑动窗口算法框架 */ void slidingWindow(String s, String t) { Map\u003cCharacter, Integer\u003e need = new HashMap\u003c\u003e(); Map\u003cCharacter, Integer\u003e window = new HashMap\u003c\u003e(); for (int i = 0; i \u003c t.length(); i++) { char c = t.charAt(i); need.put(c, need.getOrDefault(c, 0) + 1); } int left = 0, right = 0; int valid = 0; while (right \u003c s.length()) { // c 是将移入窗口的字符 char c = s.charAt(right++); // todo 进行窗口内数据的一系列更新 // todo 判断左侧窗口是否要收缩 while (window need shrink) { // d 是将移出窗口的字符 char d = s.charAt(left++); // todo 进行窗口内数据的一系列更新 } } } right：寻找可行解 left：试探最优解 ","date":"2023-08-08","objectID":"/leetcode/:4:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[3]无重复字符的最长子串.java 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。 思路 right：寻找可行解：只要无重复字符就前进，一旦出现重复字符就停止，记录坐标 left：试探最优解：出现重复字符时前进，直到重复字符消失 代码 class Solution { public int lengthOfLongestSubstring(String s) { if (s.length() == 0) { return 0; } Map\u003cCharacter, Integer\u003e map = new HashMap\u003c\u003e(); int left = 0, right = 0; int res = 1; while (right \u003c s.length()) { // c 是将移入窗口的字符，右移窗口 char c = s.charAt(right++); // 进行窗口内数据的一系列更新 map.put(c, map.getOrDefault(c, 0) + 1); // 判断左侧窗口是否要收缩 while (map.get(c) \u003e 1) { // d 是将移出窗口的字符，左移窗口 char d = s.charAt(left++); // 进行窗口内数据的一系列更新 map.put(d, map.get(d) - 1); } res = Math.max(res, right - left); } return res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:4:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[438]找到字符串中所有字母异位词.java 给定两个字符串 s 和 p，找到 s 中所有 p 的 异位词 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。 异位词 指字母相同，但排列不同的字符串。 思路 当窗口大小等于 p 的长度时，左右指针同时前进，寻找可行解。 代码 class Solution { public List\u003cInteger\u003e findAnagrams(String s, String p) { List\u003cInteger\u003e res = new ArrayList\u003c\u003e(); Map\u003cCharacter, Integer\u003e need = new HashMap\u003c\u003e(); Map\u003cCharacter, Integer\u003e window = new HashMap\u003c\u003e(); for (int i = 0; i \u003c p.length(); i++) { char c = p.charAt(i); need.put(c, need.getOrDefault(c, 0) + 1); } int left = 0, right = 0; int valid = 0; while (right \u003c s.length()) { // c 是将移入窗口的字符 char c = s.charAt(right++); // 进行窗口内数据的一系列更新 if (need.containsKey(c)) { window.put(c, window.getOrDefault(c, 0) + 1); if (window.get(c).equals(need.get(c))) { valid++; } } // 判断左侧窗口是否要收缩 if (right - left == p.length()) { if (valid == need.size()) { res.add(left); } // d 是将移出窗口的字符 char d = s.charAt(left++); // 进行窗口内数据的一系列更新 if (need.containsKey(d)) { if (window.get(d).equals(need.get(d))) { valid--; } window.put(d, window.get(d) - 1); } } } return res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:4:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"回溯 ","date":"2023-08-08","objectID":"/leetcode/:5:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[46]全排列.java 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 思路 定义结果集为全局变量； 核心是写出回溯函数，明确增添结果、退出递归的时机，向每一种可能的情况迈出一小步，进入下一次回溯，在最后还要归位； 正确的结果集：路径长度达到数字长度； 筛选条件：数字在路径中没有出现过； 对算法进行适当的剪枝，提高时间效率。 代码 class Solution { // 结果集 private List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e permute(int[] nums) { // add、remove 不会影响new ArrayList 的浅拷贝 backTrack(nums, new LinkedList\u003c\u003e()); return res; } private void backTrack(int[] nums, LinkedList\u003cInteger\u003e path) { // 增添结果、退出递归 if (path.size() == nums.length) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = 0; i \u003c nums.length; i++) { // 筛选 if (path.contains(nums[i])) { continue; } // 试探步 path.addLast(nums[i]); // 回溯 backTrack(nums, path); // 归位 path.removeLast(); } } } 复杂度 时间复杂度：O(n*n!) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:5:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[51]N 皇后.java n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。 每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 ‘Q’ 和 ‘.’ 分别代表了皇后和空位。 思路 定义结果集为全局变量； 核心是写出回溯函数，明确增添结果、退出递归的时机，向每一种通过筛选的情况迈出一小步，进入下一次回溯，在最后还要归位； 正确的结果集：第 8 行皇后符合要求； 筛选条件：任意两皇后不在同一行、同一列或同一斜线； 对算法进行适当的剪枝，提高时间效率。 代码 class Solution { // 结果集 private List\u003cList\u003cString\u003e\u003e res = new ArrayList\u003c\u003e(); public List\u003cList\u003cString\u003e\u003e solveNQueens(int n) { // 初始化棋盘 char[][] board = new char[n][n]; for (char[] line : board) { Arrays.fill(line, '.'); } backTrack(board, 0); return res; } private void backTrack(char[][] board, int row) { // 增添结果、退出递归 if (row == board.length) { List\u003cString\u003e list = new ArrayList\u003c\u003e(); for (char[] c : board) { list.add(String.copyValueOf(c)); } res.add(list); return; } for (int col = 0; col \u003c board[row].length; col++) { // 筛选 if (!isValid(board, row, col)) { continue; } // 试探步 board[row][col] = 'Q'; // 回溯 backTrack(board, row + 1); // 归位 board[row][col] = '.'; } } private boolean isValid(char[][] board, int row, int col) { int n = board.length; // 检查列是否有皇后冲突 for (int i = 0; i \u003c n; i++) { if (board[i][col] == 'Q') { return false; } } // 检查右上方是否有皇后冲突 for (int i = row - 1, j = col + 1; i \u003e= 0 \u0026\u0026 j \u003c n; i--, j++) { if (board[i][j] == 'Q') { return false; } } // 检查左上方是否有皇后冲突 for (int i = row - 1, j = col - 1; i \u003e= 0 \u0026\u0026 j \u003e= 0; i--, j--) { if (board[i][j] == 'Q') { return false; } } return true; } } 复杂度 时间复杂度：O(n!) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:5:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[22]括号生成.java 数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。 有效括号组合需满足：左括号必须以正确的顺序闭合。 思路 以左右括号剩余数为筛选、判断的标准； 正确的结果集：左右括号剩余数都为 0； 筛选条件：左右括号剩余数都为非负数，且右括号剩余数不小于左括号剩余数； 代码 class Solution { // 结果集 List\u003cString\u003e res = new LinkedList\u003c\u003e(); public List\u003cString\u003e generateParenthesis(int n) { String curStr = \"\"; // 结果演进的起点是空字符串，左右各有n个括号 backtrack(curStr, n, n); return res; } /** * 回溯法 * * @param str 路径 * @param left 左括号剩余数 * @param right 右括号剩余数 */ private void backtrack(String str, int left, int right) { //边界条件 base case if (left == 0 \u0026\u0026 right == 0) { res.add(str); return; } //减枝：左括号数必须大于等于右括号 if (left \u003c 0 || right \u003c 0 || left \u003e right) { return; } if (left \u003e 0) { backtrack(str + \"(\", left - 1, right); } if (right \u003e 0) { backtrack(str + \")\", left, right - 1); } } } 复杂度 时间复杂度： O(C_{2n}^n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:5:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"039 组合总和 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的数字可以无限制重复被选取。 代码 class Solution { // 结果集 List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e combinationSum(int[] candidates, int target) { if (candidates == null || candidates.length == 0) { return res; } Arrays.sort(candidates); backTrack(new ArrayList\u003c\u003e(), 0, 0, candidates, target); return res; } private void backTrack(List\u003cInteger\u003e path, int sum, int idx, int[] candidates, int target) { // base case if (sum == target) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = idx; i \u003c candidates.length; i++) { // 减枝 if (sum + candidates[i] \u003e target) { break; } path.add(candidates[i]); backTrack(path, sum + candidates[i], i, candidates, target); path.remove(path.size() - 1); } } } 复杂度 时间复杂度：O(n*2^n)实际运行情况远远小于这个上界可表示为 O(S)，其中 S 为所有可行解的长度之和 空间复杂度：O(target) ","date":"2023-08-08","objectID":"/leetcode/:5:4","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"040 组合总和Ⅱ 在 039 组合总和 的基础上，每个数字在每个组合中只能使用一次 代码 class Solution { // 结果集 Set\u003cList\u003cInteger\u003e\u003e res = new HashSet\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e combinationSum2(int[] candidates, int target) { if (candidates == null || candidates.length == 0) { return new ArrayList\u003c\u003e(res); } Arrays.sort(candidates); backTrack(new ArrayList\u003c\u003e(), 0, 0, candidates, target); // 返回值类型需要 Set 转 List return new ArrayList\u003c\u003e(res); } private void backTrack(List\u003cInteger\u003e path, int sum, int idx, int[] candidates, int target) { // base case if (sum == target) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = idx; i \u003c candidates.length; i++) { // 减枝 if (sum + candidates[i] \u003e target) { break; } path.add(candidates[i]); //回溯时用过的数字不能再用 backTrack(path, sum + candidates[i], i + 1, candidates, target); path.remove(path.size() - 1); } } } 复杂度 时间复杂度：O(n*2^n)实际运行情况远远小于这个上界可表示为 O(S)，其中 S 为所有可行解的长度之和 空间复杂度：O(target) ","date":"2023-08-08","objectID":"/leetcode/:5:5","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"046 全排列 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 代码 class Solution { // 结果集 List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e permute(int[] nums) { backTrack(nums, new LinkedList\u003c\u003e()); return res; } private void backTrack(int[] nums, LinkedList\u003cInteger\u003e path) { // 减枝 if (path.size() == nums.length) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = 0; i \u003c nums.length; i++) { // 筛选 if (path.contains(nums[i])) { continue; } path.addLast(nums[i]); backTrack(nums, path); path.removeLast(); } } } 复杂度 时间复杂度：O(n*n!) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:5:6","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"047 全排列Ⅱ 在 046 全排列 的基础上，nums可包含重复数字 代码 class Solution { // 结果集 Set\u003cList\u003cInteger\u003e\u003e res = new HashSet\u003c\u003e(); public List\u003cList\u003cInteger\u003e\u003e permuteUnique(int[] nums) { backTrack(nums, new LinkedList\u003c\u003e(), new boolean[nums.length]); return new ArrayList\u003c\u003e(res); } private void backTrack(int[] nums, LinkedList\u003cInteger\u003e path, boolean[] used) { // 减枝 if (path.size() == nums.length) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = 0; i \u003c nums.length; i++) { // 筛选 if (used[i]) { continue; } path.addLast(nums[i]); used[i] = true; backTrack(nums, path, used); path.removeLast(); used[i] = false; } } } 复杂度 时间复杂度：O(n*n!) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:5:7","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"动态规划 ","date":"2023-08-08","objectID":"/leetcode/:6:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[322]零钱兑换.java 给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 思路 自底向上考虑，要凑到 i 元钱有 dp[i] 种情况，新建 dp 数组； 初始值：dp[0] = 0; 状态转移方程：dp[i] = min(dp[i], 1 + dp[i - coin]); 返回值：dp[amount]; 剪枝：硬币金额大于目标金额。 代码 class Solution { public int coinChange(int[] coins, int amount) { int[] dp = new int[amount + 1]; Arrays.fill(dp, amount + 1); dp[0] = 0; for (int i = 0; i \u003c= amount; i++) { // 要凑到 i 元钱有 dp[i] 种情况 for (int coin : coins) { if (coin \u003e i) { continue; } dp[i] = Math.min(dp[i], 1 + dp[i - coin]); } } return (dp[amount] == amount + 1) ? -1 : dp[amount]; } } 复杂度 时间复杂度：O(n^2) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:6:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[53]最大子序和.java 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 思路 因为，某一位的最大子序和无非两种情况 继续累加前一个数的最大子序和 从当前数字重新累加 所以，得到状态转移方程： f(i)=max({f(i−1)+nums[i],nums[i]}) 代码 class Solution { public int maxSubArray(int[] nums) { int pre = 0, maxAns = nums[0]; for (int x : nums) { pre = Math.max(pre + x, x); maxAns = Math.max(maxAns, pre); } return maxAns; } } class Solution { public int maxSubArray(int[] nums) { int left = 0, right = 0, sum = 0, res = Integer.MIN_VALUE; while (right \u003c nums.length) { sum += nums[right++]; while (sum \u003c 0) { // d 是将移出窗口的字符 sum -= nums[left++]; } res = Math.max(sum, res); } int negative = Integer.MIN_VALUE; for (int num : nums) { negative = Math.max(num, negative); } return negative \u003c 0 ? negative : res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:6:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[300]最长递增子序列.java 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 思路 因为，某一位的最大子序长度无非两种情况 先在前面找出比当前数字更小的数字，找一个长度最长的自增 1 作为当前的最大自序长度 从当前数字重新累加（可以让初始化长度为1） 代码 class Solution { public int lengthOfLIS(int[] nums) { final int n = nums.length; int[] dp = new int[n]; // base case：dp 数组全都初始化为 1 Arrays.fill(dp, 1); int res = 0; for (int i = 0; i \u003c n; i++) { // 在前面找比当前数字更小的数字，以满足递增子序列的要求。 int cur = nums[i]; for (int j = 0; j \u003c i; j++) { if (nums[j] \u003c cur) dp[i] = Math.max(dp[i], dp[j] + 1); } // 更新最大值作为返回结果 res = Math.max(res, dp[i]); } return res; } } 复杂度 时间复杂度：O(n^2) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:6:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"栈和队列 ","date":"2023-08-08","objectID":"/leetcode/:7:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[225]用队列实现栈.java 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。 思路 用一个全局变量记录栈顶。 入栈、判空不做处理 额外处理出栈的 pop 操作，把队列中前 n-1 个元素出队再入队，返回末尾元素。 代码 class MyStack { Queue\u003cInteger\u003e queue; int top; /** * Initialize your data structure here. */ public MyStack() { queue = new LinkedList\u003c\u003e(); top = 0; } /** * Push element x onto stack. */ public void push(int x) { queue.offer(x); top = x; } /** * Removes the element on top of the stack and returns that element. */ public int pop() { int size = queue.size(); for (int i = 0; i \u003c size - 1; i++) { top = queue.peek(); queue.offer(queue.poll()); } return queue.poll(); } /** * Get the top element. */ public int top() { return top; } /** * Returns whether the stack is empty. */ public boolean empty() { return queue.isEmpty(); } } ","date":"2023-08-08","objectID":"/leetcode/:7:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[232]用栈实现队列.java 请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty） 思路 建立 2 个栈，元素先入 s1，再入 s2，这样 s2 的栈顶就相当于队尾了。 把关键操作放在 peek 处，在 pop 之前，先调用 peek ，就能确保不会出错了。 代码 class MyQueue { private Stack\u003cInteger\u003e s1, s2; /** * Initialize your data structure here. */ public MyQueue() { s1 = new Stack\u003c\u003e(); s2 = new Stack\u003c\u003e(); } /** * Push element x to the back of queue. */ public void push(int x) { s1.push(x); } /** * Removes the element from in front of queue and returns that element. */ public int pop() { // 将 s1 元素转移至 s2 peek(); return s2.pop(); } /** * Get the front element. */ public int peek() { if (s2.isEmpty()) { while (!s1.isEmpty()) { s2.push(s1.pop()); } } return s2.peek(); } /** * Returns whether the queue is empty. */ public boolean empty() { return s1.isEmpty() \u0026\u0026 s2.isEmpty(); } } ","date":"2023-08-08","objectID":"/leetcode/:7:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[496]下一个更大元素 I.java 给你两个 没有重复元素 的数组 nums1 和 nums2 ，其中nums1 是 nums2 的子集。 请你找出 nums1 中每个元素在 nums2 中的下一个比其大的值。 nums1 中数字 x 的下一个更大元素是指 x 在 nums2 中对应位置的右边的第一个比 x 大的元素。如果不存在，对应位置输出 -1 。 思路 从后往前遍历 nums2，用栈记录右侧大于当前数字的值； 以当前数字为 key，下一个更大元素为 value，建立一个 HashMap 存储； 遍历 nums1 ,获取每一个 key 的 value。 代码 class Solution { public int[] nextGreaterElement(int[] nums1, int[] nums2) { Map\u003cInteger, Integer\u003e map = new HashMap\u003c\u003e(); Stack\u003cInteger\u003e stack = new Stack\u003c\u003e(); // 从后往前遍历 for (int i = nums2.length - 1; i \u003e= 0; i--) { while (!stack.isEmpty() \u0026\u0026 nums2[i] \u003e= stack.peek()) { // 栈顶元素即将被 nums2[i] 挡住 stack.pop(); } int nextGreaterNum = stack.isEmpty() ? -1 : stack.peek(); map.put(nums2[i], nextGreaterNum); stack.push(nums2[i]); } int[] res = new int[nums1.length]; for (int i = 0; i \u003c nums1.length; i++) { res[i] = map.get(nums1[i]); } return res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:7:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[503]下一个更大元素 II.java 给定一个循环数组（最后一个元素的下一个元素是数组的第一个元素），输出每个元素的下一个更大元素。数字 x 的下一个更大的元素是按数组遍历顺序，这个数字之后的第一个比它更大的数，这意味着你应该循环地搜索它的下一个更大的数。如果不存在，则输出 -1。 思路 从后往前遍历 nums，用栈记录右侧大于当前数字的值 因为数组是循环数组，所以在赋值 -1 时还要去坐标左边找找，如果还找不到更大元素，那么就返回 -1，否则就返回第一个更大元素。 代码 class Solution { public int[] nextGreaterElements(int[] nums) { int[] res = new int[nums.length]; Stack\u003cInteger\u003e stack = new Stack\u003c\u003e(); // 从后往前遍历 for (int i = nums.length - 1; i \u003e= 0; i--) { while (!stack.isEmpty() \u0026\u0026 nums[i] \u003e= stack.peek()) { // 栈顶元素即将被 nums[i] 挡住 stack.pop(); } if (stack.isEmpty()) { res[i] = -1; // 后面找不到了，去前面找找看 for (int j = 0; j \u003c i; j++) { if (nums[j] \u003e nums[i]) { res[i] = nums[j]; break; } } } else { res[i] = stack.peek(); } stack.push(nums[i]); } return res; } } class Solution { public int[] nextGreaterElements(int[] nums) { int n = nums.length; int[] res = new int[n]; Arrays.fill(res, -1); // 单调栈中存放下标 Stack\u003cInteger\u003e stack = new Stack\u003c\u003e(); for (int i = 0; i \u003c n * 2; i++) { while (!stack.isEmpty() \u0026\u0026 nums[stack.peek()] \u003c nums[i % n]) { // 出栈的值的下一个更大元素即为 nums[i] res[stack.pop()] = nums[i % n]; } if (i \u003c n) { stack.push(i); } } return res; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:7:4","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[1047]删除字符串中的所有相邻重复项.java //给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。 //// 在 S 上反复执行重复项删除操作，直到无法继续删除。 //// 在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。 //// //// 示例： //// 输入：“abbaca”//输出：“ca”//解释：//例如，在 “abbaca” 中，我们可以删除 “bb” 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 “aaca”，其中又//只有 “aa” 可以执行重复项删除操作，所以最后的字符串为 “ca”。 class Solution { public String removeDuplicates(String s) { char[] stack = new char[s.length()]; int index = 0; for (char currentChar : s.toCharArray()) { if (index \u003e 0 \u0026\u0026 stack[index - 1] == currentChar) { index--; } else { stack[index++] = currentChar; } } return new String(stack, 0, index); } } ","date":"2023-08-08","objectID":"/leetcode/:7:5","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"其他 ","date":"2023-08-08","objectID":"/leetcode/:8:0","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"并查集（Union-Find）算法：[990]等式方程的可满足性.java 给定一个由表示变量之间关系的字符串方程组成的数组，每个字符串方程 equations[i] 的长度为 4，并采用两种不同的形式之一：“a==b” 或 “a!=b”。在这里，a 和 b 是小写字母（不一定不同），表示单字母变量名。 只有当可以将整数分配给变量名，以便满足所有给定的方程时才返回 true，否则返回 false。 思路 将方程分成等式和不等式两类； 初始化「图」； 根据等式连接两两节点； 判断不等式是否成立。 代码 class Solution { // 节点 x 的节点是 parent[x] private int[] parent; // 新增一个数组记录树的“重量” private int[] size; public boolean equationsPossible(String[] equations) { // 1. 将方程分成等式和不等式两类 List\u003cString\u003e equals = new LinkedList\u003c\u003e(); List\u003cString\u003e unEquals = new LinkedList\u003c\u003e(); for (String equation : equations) { char c = equation.charAt(1); if (c == '=') { equals.add(equation); } else if (c == '!') { unEquals.add(equation); } } // 2. 初始化「图」 parent = new int[26]; size = new int[26]; for (int i = 0; i \u003c 26; i++) { // 父节点指针初始指向自己 parent[i] = i; // 重量应该初始化 1 size[i] = 1; } // 3. 根据等式连接两两节点 for (String equal : equals) { int a = equal.charAt(0) - 'a'; int b = equal.charAt(3) - 'a'; union(a, b); } // 4. 判断不等式是否成立 for (String unEqual : unEquals) { int a = unEqual.charAt(0) - 'a'; int b = unEqual.charAt(3) - 'a'; int rootA = find(a); int rootB = find(b); if (rootA == rootB) { return false; } } return true; } /* 将 p 和 q 连接 */ private void union(int p, int q) { int rootP = find(p); int rootQ = find(q); if (rootP == rootQ) { // 二者已连接 return; } // 将两棵树合并为一棵，小树接到大树下面 if (size[rootP] \u003e size[rootQ]) { parent[rootQ] = rootP; size[rootP] += size[rootQ]; } else { parent[rootP] = rootQ; size[rootQ] += size[rootP]; } } /* 返回某个节点 x 的根节点 */ private int find(int x) { // 根节点的 parent[x] == x while (parent[x] != x) { parent[x] = parent[parent[x]]; x = parent[x]; } return x; } } 复杂度 时间复杂度：O(n) 空间复杂度：O(n) ","date":"2023-08-08","objectID":"/leetcode/:8:1","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"[146]LRU 缓存机制.java 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制 。 实现 LRUCache 类： LRUCache(int capacity) 以正整数作为容量 capacity 初始化 LRU 缓存 int get(int key) 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。 void put(int key, int value) 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字-值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。 思路 需要一个有时序的 Map 来实现算法，在 Java 语言中可以使用 LinkedHashMap； LRU 的核心是 makeRecently 方法，它把一对 key-value 放到最新的位置，也就是链表的队尾。 get 方法直接去 Map 中找，并调用 makeRecently 方法； put 方法需要考虑缓存大小超出给定容量的情况，若 key 是第一次出现，则需要删除最久未使用的 key-value 对，并添加新的 key-value 对；若原本就存在 key，则更新它的 value，并调用 makeRecently 方法。 代码 class LRUCache { int capacity; LinkedHashMap\u003cInteger, Integer\u003e cache; public LRUCache(int capacity) { this.capacity = capacity; cache = new LinkedHashMap\u003c\u003e(); } public int get(int key) { if (!cache.containsKey(key)) { return -1; } return makeRecently(key); } public void put(int key, int value) { // 容量已满，需要移除 if (cache.size() == capacity \u0026\u0026 !cache.containsKey(key)) { int oldestKey = cache.keySet().iterator().next(); cache.remove(oldestKey); } cache.put(key, value); makeRecently(key); } private int makeRecently(int key) { int val = cache.get(key); // 删除 key，重新插入到队尾 cache.remove(key); cache.put(key, val); return val; } } 复杂度 时间复杂度：O(1) 空间复杂度：O(capacity) ","date":"2023-08-08","objectID":"/leetcode/:8:2","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["practice"],"content":"050 Pow(x,n) int转换为long Java 代码中 int32 变量 n \\in [-2147483648, 2147483647]，因此当 n = -2147483648 时执行 n = -n会因越界而赋值出错。解决方法是先将 n 存入 long 变量 b ，后面用 b 操作即可。 代码（迭代） class Solution { public double myPow(double x, int n) { if (x == 0.0f) return 0.0d; //防止n=-n赋值越界 long b = n; // n=0情况返回1 double res = 1.0; if (b \u003c 0) { x = 1 / x; b = -b; } while (b \u003e 0) { if ((b \u0026 1) == 1) { //奇数 res *= x; } x *= x; //按位右移，相当于除以2 b \u003e\u003e= 1; } return res; } } 复杂度 时间复杂度：O(logn) 空间复杂度：O(1) ","date":"2023-08-08","objectID":"/leetcode/:8:3","tags":["LeetCode","DS"],"title":"LeetCode 大全","uri":"/leetcode/"},{"categories":["学术研究"],"content":"咱就是说最后没报名","date":"2023-08-08","objectID":"/kaggle/","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"导学 ","date":"2023-08-08","objectID":"/kaggle/:1:0","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"赛题 结构化(\u003e50%)： 规整，纬度固定 自然语言 语音识别 计算机视觉 ","date":"2023-08-08","objectID":"/kaggle/:1:1","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"时间轴 entry deadline： 最晚参赛时间 team merger deadline： 最晚组队时间 final submission deadline： 最终提交截止时间 ","date":"2023-08-08","objectID":"/kaggle/:1:2","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"结构化 ","date":"2023-08-08","objectID":"/kaggle/:2:0","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"树模型 适用场景： 类别字段较多、聚合特征较多 XGBoost(2015) https：//xgboost.readthedocs.io/en/latest 优点：提出时间较早的高阶树模型，精度较好 缺点：训练时间较长，对类别特征支持不友好 接口：sklearn接口和原生接口 LighGBM(2018) https：//lightgbm.readthedocs.io/en/latest 优点：对节点分裂进行改进，速度有一定提升 缺点：加入了随机性，精度和稳定性有所下降 接口：sklearn接口和原生接口 CatBoost(2019) https：//catboost.ai/ 优点：支持类别和字符串分裂 缺点：容易过拟合 接口：sklearn接口和原生接口 ","date":"2023-08-08","objectID":"/kaggle/:2:1","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"数据集Dataset training set test set：一般不能用来训练；可能分为AB榜单 validation set 只要有反馈，就有过拟合 ","date":"2023-08-08","objectID":"/kaggle/:2:2","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"模型集成 vote 投票 blend 加权 stacking 交叉验证中对模型进行多折训练 深度学习模型集成 dropout 随机删减网络结构 snapshot 集成多个局部最优 ","date":"2023-08-08","objectID":"/kaggle/:2:3","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["学术研究"],"content":"特征工程 决定了模型的精度上限 类别特征 Onehot：one-of-k，100、010、001 增加数据维度，一般适用于取值个数小于10 LabelEncoder：标号，不增加维度，默认字典顺序 Ordinal Encoding：可以手动给定编号 BinaryEncoder：二进制编码（压缩版 Onehot） Frequency/Count Encoding：计数（训练集和测试集取值需要一致） Mean/Target Encoding：帮助模型快速收敛，但很容易过拟合 数值特征 容易出现异常值和离群点 Round：取整（防止模型去学习细枝末节的规律） Box/Bins：分类 日期字段 需要特殊处理（登录日志、消费时间） ","date":"2023-08-08","objectID":"/kaggle/:2:4","tags":["大数据"],"title":"Kaggle","uri":"/kaggle/"},{"categories":["interview"],"content":"技术选型 mongodb：增删改查 es：大数据搜索 lucene：分布式搜索 ","date":"2023-08-08","objectID":"/elasticsearch/:0:1","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"倒排索引 根据分词找到文档id，根据文档id找到文档。 索引：帮助快速搜索，以数据结构为载体，以文件的形式落地。 切词：word segmentation 规范化：normalization 去重：distinct 字典序：sorted ","date":"2023-08-08","objectID":"/elasticsearch/:0:2","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"日志采集 ELK LogStash 筛选 ElasticSearch 存储 Kibana 可视化 ","date":"2023-08-08","objectID":"/elasticsearch/:0:3","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"1. Elasticsearch 和传统数据库有什么区别？ 传统数据库通常是关系型数据库，而 Elasticsearch 是一个非关系型数据库。传统数据库主要用于存储结构化数据，而 Elasticsearch 主要用于存储非结构化数据。传统数据库的查询通常是基于 SQL 的，而 Elasticsearch 的查询是基于 JSON 的。 ","date":"2023-08-08","objectID":"/elasticsearch/:0:4","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"2. Elasticsearch 的数据结构是怎样的？ Elasticsearch 的数据结构是基于文档和索引的。每个文档代表一个数据实体，每个索引代表一个数据集合。每个文档可以包含多个字段，每个字段可以是不同的数据类型，例如文本、数字、日期等等。 ","date":"2023-08-08","objectID":"/elasticsearch/:0:5","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"3. Elasticsearch 的查询语言是什么？ Elasticsearch 的查询语言是基于 JSON 的查询语言。它可以使用各种查询类型，例如 match、term、range、bool 等等，来实现各种复杂的查询操作。 ","date":"2023-08-08","objectID":"/elasticsearch/:0:6","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"4. 相关代码 // 创建一个 Elasticsearch 客户端对象 RestHighLevelClient client = new RestHighLevelClient( RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\")) ); // 执行一个聚合操作 SearchRequest request = new SearchRequest(\"my_index\"); SearchSourceBuilder builder = new SearchSourceBuilder(); builder.aggregation(AggregationBuilders.terms(\"by_user\").field(\"user\")); request.source(builder); SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 解析聚合结果 Terms byUser = response.getAggregations().get(\"by_user\"); for (Terms.Bucket bucket : byUser.getBuckets()) { String user = bucket.getKeyAsString(); long count = bucket.getDocCount(); System.out.println(user + \": \" + count); } // ------------------------------------------------------------------ // 构建一个查询条件 SearchRequest request = new SearchRequest(\"my_index\"); SearchSourceBuilder builder = new SearchSourceBuilder(); builder.query(QueryBuilders.matchQuery(\"message\", \"hello\")); request.source(builder); // 执行查询操作 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 解析查询结果 SearchHits hits = response.getHits(); for (SearchHit hit : hits) { String id = hit.getId(); String message = hit.getSourceAsMap().get(\"message\").toString(); System.out.println(id + \": \" + message); } // 关闭 Elasticsearch 客户端对象 client.close(); 创建一个 Elasticsearch 客户端对象 RestHighLevelClient 创建索引 CreateIndexRequest 添加一个文档 IndexRequest 更新一个文档 UpdateRequest 删除一个文档 DeleteRequest 搜索文档 SearchRequest 查询条件 SearchSourceBuilder 查询结果 SearchHits ","date":"2023-08-08","objectID":"/elasticsearch/:0:7","tags":["ElasticSearch"],"title":"🚩ElasticSearch","uri":"/elasticsearch/"},{"categories":["interview"],"content":"Tomcat 定义 Tomcat = HTTP 服务器 + Servlet 容器，是一个Web容器 HTTP服务器：处理HTTP请求并响应结果 Servlet容器：将请求转发到具体的Servlet，用来加载和管理业务类 ","date":"2023-08-08","objectID":"/tomcat/:0:1","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["interview"],"content":"访问url的过程 ","date":"2023-08-08","objectID":"/tomcat/:0:2","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["interview"],"content":"Servlet Servlet指的是任何实现了 Servlet 接口的类，主要用于处理客户端传来的 HTTP 请求，并返回一个响应。 // 几乎所有的Java Web 框架（如Spring） 都是基于Servlet的封装 public interface Servlet { void init(ServletConfig config) throws ServletException; ServletConfig getServletConfig(); // 实现业务逻辑 void service(ServletRequest req,ServletRespons res) throws ServletException, IOException; String getServletInfo(); void destroy(); } ","date":"2023-08-08","objectID":"/tomcat/:0:3","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["interview"],"content":"Tomcat 总体架构 Tomcat 要实现2个核心功能： 处理 Socket 连接，转化 字节流-\u003eRequest-\u003eResponse-\u003e字节流 加载和管理Servlet，处理 Request请求 连接器的作用：对Servlet容器通过ProtocalHandler屏蔽了协议和IO模型，使得容器获取到的都是一个标准的ServletRequest对象 连接器的3个高内聚的功能：Endpoint``Processo``Adapter Endpoint ：网络通信 socket（TCP/IP） ⬇️字节流⬇️ Processor：应用层协议解析（HTTP) ⬇️Tomcat Request⬇️ Adapter：适配器模式转化 request 和 response ⬇️ServletRequest⬇️ 容器 Tomcat中有4种容器，它们是父子关系，从大到小如下： Engine：引擎 Host：虚拟主机/站点【域名】 Context：Web应用程序【Web应用路径】 Wrapper： Servlet【Servlet映射的路径】，与一个url请求相对应 Mapper组件：将用户请求的URL定位到一个Servlet，组件中保存了容器组件与访问路径的映射关系 ","date":"2023-08-08","objectID":"/tomcat/:0:4","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["interview"],"content":"Java 类加载器 引导类加载器 bootstrap：只有它是C++编写，只加载包名为 java、javax、sun等开头的核心类库 JAVA_HOME/jre/lib 扩展类加载器 ExtClassLoader：派生于 ClassLoader 类 JAVA_HOME/jre/lib/ext 应用类加载器 AppClassLoader CLASSPATH 或 java.class.path 当解析一个类型到另一个类型的引用的时候，JVM 需要保证这两个类型的类加载器是相同的。 类加载过程 加载 Loading：.class 文件 -\u003e 类的二进制字节流，生成一个代表这个类的java.lang.Class 对象，作为方法区这个类的数据访问入口。 链接 Linking： 验证 Verify：保证被加载类的正确性，不会危害虚拟机本身。（以 CAFEBABE 开头） 准备 Prepare：为变量分配初始值（零），不包括final static 修饰的常量，它在编译时就显式初始化了，也不包括实例变量分配初始化，它和对象一起分配到堆中【给类变量默认赋值】 解析 Resolve：符号引用 -\u003e 直接引用 初始化 Initialization：执行类构造器方法\u003cclinit\u003e() ，【给类变量显式赋值，即静态代码块赋值】 口诀：父静子静（初始化），父代父构，子代子构 使用：JVM 从入口方法开始执行 卸载：销毁 class 对象 ","date":"2023-08-08","objectID":"/tomcat/:0:5","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["interview"],"content":"🌟 双亲委派机制 双亲委派机制：加载某个类时，优先交给上级类加载器加载。父类加载器无法完成加载才会还给子类去加载，例如自定义的String类永远无法执行，保证了系统的安全性，这就是沙箱安全机制。 避免类的重复加载 保护程序安全，防止核心API被随意篡改 Tomcat 打破双亲委派机制？ 目的：优先加载 Web 应用目录下的类。 原因：若使用JVM默认的AppClassLoader加载Web应用，AppClassLoader只能加载一个Servlet类，在加载第二个同名Servlet类时，AppClassLoader会返回第一个Servlet类的Class实例。 因为在AppClassLoader眼里，同名Servlet类只能被加载一次。 方法：继承ClassLoader抽象类，并重写它的loadClass方法。（因为ClassLoader的默认实现是双亲委派） Tomcat 类加载器的层次结构 Web应用之间的类如何隔离？ 自定义 WebAppClassLoader 类加载器，为每个Web应用创建一个实例。 Web应用之间的类如何共享库类？ 设计 SharedClassLoader 类加载器作为 WebAppClassLoader 类加载器的父类，专门来加载 Web 应用之间共享的类，子类加载不到的类会委托父加载器去加载。 Tomcat本身的类与Web应用的类如何隔离？ 设计 CatalinaClassLoader 类加载器，专门加载 Tomcat 自身的类。 Tomcat本身的类与Web应用的类如何隔共享数据？ 同理，设计 CommonClassLoader 作为CatalinaClassLoader 和SharedClassLoader 的父类。 ","date":"2023-08-08","objectID":"/tomcat/:0:6","tags":["Java"],"title":"🚩Tomcat","uri":"/tomcat/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/rpc_draft/","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"Socket 在java.net包中，有两个常用类： Socket：用于客户端 ServerSocket：用于服务端 Socket 网络通信过程分为4步： 建立服务端，监听客户端请求 客户端请求，建立连接 传递数据 关闭资源 具体到服务端： 创建 ServerSocket 对象并且绑定ip+port server.bind(new InetSocketAddress(ip,port)) 监听请求 accept() 连接建立后，通过输入流读取客户端发送到请求信息 通过输出流向客户端发送响应信息 关闭资源 具体到客户端： 创建 Socket 对象并连接服务器 socket.connect(inetSocketAddress) 连接建立后，通过输出流向服务器端发送请求信息 通过输入流获取服务器响应信息 关闭资源 ","date":"2023-08-08","objectID":"/rpc_draft/:1:0","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"Serailizable ","date":"2023-08-08","objectID":"/rpc_draft/:2:0","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"1. Serializable 接口 ","date":"2023-08-08","objectID":"/rpc_draft/:2:1","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"2. transient 关键字 ","date":"2023-08-08","objectID":"/rpc_draft/:2:2","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"3. 各种序列化 hessian2序列化：dubbo rpc默认启用的序列化方式，源自 hessian lite json序列化：可读性强 java序列化：性能不好 Kryo序列化：专门针对Java语言，性能非常好，推荐面向生产环境 ProtoBuf/ProtoStuff：跨语言序列化 ","date":"2023-08-08","objectID":"/rpc_draft/:2:3","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"RPC ","date":"2023-08-08","objectID":"/rpc_draft/:3:0","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"1. 基础 ","date":"2023-08-08","objectID":"/rpc_draft/:3:1","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"2. 静态代理和动态代理 ","date":"2023-08-08","objectID":"/rpc_draft/:3:2","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"3. 多个PRC服务如何保证一致性？ TCC+MQ 保证最终一致性 TCC：Try-Confirm-Commit MQ：consumer的重试机制 ","date":"2023-08-08","objectID":"/rpc_draft/:3:3","tags":["Network"],"title":"RPC 杂记","uri":"/rpc_draft/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/java_draft/","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["draft"],"content":"11.1 Lambda ","date":"2023-08-08","objectID":"/java_draft/:0:1","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["draft"],"content":"11.2 Stream ","date":"2023-08-08","objectID":"/java_draft/:0:2","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["draft"],"content":"11.3 设计原则和设计模式 反模式：死用模式，都是反模式 ","date":"2023-08-08","objectID":"/java_draft/:0:3","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["draft"],"content":"11.4 单元测试 ","date":"2023-08-08","objectID":"/java_draft/:0:4","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["draft"],"content":"11.5 Guava ","date":"2023-08-08","objectID":"/java_draft/:0:5","tags":["Java"],"title":"Java 杂记","uri":"/java_draft/"},{"categories":["读书笔记"],"content":"一、基础知识 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:0","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"1. 函数式编程 函数式编程中的函数的主要意思是“把函数作为一等值”，不过它也常常隐含着第二层意思，即“执行时在元素之间无互动”。 引用透明性——“没有可感知的副作用” 不改变对调用者可见的变量。返回的结果基于参数却没有修改任何一个传入的参数 不进行I/O 不抛出异常 选择使用引用透明的函数 也允许函数内部执行一些非函数式的操作，只要这些操作的结果不会暴露给系统中的其他部分。作为函数式的程序，你的函数或方法调用的库函数如果有副作用，你必须设法隐藏它们的非函数式行为，否则就不能调用这些方法。 被称为“函数式”的函数或方法都只能修改本地变量。除此之外，它引用的对象都应该是不可修改的对象。 要被称为函数式，函数或者方法不应该抛出任何异常。旦抛出异常，就意味着结果被终止了；不再像我们之前讨论的黑盒模式那样，由return返回一个恰当的结果值。 —— 请使用Optional类型 Optional类：它是一个容器对象，可以包含，也可以不包含一个值。Optional中有方法来明确处理值不存在的情况，这样就可以避免NullPointer异常了。 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:1","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"★2. lambda表达式：行为参数化（灵活、简洁） 函数式接口（用@FunctionalInterface标记）与匿名内部类： 函数式接口（加前缀：Int、Double、Long） Predicate： T -\u003e boolean （谓词） Consumer： T -\u003e void Function： T,R -\u003e R Supplier：() -\u003e R 范型T只能绑定到引用类型 闭包：闭包就是一个函数的实例，且它可以无限制地访问那个函数的非本地变量。现在，Java 8的Lambda和匿名类可以做类似于闭包的事情：它们可以作为参数传递给方法，并且可以访问其作用域之外的变量。但有一个限制：它们不能修改定义Lambda的方法的局部变量的内容。这些变量必须是隐式最终的。可以认为Lambda是对值封闭，而不是对变量封闭。 方法引用：根据已有的方法实现来创建Lambda表达式 指向静态方法 指向任意类型实例方法 指向现有对象的实例方法 构造函数引用：ClassName :: new 比较器链： ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:2","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"二、函数式数据处理 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:2:0","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"★1. 流 中间操作：返回stream类型 filter：排除元素，Predicate，T -\u003e boolean map：提取信息，Function\u003cT,R\u003e, T -\u003e R limit：截断流（有状态-有界） skip：与limit互补（有状态-有界） sorted：排序，Comparator, (T,T) -\u003e int（有状态-无界） distinct：去重（有状态-无界） flatMap：让你把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流 终端操作：返回非stream类型 collect：转换成其他格式（返回集合） count：计数（返回long） forEach：遍历（返回void） （以下都是短路求值） anyMatch：至少匹配一个（返回boolean） allMatch：都匹配（返回boolean） noneMatch：都不匹配（返回boolean） findAny：找到任意（返回Optional类） findFirst：找到第一个，在并行上限制更多，如果不关心返回的元素是哪个，请使用findAny。（返回Optional类） reduce： BinaryOperator ，(T, T) -\u003e T （返回Optional类）（有状态-有界） 流只能消费一次。 1-1. 原始类型流特化 Java 8引入了三个原始类型特化流接口来解决这个问题：IntStream、DoubleStream和LongStream，分别将流中的元素特化为int、long和double，从而避免了暗含的装箱成本。要记住的是，这些特化的原因并不在于流的复杂性，而是装箱造成的复杂性——即类似int和Integer之间的效率差异。 1-2. 映射到数据流 转换回对象流 //将Stream转换为数值流 IntStream intStream = menu.stream().mapToInt(Dish::getCalories); //将数值流转换为Stream Stream\u003cInteger\u003e stream = intStream.boxed(); 数值范围 range是不包含结束值的，而rangeClosed则包含结束值。 生成勾股数 Stream\u003cint[]\u003e pythagoreanTriples = IntStream.rangeClosed(1, 100).boxed() .flatMap(a -\u003e IntStream.rangeClosed(a, 100) .filter(b -\u003e Math.sqrt(a*a + b*b) % 1 == 0) .mapToObj(b -\u003e new int[]{a, b, (int)Math.sqrt(a * a + b * b)}) ); Stream\u003cdouble[]\u003e pythagoreanTriples2 = IntStream.rangeClosed(1, 100).boxed() .flatMap(a -\u003e IntStream.rangeClosed(a, 100) .mapToObj( b -\u003e new double[]{a, b, Math.sqrt(a*a + b*b)}) .filter(t -\u003e t[2] % 1 == 0)); 1-3. 无限流(斐波那契数列) iterate：使用iterate的方法则是纯粹不变的：它没有修改现有状态，但在每次迭代时会创建新的元组 Stream.iterate(new int[]{0, 1}, t -\u003e new int[]{t[1],t[0] + t[1]}) .limit(10) .map(t -\u003e t[0]) .forEach(System.out::println); //这段代码将生成斐波纳契数列：0, 1, 1, 2, 3, 5, 8, 13, 21, 34… generate：getAsInt()在调用时会改变对象的状态，由此在每次调用时产生新的值 IntSupplier fib = new IntSupplier(){ private int previous = 0; private int current = 1; public int getAsInt(){ int oldPrevious = this.previous; int nextValue = this.previous + this.current; this.previous = this.current; this.current = nextValue; return oldPrevious; } }; IntStream.generate(fib).limit(10).forEach(System.out::println); 1-4. reduce 1-5. 收集器 Collector接口 //T是流中要收集的项目的泛型。 //A是累加器的类型，累加器是在收集过程中用于累积部分结果的对象。 //R是收集操作得到的对象（通常但并不一定是集合）的类型。 public interface Collector\u003cT, A, R\u003e { Supplier\u003cA\u003e supplier(); BiConsumer\u003cA, T\u003e accumulator(); Function\u003cA, R\u003e finisher(); BinaryOperator\u003cA\u003e combiner(); Set\u003cCharacteristics\u003e characteristics(); } 自己实现收集器 List\u003cDish\u003e dishes = menuStream.collect(new ToListCollector\u003cDish\u003e()); ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:2:1","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"★2. 用 Optional 取代 null null的检查只会掩盖问题，并未真正地修复问题 //1. 声明一个空的Optional Optional\u003cCar\u003e optCar = Optional.empty(); //2. 依据一个非空值创建Optional //如果car是一个null，这段代码会立即抛出一个NullPointerException Optional\u003cCar\u003e optCar = Optional.of(car); //3. 可接受null的Optional //如果car是null，那么得到的Optional对象就是个空对象 Optional\u003cCar\u003e optCar = Optional.ofNullable(car); public String getCarInsuranceName(Optional\u003cPerson\u003e person) { return person.flatMap(Person::getCar) .flatMap(Car::getInsurance) .map(Insurance::getName) .orElse(\"Unknown\"); //如果Optional的结果值为空，设置默认值 } Optional 通过类型系统让你的域模型中隐藏的知识显式地体现在你的代码中，换句话说，你永远都不应该忘记语言的首要功能就是沟通，即使对程序设计语言而言也没有什么不同。声明方法接受一个Optional参数，或者将结果作为Optional类型返回，让你的同事或者未来你方法的使用者，很清楚地知道它可以接受空值，或者它可能返回一个空值。 2-1. Optional 的方法 get()：如果变量存在，它直接返回封装的变量值，否则就抛出一个NoSuchElementException异常 orElse(T other)：允许你在Optional对象不包含值时提供一个默认值 orElseGet(Supplier\u003c? extends T\u003e other)：是orElse方法的延迟调用版，Supplier 方法只有在Optional对象不含值时才执行调用。如果创建默认值是件耗时费力的工作，你应该考虑采用这种方式（借此提升程序的性能），或者你需要非常确定某个方法仅在Optional为空时才进行调用，也可以考虑该方式（这种情况有严格的限制条件）。 orElseThrow(Supplier\u003c? extends X\u003e exceptionSupplier)：自定义抛出异常 ifPresent(Consumer\u003c? super T\u003e)：能在变量值存在时执行一个作为参数传入的方法，否则就不进行任何操作。 不推荐使用基础类型的Optional，因为基础类型的Optional不支持map、flatMap以及filter方法，而这些却是Optional类最有用的方法。 2-2. 实战案例 // 使用 Optional 前 public int readDuration(Properties props, String name) { String value = props.getProperty(name); if (value != null) { try { int i = Integer.parseInt(value); if (i \u003e 0) { return i; } } catch (NumberFormatException nfe) { } } return 0; } // 使用 Optional 后 public int readDuration(Properties props, String name) { return Optional.ofNullable(props.getProperty(name)) .flatMap(OptionalUtility::stringToInt) .filter(i -\u003e i \u003e 0) .orElse(0); } ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:2:2","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"3. 内部迭代 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:2:3","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"三、高效 Java 8 编程 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:0","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"1. 各版本变化 Java 5：引入 for-each 循环，替代迭代器 Java 7：引入菱形操作符\u003c\u003e，使用范型 Java 8：lambda表达式，函数式编程，让设计模式更灵活 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:1","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"🌟2. 设计模式 2-1. 策略模式 场景：根据不同条件进行筛选。 某算法的接口 接口的多种实现 策略对象的客户 // 1. 不使用 Lambda 表达式 public interface ValidationStrategy { boolean execute(String s); } public class IsAllLowerCase implements ValidationStrategy { public boolean execute(String s){ return s.matches(\"[a-z]+\"); } } public class IsNumeric implements ValidationStrategy { public boolean execute(String s){ return s.matches(\"\\\\d+\"); } } public class Validator{ private final ValidationStrategy strategy; public Validator(ValidationStrategy v){ this.strategy = v; } public boolean validate(String s){ return strategy.execute(s); } } Validator numericValidator = new Validator(new IsNumeric()); boolean b1 = numericValidator.validate(\"aaaa\"); //FALSE Validator lowerCaseValidator = new Validator(new IsAllLowerCase ()); boolean b2 = lowerCaseValidator.validate(\"bbbb\"); //TRUE // 2. 使用 Lambda 表达式 Validator numericValidator = new Validator((String s) -\u003e s.matches(\"[a-z]+\")); boolean b1 = numericValidator.validate(\"aaaa\"); Validator lowerCaseValidator = new Validator((String s) -\u003e s.matches(\"\\\\d+\")); boolean b2 = lowerCaseValidator.validate(\"bbbb\"); 2-2. 模板方法 场景：需要采用某个算法的框架，同时又希望有一定的灵活度，能对它的某些部分进行改进。 // 1. 不使用 Lambda 表达式 abstract class OnlineBanking { public void processCustomer(int id){ Customer c = Database.getCustomerWithId(id); makeCustomerHappy(c); } abstract void makeCustomerHappy(Customer c); } public void processCustomer(int id, Consumer\u003cCustomer\u003e makeCustomerHappy){ Customer c = Database.getCustomerWithId(id); makeCustomerHappy.accept(c); } // 2. 使用 Lambda 表达式 public void processCustomer(int id, Consumer\u003cCustomer\u003e makeCustomerHappy){ Customer c = Database.getCustomerWithId(id); makeCustomerHappy.accept(c); } new OnlineBankingLambda().processCustomer(1337, (Customer c) -\u003e System.out.println(\"Hello \" + c.getName()); 2-3. 观察者模式 场景：某些事件发生时（比如状态转变），subject需要自动地通知其他observer。 subject：主题 observer：观察者 // 1. 不使用 Lambda 表达式 interface Observer { void notify(String tweet); } class NYTimes implements Observer{ public void notify(String tweet) { if(tweet != null \u0026\u0026 tweet.contains(\"money\")){ System.out.println(\"Breaking news in NY! \" + tweet); } } } class Guardian implements Observer{ public void notify(String tweet) { if(tweet != null \u0026\u0026 tweet.contains(\"queen\")){ System.out.println(\"Yet another news in London... \" + tweet); } } } interface Subject{ void registerObserver(Observer o); void notifyObservers(String tweet); } class Feed implements Subject{ private final List\u003cObserver\u003e observers = new ArrayList\u003c\u003e(); public void registerObserver(Observer o) { this.observers.add(o); } public void notifyObservers(String tweet) { observers.forEach(o -\u003e o.notify(tweet)); } } Feed f = new Feed(); f.registerObserver(new NYTimes()); f.registerObserver(new Guardian()); //observe ↓↓↓ f.notifyObservers(\"The queen said her favourite book is Java 8 in Action!\"); //输出： //Yet another news in London... //The queen said her favourite book is Java 8 in Action! // 2. 使用 Lambda 表达式 // 简化了观察者的代码 interface Subject{ void registerObserver(Observer o); void notifyObservers(String tweet); } class Feed implements Subject{ private final List\u003cObserver\u003e observers = new ArrayList\u003c\u003e(); public void registerObserver(Observer o) { this.observers.add(o); } public void notifyObservers(String tweet) { observers.forEach(o -\u003e o.notify(tweet)); } } Feed f = new Feed(); f.registerObserver((String tweet) -\u003e { if(tweet != null \u0026\u0026 tweet.contains(\"money\")){ System.out.println(\"Breaking news in NY! \" + tweet); } }); f.registerObserver((String tweet) -\u003e { if(tweet != null \u0026\u0026 tweet.contains(\"queen\")){ System.out.println(\"Yet another news in London... \" + tweet); } }); f.notifyObservers(\"The queen said her favourite book is Java 8 in Action!\"); //输出： //Yet another news in London... //The queen said her favourite book is Java 8 in Action! 2-4. 责任链模式 场景：创建处理对象序列（比如操作序列） 定义一个代表处理对象的抽象类 // 1. 不使用 Lambda 表达式 public abstract class ProcessingObject\u003cT\u003e { protected ProcessingObject\u003cT\u003e successor; public void setSuccessor(ProcessingObject\u003cT\u003e successor){ this.successor = successor; } public T handle(T input){ T r = handleWork(input); if(successor != null){ return successor.handle(r); } return r; } abstract protected T handleWork(T input); } public class HeaderTextProcessing extends ProcessingObject\u003cString\u003e { public String handleWork(String text){ return \"From Raoul, Mario and Alan: \" + text; } } public class SpellCheckerProcessing extends ProcessingObject\u003cString\u003e { public String handleWork(String text){ return text.replaceAll(\"labda\", \"lambda\"); } } ProcessingObject\u003cString\u003e p1 = new HeaderTextProcessing(); ProcessingObject\u003cString\u003e p2 = new SpellCheckerProcessing(); p1.setSuccessor(p2); String result = p1.handle(\"Aren't labdas really sexy?!!\"); System.out.println(result); // 2. 使用 Lambda 表达式 UnaryOperator\u003cString\u003e headerProcessing = (String text) -\u003e \"From Raoul, Mario and Alan: \" + text; UnaryOperator\u003cString\u003e spellCheckerProcessing = (String text) -\u003e text.replaceAll(\"labda\", \"lambda\"); Function\u003cString, String\u003e pipeline = headerProcessing.andThen(spellCheckerProcessing); String result = pipeline.apply(\"Aren't labdas really sexy?!!\"); 2-5. 工厂模式 场景：无需向客户暴露实例化的逻辑就能完成对象的创建 // 1. 不使用 Lambda 表达式 public class ProductFactory { public static Product createProduct(String name){ switch(name){ case \"loan\": return new Loan(); case \"stock\": return new Stock(); case \"bond\": return new Bond(); default: throw new RuntimeException(\"No such product \" + name); } } } Product p = ProductFactory.createProduct(\"loan\"); // 2. 使用 Lambda 表达式 Supplier\u003cProduct\u003e loanSupplier = Loan::new; Loan loan = loanSupplier.get(); final static Map\u003cString, Supplier\u003cProduct\u003e\u003e map = new HashMap\u003c\u003e(); static { map.put(\"loan\", Loan::new); map.put(\"stock\", Stock::new); map.put(\"bond\", Bond::new); } public static Product createProduct(String name){ Supplier\u003cProduct\u003e p = map.get(name); if(p != null) return p.get(); throw new IllegalArgumentException(\"No such product \" + name); } ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:2","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"3. peek查看中间值 List\u003cInteger\u003e numbers = Arrays.asList(2, 3, 4, 5); List\u003cInteger\u003e result = numbers.stream() .peek(x -\u003e System.out.println(\"from stream: \" + x)) .map(x -\u003e x + 17) .peek(x -\u003e System.out.println(\"after map: \" + x)) .filter(x -\u003e x % 2 == 0) .peek(x -\u003e System.out.println(\"after filter: \" + x)) .limit(3) .peek(x -\u003e System.out.println(\"after limit: \" + x)) .collect(toList()); /* 输出： from stream: 2 after map: 19 from stream: 3 after map: 20 after filter: 20 after limit: 20 from stream: 4 after map: 21 from stream: 5 after map: 22 after filter: 22 after limit: 22 */ ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:3","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"★4. 接口的默认方法 public interface Sized { int size(); default boolean isEmpty() { return size() == 0; } } 4-1. 继承和实现冲突的问题 4-2. 三条规则 首先，类或父类中显式声明的方法，其优先级高于所有的默认方法。 如果用第一条无法判断，方法签名又没有区别，那么选择提供最具体实现的默认方法的接口。函数签名相同时，优先选择拥有最具体实现的默认方法的接口，即如果B继承了A，那么B就比A更加具体。 最后，如果冲突依旧无法解决，继承了多个接口的类必须通过显式覆盖和调用期望的方法，显式地选择使用哪一个默认方法的实现。 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:4","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"5. 组合式异步编程 5-1. 并发与并行 ★5-2. Future -\u003e CompletableFuture Stream和CompletableFuture的设计都遵循了类似的模式：它们都使用了Lambda表达式以及流水线的思想。从这个角度，你可以说 CompletableFuture和Future的关系就跟Stream和Collection的关系一样。 Collection主要是为了存储和访问数据，而Stream则主要用于描述对数据的计算 通过Stream你可以对一系列的操作进行流水线，通过map、filter或者其他类似的方法提供行为参数化，它可有效避免使用迭代器时总是出现模板代码。 类似地，CompletableFuture提供了像thenCompose、thenCombine、allOf这样的操作，对Future涉及的通用设计模式提供了函数式编程的细粒度控制，有助于避免使用命令式编程的模板代码。 实例 List\u003cShop\u003e shops = Arrays.asList(new Shop(\"BestPrice\"), new Shop(\"LetsSaveBig\"), new Shop(\"MyFavoriteShop\"), new Shop(\"BuyItAll\")); // 1. 顺序查询 耗时 4032ms public List\u003cString\u003e findPrices(String product) { return shops.stream() .map(shop -\u003e String.format(\"%s price is %.2f\", shop.getName(), shop.getPrice(product))) .collect(toList()); } // 2. 并行流 耗时 1180ms public List\u003cString\u003e findPrices(String product) { return shops.parallelStream() .map(shop -\u003e String.format(\"%s price is %.2f\", shop.getName(), shop.getPrice(product))) .collect(toList()); } // 3. 使用工厂方法supplyAsync创建CompletableFuture对象 耗时 2005ms List\u003cCompletableFuture\u003cString\u003e\u003e priceFutures = shops.stream() .map(shop -\u003e CompletableFuture.supplyAsync( () -\u003e String.format(\"%s price is %.2f\", shop.getName(), shop.getPrice(product)))) .collect(toList()); return priceFutures.stream() .map(CompletableFuture::join) .collect(toList()); //处理5个商店，顺序执行版本耗时5025ms，并行流版本耗时2177ms，而CompletableFuture版本耗时2006ms //处理9个商店，并行流版本耗时3143ms，而CompletableFuture版本耗时3009ms 这里使用了两个不同的Stream流水线，而不是在同一个处理流的流水线上一个接一个地放置两个map操作——这其实是有缘由的。考虑流操作之间的延迟特性，如果你在单一流水线中处理流，发向不同商家的请求只能以同步、顺序执行的方式才会成功。因此，每个创建CompletableFuture对象只能在前一个操作结束之后执行查询指定商家的动作、通知join方法返回计算结果。 5-3. 使用定制的执行器 private final Executor executor = Executors.newFixedThreadPool(Math.min(shops.size(), 100), new ThreadFactory() { public Thread newThread(Runnable r) { Thread t = new Thread(r); //使用守护线程——这种方式不会阻止程序的关停 t.setDaemon(true); return t; } }); //改进之后，使用CompletableFuture方案的程序处理5个商店仅耗时1021ms，处理9个商店时耗时1022ms。 CompletableFuture.supplyAsync(() -\u003e shop.getName() + \" price is \" + shop.getPrice(product), executor); 守护进程 Java程序无法终止或者退出一个正在运行中的线程，所以最后剩下的那个线程会由于一直等待无法发生的事件而引发问题。与此相反，如果将线程标记为守护进程，意味着程序退出时它也会被回收 5-4. CompletableFuture 与 parallelStream 的选择 如果你进行的是计算密集型的操作，并且没有I/O，那么推荐使用Stream接口，因为实现简单，同时效率也可能是最高的（如果所有的线程都是计算密集型的，那就没有必要创建比处理器核数更多的线程）。 反之，如果你并行的工作单元还涉及等待I/O的操作（包括网络连接等待），那么使用CompletableFuture灵活性更好，你可以像前文讨论的那样，依据等待/计算，或者W/C的比率设定需要使用的线程数。这种情况不使用并行流的另一个原因是，处理流的流水线中如果发生I/O等待，流的延迟特性会让我们很难判断到底什么时候触发了等待。 5-5. join 与 get 方法，构造同步和异步操作 CompletableFuture类中的join方法和Future接口中的get有相同的含义，并且也声明在Future接口中，它们唯一的不同是join不会抛出任何检测到的异常。 future.thenCompose public List\u003cString\u003e findPrices(String product) { List\u003cCompletableFuture\u003cString\u003e\u003e priceFutures = shops.stream() //1. 获取价格 .map(shop -\u003e CompletableFuture.supplyAsync( () -\u003e shop.getPrice(product), executor)) //2. 解析报价 .map(future -\u003e future.thenApply(Quote::parse)) //3. 为计算折扣价格构造Future //如果不希望等到第一个任务完全结束才开始第二项任务，使用thenCombine方法 .map(future -\u003e future.thenCompose(quote -\u003e CompletableFuture.supplyAsync( () -\u003e Discount.applyDiscount(quote), executor))) .collect(toList()); return priceFutures.stream() .map(CompletableFuture::join) .collect(toList()); } //thenCompose方法像CompletableFuture类中的其他方法一样，也提供了一个以Async后缀结尾的版本thenComposeAsync。 //通常而言，名称中不带Async的方法和它的前一个任务一样，在同一个线程中运行；而名称以Async结尾的方法会将后续的任务提交到一个线程池，所以每个任务是由不同的线程处理的。 //就这个例子而言，第二个CompletableFuture对象的结果取决于第一个CompletableFuture，所以无论你使用哪个版本的方法来处理CompletableFuture对象，对于最终的结果，或者大致的时间而言都没有多少差别。 //我们选择thenCompose方法的原因是因为它更高效一些，因为少了很多线程切换的开销。 future.thenCombine Future\u003cDouble\u003e futurePriceInUSD = CompletableFuture.supplyAsync(() -\u003e shop.getPrice(product)) .thenCombine( CompletableFuture.supplyAsync( () -\u003e exchangeService.getRate(Money.EUR, Money.USD)), (price, rate) -\u003e price * rate ); ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:5","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"6. 新的日期和时间API 6-1. LocalDate 、 LocalTime 和 LocalDateTime LocalDate date = LocalDate.of(2014, 3, 18); LocalDate date1 = LocalDate.parse(\"2014-03-18\"); LocalTime time1 = LocalTime.parse(\"13:45:20\"); LocalDateTime dt = LocalDateTime.of(2014, Month.MARCH, 18, 13, 45, 20); LocalDate date2 = dt.toLocalDate(); LocalTime time2 = dt.toLocalTime(); Temporal接口定义了如何读取和操纵为时间建模的对象的值。 6-2. 定义 Duration 或 Period 很自然地你会想到，我们需要创建两个Temporal对象之间的duration。Duration类的静态工厂方法between就是为这个目的而设计的。 // Duration 对应 time Duration d1 = Duration.between(time1, time2); Duration d2 = Duration.between(dateTime1, dateTime2); // Period 对应 date Period tenDays = Period.ofDays(10); Period threeWeeks = Period.ofWeeks(3); Period twoYearsSixMonthsOneDay = Period.of(2, 6, 1); 截至目前，我们介绍的这些日期时间对象都是不可修改的，这是为了更好地支持函数式编程，确保线程安全，保持领域模式一致性而做出的重大设计决定。 6-3. 操纵日期 // 1. 绝对修改 LocalDate date1 = LocalDate.of(2014, 3, 18); LocalDate date2 = date1.withYear(2011); //2011-03-18 LocalDate date3 = date2.withDayOfMonth(25); //2011-03-25 LocalDate date4 = date3.with(ChronoField.MONTH_OF_YEAR, 9);//2011-09-25 // 2. 相对修改 LocalDate date1 = LocalDate.of(2014, 3, 18); LocalDate date2 = date1.plusWeeks(1); //2014-03-25 LocalDate date3 = date2.minusYears(3); //2011-03-25 LocalDate date4 = date3.plus(6, ChronoUnit.MONTHS); //2011-09-25 //上面的plus方法也是通用方法，它和minus方法都声明于Temporal接口中 6-4. 设计一个NextWorkingDay类 TemporalAdjuster nextWorkingDay = TemporalAdjusters.ofDateAdjuster( temporal -\u003e { DayOfWeek dow = DayOfWeek.of(temporal.get(ChronoField.DAY_OF_WEEK)); int dayToAdd = 1; if (dow == DayOfWeek.FRIDAY) dayToAdd = 3; if (dow == DayOfWeek.SATURDAY) dayToAdd = 2; return temporal.plus(dayToAdd, ChronoUnit.DAYS); }); date = date.with(nextWorkingDay); 6-5. 日期和 String 转换 // 使用 LocalDate 的 format 和 parse 方法 LocalDate date = LocalDate.of(2014, 3, 18); String s1 = date.format(DateTimeFormatter.BASIC_ISO_DATE); //20140318 String s2 = date.format(DateTimeFormatter.ISO_LOCAL_DATE); //2014-03-18 LocalDate date1 = LocalDate.parse(\"20140318\", DateTimeFormatter.BASIC_ISO_DATE); LocalDate date2 = LocalDate.parse(\"2014-03-18\", DateTimeFormatter.ISO_LOCAL_DATE); // 自定义模式 DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"dd/MM/yyyy\"); LocalDate date1 = LocalDate.of(2014, 3, 18); String formattedDate = date1.format(formatter); LocalDate date2 = LocalDate.parse(formattedDate, formatter); ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:6","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"四、超越 Java 8 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:0","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"1. 基于“尾-递”的阶乘 static long factorialTailRecursive(long n) { return factorialHelper(1, n); } static long factorialHelper(long acc, long n) { return n == 1 ? acc : factorialHelper(acc * n, n-1); } 这种形式的递归是非常有意义的，现在我们不需要在不同的栈帧上保存每次递归计算的中间值，编译器能够自行决定复用某个栈帧进行计算。实际上，在factorialHelper的定义中，立即数（阶乘计算的中间结果）直接作为参数传递给了该方法。再也不用为每个递归调用分配单独的栈帧用于跟踪每次递归调用的中间值——通过方法的参数能够直接访问这些值。 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:1","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"2. 科里化（curry） static DoubleUnaryOperator curriedConverter(double f, double b){ return (double x) -\u003e x * f + b; } DoubleUnaryOperator convertCtoF = curriedConverter(9.0/5, 32); DoubleUnaryOperator convertUSDtoGBP = curriedConverter(0.6, 0); DoubleUnaryOperator convertKmtoMi = curriedConverter(0.6214, 0); ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:2","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"3. Scala 声明变量 s : String 创建集合val authorsToAge = Map(\"Raoul\" -\u003e 23, \"Mario\" -\u003e 40, \"Alan\" -\u003e 53) val 表示不可变，var表示可变 支持任意大小的元组 val book = (2014, \"Java 8 in Action\", \"Manning\") //元组类型为(Int, String, String) val numbers = (42, 1337, 0, 3, 14) println(book._1) //2014 println(numbers._4) //3 Scala中的Stream可以记录它曾经计算出的值，所以之前的元素可以随时进行访问。除此之外，Stream还进行了索引，所以Stream中的元素可以像List那样通过索引访问。注意，这种抉择也附带着开销，由于需要存储这些额外的属性，和Java 8中的Stream比起来，Scala版本的Stream内存的使用效率变低了，因为Scala中的Stream需要能够回溯之前的元素，这意味着之前访问过的元素都需要在内存“记录下来”（即进行缓存）。 Option: //Java public String getCarInsuranceName(Optional\u003cPerson\u003e person, int minAge) { return person.filter(p -\u003e p.getAge() \u003e= minAge) .flatMap(Person::getCar) .flatMap(Car::getInsurance) .map(Insurance::getName) .orElse(\"Unknown\"); } //Scala def getCarInsuranceName(person: Option[Person], minAge: Int) = person.filter(_.getAge() \u003e= minAge) .flatMap(_.getCar) .flatMap(_.getInsurance) .map(_.getName).getOrElse(\"Unknown\") //在前面的代码中，你使用的是_.getCar（并未使用圆括号），而不是_.getCar() （带圆括号）。Scala语言中，执行方法调用时，如果不需要传递参数，那么函数的圆括号是可以省略的。 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:3","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"4. 闭包 闭包是一个函数实例，它可以不受限制地访问该函数的非本地变量。 不过Java 8中的Lambda表达式自身带有一定的限制：它们不能修改定义Lambda表达式的函数中的本地变量值。这些变量必须隐式地声明为final。 这些背景知识有助于我们理解“Lambda避免了对变量值的修改，而不是对变量的访问”。 ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:4","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"5. trait Scala还提供了另一个非常有助于抽象对象的特性，名称叫trait。它是Scala为实现Java中的接口而设计的替代品。trait中既可以定义抽象方法，也可以定义带有默认实现的方法。trait同时还支持Java中接口那样的多继承，所以你可以将它们看成与Java 8中接口类似的特性，它们都支持默认方法。trait中还可以包含像抽象类这样的字段，而Java 8的接口不支持这样的特性。那么，trait就类似于抽象类吗？显然不是，因为trait支持多继承，而抽象类不支持多继承。Java支持类型的多继承，因为一个类可以实现多个接口。现在，Java 8通过默认方法又引入了对行为的多继承，不过它依旧不支持对状态的多继承，而这恰恰是trait支持的。 trait Sized{ var size : Int = 0 def isEmpty() = size == 0 //带默认实现的isEmpty方法 } class Empty extends Sized println(new Empty().isEmpty()) class Box val b1 = new Box() with Sized //在对象实例化时构建trait println(b1.isEmpty()) val b2 = new Box() b2.isEmpty() //编译错误：因为Box类的声明并未继承Sized ","date":"2023-08-08","objectID":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:5","tags":["Java"],"title":"《Java8实战》笔记","uri":"/java8%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["interview"],"content":"学习 Java 必须掌握的基础知识","date":"2023-08-08","objectID":"/java_base/","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"元注解 @Retention用于标明注解被保留的阶段 RetentionPolicy SOURCE 源文件保留 CLASS 编译期保留，默认值（.class：RuntimeInvisibleAnnotations） RUNTIME 运行期保留，可通过反射去获取注解信息（.class： RuntimeVisibleAnnotations） @Target用于标明注解使用的范围 ElementType @Inherited用于标明注解可继承 ","date":"2023-08-08","objectID":"/java_base/:0:1","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"反射 反射类及反射方法的获取，都是通过从列表中搜寻查找匹配的方法，所以查找性能会随类的大小方法多少而变化； 每个类都会有一个与之对应的Class实例，从而每个类都可以获取method反射方法，并作用到其他实例身上； 反射也是考虑了线程安全的，放心使用； 反射使用软引用relectionData缓存class信息，避免每次重新从jvm获取带来的开销； 反射调用多次生成新代理Accessor, 而通过字节码生存的则考虑了卸载功能，所以会使用独立的类加载器； invoke时，是通过 MethodAccessor 进行调用的，而 MethodAccessor 是个接口，在第一次时调用 acquireMethodAccessor() 进行新创建。 进行 ma.invoke(obj, args); 调用时，调用 DelegatingMethodAccessorImpl.invoke();最后被委托到 NativeMethodAccessorImpl.invoke() 在ClassDefiner.defineClass方法实现中，每被调用一次都会生成一个DelegatingClassLoader类加载器对象 ，这里每次都生成新的类加载器，是为了性能考虑，在某些情况下可以卸载这些生成的类，因为类的卸载是只有在类加载器可以被回收的情况下才会被回收的，如果用了原来的类加载器，那可能导致这些新创建的类一直无法被卸载。而反射生成的类，有时候可能用了就可以卸载了，所以使用其独立的类加载器，从而使得更容易控制反射类的生命周期。 当找到需要的方法，都会copy一份出来，而不是使用原来的实例，从而保证数据隔离； 调度反射方法，最终是由jvm执行invoke0()执行； ","date":"2023-08-08","objectID":"/java_base/:0:2","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"API vs SPI？ SPI - “接口”位于“调用方”所在的“包”中 概念上更依赖调用方。 组织上位于调用方所在的包中。 实现位于独立的包中。 常见的例子是：插件模式的插件。 使用SPI机制的缺陷： 不能按需加载，需要遍历所有的实现，并实例化，然后在循环中才能找到我们需要的实现。如果不想用某些实现类，或者某些类实例化很耗时，它也被载入并实例化了，这就造成了浪费。 获取某个实现类的方式不够灵活，只能通过 Iterator 形式获取，不能根据某个参数来获取对应的实现类。 多个并发多线程使用 ServiceLoader 类的实例是不安全的。 API - “接口”位于“实现方”所在的“包”中 概念上更接近实现方。 组织上位于实现方所在的包中。 实现和接口在一个包中。 ","date":"2023-08-08","objectID":"/java_base/:0:3","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"happens-before规则？ Java 虚拟机在进行代码编译优化时，会出现指令重排序问题，为了避免编译优化对并发编程安全性的影响，Java 虚拟机需要 happens-before 规则限制或禁止编译优化的场景，保证系统并发的安全性。 ","date":"2023-08-08","objectID":"/java_base/:0:4","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"为什么金额不能使用double，要使用BigDecimal？ 浮点数不精确的根本原因在于尾数部分的位数是固定的，一旦需要表示的数字的精度高于浮点数的精度，那么必然产生误差！这在处理金融数据的情况下是绝对不允许存在的。 ","date":"2023-08-08","objectID":"/java_base/:0:5","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"强引用、软引用、弱引用、幻象引用有什么区别？ 强引用，就是我们最常见的普通对象引用，只要还有强引用指向一个对象，就能表明对象还“活着”，垃圾收集器不会碰这种对象。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以被垃圾收集的了，当然具体回收时机还是要看垃圾收集策略。 软引用（SoftReference），是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象。JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 反射使用软引用relectionData缓存class信息，避免每次重新从jvm获取带来的开销； 弱引用（WeakReference）并不能使对象豁免垃圾收集，仅仅是提供一种访问在弱引用状态下对象的途径。这就可以用来构建一种没有特定约束的关系，比如，维护一种非强制性的映射关系，如果试图获取时对象还在，就使用它，否则重现实例化。它同样是很多缓存实现的选择。 虚引用，你不能通过它访问对象。幻象引用仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制，比如，通常用来做所谓的 Post-Mortem 清理机制，我在专栏上一讲中介绍的 Java 平台自身 Cleaner 机制等，也有人利用幻象引用监控对象的创建和销毁。 ","date":"2023-08-08","objectID":"/java_base/:0:6","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"NIO Buffer：高效的数据容器，除了布尔类型，所有原始数据类型都有相应的 Buffer 实现。 Channel：类似在 Linux 之类操作系统上看到的文件描述符，是 NIO 中被用来支持批量式 IO 操作的一种抽象。File 或者 Socket，通常被认为是比较高层次的抽象，而 Channel 则是更加操作系统底层的一种抽象，这也使得 NIO 得以充分利用现代操作系统底层机制，获得特定场景的性能优化，例如，DMA（Direct Memory Access）等。不同层次的抽象是相互关联的，我们可以通过 Socket 获取 Channel，反之亦然。 Selector：是 NIO 实现多路复用的基础，它提供了一种高效的机制，可以检测到注册在 Selector 上的多个 Channel 中，是否有 Channel 处于就绪状态，进而实现了单线程对多 Channel 的高效管理。 Charset：提供 Unicode 字符串定义，NIO 也提供了相应的编解码器等 NIO 引入的多路复用机制，可作为线程池机制的平替： public class NIOServer extends Thread { public void run() { try (Selector selector = Selector.open(); ServerSocketChannel serverSocket = ServerSocketChannel.open();) {// 创建Selector和Channel serverSocket.bind(new InetSocketAddress(InetAddress.getLocalHost(), 8888)); // Q：为什么我们要明确配置非阻塞模式呢？ serverSocket.configureBlocking(false); // 注册到Selector，并说明关注点 serverSocket.register(selector, SelectionKey.OP_ACCEPT); while (true) { selector.select();// 阻塞等待就绪的Channel，这是关键点之一 Set\u003cSelectionKey\u003e selectedKeys = selector.selectedKeys(); Iterator\u003cSelectionKey\u003e iter = selectedKeys.iterator(); while (iter.hasNext()) { SelectionKey key = iter.next(); // 生产系统中一般会额外进行就绪状态检查 sayHelloWorld((ServerSocketChannel) key.channel()); iter.remove(); } } } catch (IOException e) { e.printStackTrace(); } } private void sayHelloWorld(ServerSocketChannel server) throws IOException { try (SocketChannel client = server.accept();) { client.write(Charset.defaultCharset().encode(\"Hello world!\")); } } // 省略了与前面类似的main } A：这是因为阻塞模式下，注册操作是不允许的，会抛出 IllegalBlockingModeException 异常。 ","date":"2023-08-08","objectID":"/java_base/:0:7","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"todo：什么是零拷贝？ 定义：计算机执行IO操作时，CPU不需要将数据从一个存储区域复制到另一个存储区域，从而可以减少上下文切换和CPU的拷贝时间，是一种IO操作优化技术。 传统拷贝： 内核空间：操作系统内核访问的区域，是受保护的内存空间，主要提供进程调度、内存分配、连接硬件资源等功能； 用户空间：用户应用程序访问的内存区域。（fopen、fwrite、fread） 系统调用：从用户态进入内核态，发生CPU上下文的切换（open、write、read） 零拷贝的三种实现方式： mmap+write sendfile 带有DMA收集拷贝功能的sendfile Java中的零拷贝： ","date":"2023-08-08","objectID":"/java_base/:0:8","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"JIT 引入JIT（Just-In-Time）编译的主要目的是提高程序的执行性能。传统的编译方式是提前将源代码编译成机器码，然后在运行时执行。而JIT编译是在程序运行时将部分代码进行动态编译成机器码，以提高执行效率。 JIT编译器会在运行时对热点代码进行监测和分析，热点代码指的是频繁执行的代码块或方法。一旦确定某段代码是热点代码，JIT编译器会将其编译成机器码，并缓存起来，以便下次执行时直接使用。这样可以避免每次执行都需要进行解释和执行源代码的性能损耗。 逃逸分析 是一种静态分析技术，用于确定程序中的对象是否逃逸出方法的作用域。逃逸指的是对象在方法外被引用或传递给其他方法使用的情况 栈上分配和标量替换。栈上分配将对象分配在线程栈上，减少堆内存的使用。 标量替换将对象拆分为独立的字段，存储在寄存器或栈上。 ","date":"2023-08-08","objectID":"/java_base/:0:9","tags":["Java"],"title":"🚩Java 基础","uri":"/java_base/"},{"categories":["interview"],"content":"jstack 的使用方式/如何排查 CPU 飙升问题？ jps -l：先找到正在运行的 Java 进程的进程 ID。 jstack -l 1234 \u003e thread_dump.txt：生成 Java 进程的线程快照，包括所有线程的堆栈跟踪信息、线程状态、锁信息等，使用 \u003e 输出到文件 使用 -l输出锁的信息 使用 -gc 输出 gc 信息 使用 VisualVM 读取以上文件，进行可视化分析。 CPU 占用 100% 可能是什么引起的？ CPU密集型操作： 频繁的GC; 如果访问量很高，可能会导致频繁的GC甚至FGC。当调用量很大时，内存分配将如此之快以至于GC线程将连续执行，这将导致CPU飙升。 序列化和反序列化。当程序执行xml解析时，调用量会增加，从而导致CPU变满。 正则表达式。我遇到了正则表达式使CPU充满的情况; 原因可能是Java正则表达式使用的引擎实现是NFA自动机，它将在字符匹配期间执行回溯。 线程上下文切换; 有许多已启动的线程，这些线程的状态在Blocked（锁定等待，IO等待等）和Running之间发生变化。当锁争用激烈时，这种情况很容易发生。 有些线程正在执行非阻塞操作，例如 while(true)语句。如果在程序中计算需要很长时间，则可以使线程休眠。 ","date":"2023-08-08","objectID":"/linux/:0:1","tags":["Linux"],"title":"🚩Linux 面试题","uri":"/linux/"},{"categories":["interview"],"content":"如何切换 jdk 版本？ sudo update-alternatives --config java ","date":"2023-08-08","objectID":"/linux/:0:2","tags":["Linux"],"title":"🚩Linux 面试题","uri":"/linux/"},{"categories":["interview"],"content":"如何查看磁盘使用情况？ df -h：其中 -h 选项可以将磁盘大小、已用空间、可用空间以 GB 或 MB 显示 du -h：递归地计算目录和文件的大小，可以帮助我们查找磁盘空间占用大的文件和目录 du --max-depth=1 -h 只看目录占用大小 ","date":"2023-08-08","objectID":"/linux/:0:3","tags":["Linux"],"title":"🚩Linux 面试题","uri":"/linux/"},{"categories":["interview"],"content":"查看日志的命令 实时监控日志最后 100 行 tail -n 100 -f xxx.log 按行号查询 收尾 100 行：head/tail -n 100 第 100-300 行：cat xx.log | tail -n +100 | head -n 200 按日期查询 sed -n’/2023-06-17 08:00:000/,/2023-06-23 17:00:000/p’ xx.log 以上两个时间必须在日志中出现过 查询并输出日志 cat -n xx.log | grep “error” | error.txt 线上诊断：Arthas，指定 ip+port 即可 ","date":"2023-08-08","objectID":"/linux/:0:4","tags":["Linux"],"title":"🚩Linux 面试题","uri":"/linux/"},{"categories":["interview"],"content":"#{}和${}的区别是什么？ #{}是预编译处理，${}是字符串替换。 Mybatis 在处理#{}时，会将 sql 中的#{}替换为?号，调用 PreparedStatement 的set 方法来赋值； Mybatis 在处理${}时，就是把${}替换成变量的值。使用#{}可以有效的防止 SQL 注入，提高系统安全性。 使用 ${} 的情况 当sql中表名是从参数中取的情况 order by排序语句中，因为order by 后边必须跟字段名，这个字段名不能带引号，如果带引号会被识别会字符串，而不是字段。 ","date":"2023-08-08","objectID":"/mybatis/:0:1","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"Mybatis 是如何进行分页的？分页插件的原理是什么？ Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页。可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。 ","date":"2023-08-08","objectID":"/mybatis/:0:2","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"Mybatis 的一级、二级缓存？ 基于 PerpetualCache，本质是一个 HashMap。 一级缓存（默认开启）: 存储作用域为 Session，当 Session flush 或 close 时清空 二级缓存（默认关闭）：存储作用域为** Mapper(Namespace)** 需要实现 serializable 接口 只有会话提交或关闭后，一级缓存中的数据才会转移到二级缓存中 C/U/D 操作后，该作用域下所有 select 中的缓存将被 clear。 ","date":"2023-08-08","objectID":"/mybatis/:0:3","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"MyBatis 执行流程？ 读取配置文件 SqlSessionFactory：唯一的会话工厂 SqlSession：多个会话 Executor：执行器，执行数据库操作并维护缓存 MappedStatement：是 Executor 接口中的参数，封装了映射信息，接收入参，返回出参 ","date":"2023-08-08","objectID":"/mybatis/:0:4","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"MyBatis 开启延迟加载 lazyLoadingEnabled = true（默认false），原理： 使用 CGLIB 创建目标对象的代理对象 调用目标方法时，进入拦截器 invoke 方法，若目标方法为 null，执行 SQL 重新调用 ","date":"2023-08-08","objectID":"/mybatis/:0:5","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"Java 对象与 MySQL 对象如何映射？ MyBatis 默认使用 JDBC 的预编译语句来执行 SQL 语句。在将 String 类型的参数传递给 SQL 语句时，MyBatis 会自动将其转换为 VARCHAR 类型。 自定义类型转换 - 实现 TypeHandler 接口","date":"2023-08-08","objectID":"/mybatis/:0:6","tags":["MyBatis"],"title":"🚩MyBatis 面试题","uri":"/mybatis/"},{"categories":["interview"],"content":"Principle 原则 x 6 Open Close 开闭原则：对扩展开放，对修改关闭（通过接口和抽象类） Liskov Substitution 里氏代换：子类可替换父类 Dependence Inversion 依赖倒转：针对接口编程，依赖于抽象而非具体 Interface Segregation 接口隔离： 多个隔离的接口优于单个接口，以解耦合 Demeter 迪米特：最少知道原则，实体间减少相互作用，模块独立 Composite Reuse 合成复用：用聚合，而非继承。 ","date":"2023-08-08","objectID":"/design-pattern/:0:1","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"Create 创建型 x 5 Singleton 6种 Prototype 克隆大对象 Builder 内部结构复杂，关注顺序 Factory 日志、数据库 Abstract Factory 多个产品族（换皮肤） ","date":"2023-08-08","objectID":"/design-pattern/:0:2","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"Structure 结构型 x 7 Adapter 适配器 补救措施 Bridge 桥接 扩展两个独立变化的维度 Composite 组合 部分-整体 Decorator 装饰器 不增加子类来扩展类 Facade 外观 隐藏系统复杂性，违背开闭原则 Flyweight 享元 不可分辨的大量对象 Proxy 代理 创建具有原对象的新对象 和适配器模式的区别：适配器模式主要改变所考虑对象的接口，而代理模式不能改变所代理类的接口。 2、和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。 ","date":"2023-08-08","objectID":"/design-pattern/:0:3","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"Behavior 行为型 x 11 Chain of Responsibility 责任链 多个对象同时处理一个请求 Command 命令 可支持撤销、恢复操作 Interpreter 解释器 高频、特定类型的问题 Iterator 迭代器 顺序访问聚合对象 Mediator 中介者 1对n的网状结构-\u003e1对1的星形结构 Memento 备忘录 为满足迪米特原则，需要一个管理备忘录的类 原型模式+备忘录模式，可节约内存 常用于实现撤销和重做功能，也可以用于实现数据快照、事务管理等应用场景 | | Observer 观察者 | 某对象通知其他对象 | | State 状态 | 行为随状态而改变，替代 if…else… 语句 | | Strategy 策略 | 动态在多种行为中选择一种 | | Template 模板 | 抽离通用方法，加final修饰 | | Visitor 访问者 | 违反迪米特、依赖倒转原则 SimpleFileVisitor | ","date":"2023-08-08","objectID":"/design-pattern/:0:4","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"工厂（创建）+策略（行为）模式 使用场景：多种方式登录、多种付款方式、阶梯计算 types: # key:请求参数 account: accountGranter # 类名首字母改小写 sms: smsGranter we_chat: weChatGranter @Component public class WeChatGranter implements UserGranter{ @Override public LoginResp login(LoginReq loginReq){ // to do return new LoginResp(); } } @Component public class UserLoginFactory implements ApplicationContextAware{ private static Map\u003cString, UserGranter\u003e granterPool = new ConcurrentHashMap\u003c\u003e(); @Override public void setApplicationContext(ApplicationContext application){ // 获取容器中的 bean 对象 loginTypeConfig.getTypes().forEach((k,v)-\u003e{ granterPool.put(k,(UserGranter) application.getBean(b)); }); } // 对外开放 public UserGranter getGranter(String grantType){ return granterPool.get(grantType); } } ","date":"2023-08-08","objectID":"/design-pattern/:0:5","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"单点登录 SSO = Single Sign On 一次登录，访问所有信任的应用系统。 集成认证中心到应用系统中，如 spring-security-oauth2 在认证中心中配置应用系统的信息，包括应用名称、域名、回调地址，同时为用户分配唯一 id 集成认证中心提供的 SDK 或库 用户未登录时，重定向（redirect）到认证中心进行登录，验证完身份重定向回应用系统，在 URL 参数中携带用户的身份标识符 记录用户登录状态于 Session 中 ","date":"2023-08-08","objectID":"/design-pattern/:0:6","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["interview"],"content":"管道+过滤器模式（Channel+Filter） 所有的复杂处理，都可以抽象为管道+过滤器模式（Channel+Filter），用于服务的过滤 实现额外的增强处理，如AOP 中断当前处理流程，返回特定数据 ","date":"2023-08-08","objectID":"/design-pattern/:0:7","tags":["Design Pattern"],"title":"🚩Design Pattern - 设计模式","uri":"/design-pattern/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/jvm_draft_2/","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"1.1 字节码技术 编译：javac demo/jvm/Xxx.java 查看字节码： javap -c demo.jvm.Xxx JVM 是一台基于栈的计算机器 每个线程都有一个独属于自己的线程栈（Stack），用于存储栈帧（Frame），每一次方法调用，JVM会自动创建一个栈帧。 栈帧 = 操作数栈 + 局部变量数组 + Class引用（指向当前方法在运行时常量池中对应的Class） 方法调用的指令： invokestatic：调用静态方法，最快 invokespecial：调用构造函数、同一个类中的private方法、可见的超类方法 invokevirtual：调用public、protected、package级的私有方法 invokeinterface：通过接口引用来调用方法 invokedynamic：支持lambda表达式的实现基础 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:1","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"1.2 类加载器 1.2.1 生命周期 加载（Loading）：找 Class 文件 验证（Verification）：验证格式、依赖【链接（Linking）】 准备（Preparation）：静态字段、方法表【链接（Linking）】 解析（Resolution）：符号解析为引用【链接（Linking）】 初始化（Initialization）：构造器、静态变量赋值、静态代码块 父类静态变量 父类静态代码块 子类静态变量 子类静态代码块 父类非静态变量 父类非静态代码块 父类构造函数 子类非静态变量 子类非静态代码块 子类构造函数 首先，静态优先于非静态，其次，父类优先于子类，最后变量优先于代码块，另外，构造函数属于非静态，在代码块之后执行。 使用（Using） 卸载（Unloading） 1.2.2 不会初始化的情况（可能会加载） 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化； 定义对象数组，不会触发该类初始化； 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发类初始化； 通过类名获取 Class 对象，不会触发类初始化； 通过 Class.forName 加载指定类时，如果指定参数 initialize 为 false 时，也不会触发类初始化，否则就会初始化； 通过 ClassLoader 默认的 loadClass 方法，只加载，不会初始化。 1.2.3 类加载器 启动类加载器（BootstrapClassLoader） 扩展类加载器（ExtClassLoader） 应用类加载器（AppClassLoader） 特点： 双亲委托 负责依赖 缓存加载 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:2","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"1.3 内存模型 Java进程组成： 栈 Stack 堆 Heap 年轻代 Young generation 新生代 Eden space 存活区 Survivor space：总有一个是空的 S0 S1 老年代 Old generation （进入老年代阈值-XX:+MaxTenuringThreshold=15） 非堆 Non-Heap：本质上还是 Heap，只是不归 GC 管理 元数据区 Metaspace -XX:MaxMetaspaceSize=256m 常量池 方法区 CCS = Compressed Class Space：存放 class 信息，和 Metaspace 有交叉 Code Cache：存放 JIT（just-in-time compilation 及时编译） 编译器编译后的本地机器代码 JVM自身 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:3","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"1.4 启动参数 -D：设置系统属性 -X：非标准参数，基本都是传给 JVM 的，默认 JVM 实现这些参数的功能，但是并不保证所有 JVM 都满足，也不保证向后兼容。 -XX：非稳定参数，随时可能在下一版本取消 -XX:+Use**GC：使用 G1/ConcMarkSweep、Serial、Parallel GC -XX: +UnlockExperimentalVMOptions -XX:+UseZGC：Java 11+ 有 ZGC -XX: +UnlockExperimentalVMOptions -XX:+UseShenandoahGC：Java 12+ 有 Shenandoah GC 分析诊断 # 当 OOM（内存溢出） 产生，自动 Dump 堆内存 java -XX:+HeapDumpOnOutOfMemoryError -Xmx256m ConsumeHeap # 指定内存溢出时 Dump 文件的目录 java -XX:HeapDumpPath=/usr/local/ ConsumeHeap # 致命错误的日志文件名 java -XX:ErrorFile=filename # 远程调试 java -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1506 JavaAgent Agent时JVM中的一项黑科技，可以通过无侵入方式注入AOP代码或执行统计，权限非常大。 # 启用native方式的agent，参考 LD_LIBRARY_PATH 路径 -agentlib:libname[=options] # 启用native方式的agent -agentpath:pathname[=options] # 启用外部的agent库，如pinpoint.jar等等 -javaagent:jarpath[=options] # 禁用所有agent -Xnoagent ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:4","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.1 内置命令行工具 javap：反编译class文件 jar：打包文件或目录成.jar文件 jps：查看Java进程 jstat：查看JVM内部gc相关信息 -class 类加载（ClassLoader）信息统计 -compiler JIT即时编译器相关的统计信息 -gc GC相关堆内存信息（单位：kb） -gccapacity 各个内存池分代空间的容量 -gccauce 看上次、本次GC的原因 -gcnew 年轻代的统计信息 -gcnewcapacity -gcold 老年代和元数据区的行为统计 -gcoldcapacity -gcmetacapacity -gcutil GC相关区域的使用率统计（单位：%） -printcompilation 打印JVM编译统计信息 jmap：查看heap或类占用空间统计 -heap [pid]打印堆内存的配置和使用信息 -histo [pid]看哪个类占用的空间最多，直方图 -dump:format=b,file=xxxx.hprof [pid] 备份堆内存 jstack：查看线程信息 -F：强制执行 thread dump，可在Java进程卡死时使用 -m：混合模式，将 Java 帧和 native 帧一起输出 -l：长列表模式，将线程相关的locks信息一起输出，比如持有的锁，等待的锁 jcmd pid help：综合了前面几个命令 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:5","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.2 内置图形化工具 jconsole jvisualvm visualgc jmc 用于对 Java 应用程序进行管理、监视、概要分析和故障排除的工具套件。 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:6","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.3 GC 原理 为什么要分代？ 大部分新生对象很快无用 存活较长时间的对象，可能存活更长时间 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:7","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.4 串行/并行 GC ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:8","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.5 CMS/G1 GC ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:9","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.6 ZGC/Shenandoah GC ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:10","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"2.7 GC 总结 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:11","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"3.1 GC 日志解读与分析 -XX:+UseSerialGC -XX:+UseParallelGC -XX:+UseConcMarkSweepGC -XX:+UseG1GC ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:12","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"3.2 JVM 线程堆栈数据分析 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:13","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"3.3 内存分析与相关工具 堆内存溢出 元空间溢出 本地方法栈溢出 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:14","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"3.4 JVM 问题分析调优经验 ","date":"2023-08-08","objectID":"/jvm_draft_2/:0:15","tags":["JVM"],"title":"JVM 杂记（二）","uri":"/jvm_draft_2/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/jvm_draft_1/","tags":["JVM"],"title":"JVM 杂记（一）","uri":"/jvm_draft_1/"},{"categories":["draft"],"content":"JVM 架构模型 Java 编译器输入的指令流基本上时一种基于栈的指令集架构，适用于资源受限的系统，大部分是零地址指令，指令集更小，编译器容易实现，不需要硬件支持，可移植性好，更好实现跨平台。 JVM 生命周期 启动：通过引导类加载器(bootstrap class loader)创建一个初始类（initial class） 执行：JVM 进程 退出：某线程调用Runtime类的exit或halt方法是其中一种 ","date":"2023-08-08","objectID":"/jvm_draft_1/:0:1","tags":["JVM"],"title":"JVM 杂记（一）","uri":"/jvm_draft_1/"},{"categories":["draft"],"content":"运行时数据区 栈、堆、元空间的交互关系： 元空间 Metaspace（进程拥有，线程间共享） 又名非堆，常看作是一块独立于 Java 堆的内存空间，不在虚拟机设置的内存中，而是使用本地内存。 类型信息（class、interface、enum、annotation）、 域（Field）信息 域名称 域类型 域修饰符（public、private、p rotected、static、final、volatile、transient） 方法信息 方法名 方法返回类型（包括void） 方法参数（按顺序） 方法修饰符 方法字节码、操作数栈、局部变量表及大小（abstact和native方法除外） 异常表 包名、类名 父类名 修饰符（public、abstract、final） 这个类型直接接口的有序列表 运行时常量池：在类加载后，存放编译生成的各种字面量与符号引用 静态变量 即时编译器（JIT）编译后的代码缓存 为什么永久代要被元空间替换？ 为永久代设置空间大小是很难确定的。 对永久代进行调优是很困难的。 🌟堆（进程拥有，线程间共享）——数据存储 字符串常量池、静态变量仍然在堆。 设置参数 -X：jvm的运行参数 ms：memory start，初始内存大小，默认电脑内存/64 mx：memory max，最大内存大小，默认电脑内存/4 mn：新生代大小 -XX: NewRatio（老年代/新生代）=2（默认）：表示新生代占1，老年代占2，新生代占整个堆的1/3 SurvivorRatio（eden区/survivor区）=8（默认）：表示eden区占8，survivor区各占1，有时需要显式指定才生效 【+用-不用】UseAdaptiveSizePolicy：使用自适应的内存分配策略 Max TenuringThreshold=15（默认）：表示age==16时晋升至老年代 对象大小大于To Space可用内存时直接晋升 如果 Survivor 区中相同年龄的所有对象大小的总和占比超过一半，则大于等于该年龄的对象直接晋升 HandlePromotionFailure=true（默认）：空间分配担保，在发生Minor GC之前，检查老年代最大可用连续空间是否大于新生代所有对象的总空间，如果大于，则此次Minor GC 是安全的，否则查看HandlePromotionFailure值，如果为false改为执行 Full GC，如果为true，继续检查老年代最大可用空间是否大于历次晋升老年代对象的平均大小 如果大于，执行Minor GC（仍有风险） 如果小于，执行Full GC JDK7以后改为两次检验结果大于有一个成立就 Minor GC，否则Full GC。 【+用-不用】PrintGCDetails：显示 GC 细节信息(简略版去掉Details) 【+用-不用】PrintFlagsInitial：查看所有参数的默认初始值 【+用-不用】PrintFlagsFinal：查看所有参数的最终值 【+用-不用】UseTLAB：使用 TLAB TLABWasteTargetPercent：设置 TLAB 占用 Eden 空间的百分比，默认1% 【+用-不用】DoEscapeAnalysis：打开逃逸分析（jdk7+默认打开） 【+用-不用】PrintEscapeAnalysis：查看逃逸分析的筛选结果 【Error: VM option ‘PrintEscapeAnalysis’ is notproduct and is available only in debug version of VM.】 【+用-不用】EliminateAllocations：打开标量替换 MetaspaceSize=20.75M（默认）：设置元空间初始分配空间，建议设置一个较高的值，避免频繁Full GC MaxMetaspaceSize：设置元空间最大空间（默认无限制） jps：查看当前运行中的进程 jinfo -flag SurvivorRatio 进程id：查看某个进程的某个参数情况 年轻代与老年代 YoungGen Eden：几乎所有Java对象都是在这里被 new 出来的，绝大部分（80%）对象的销毁都在新生代进行了 survivor 0 survivor 1 OldGen：大对象直接分配到老年代（所以要少创建大对象） GC = Garbage Collection 频繁收集年轻代，较少收集老年代，几乎不动元空间。 Young GC/Minor GC：对新生代GC。 在Eden区满时触发，Survivor区满时不触发。 会引发STW，暂停其它用户的线程，直到GC结束才恢复。 Old GC/Major GC：对老年代GC。 老年代空间不足时，会先尝试触发Minor GC，如果之后空间还不足，则触发 Major GC，如果还不足就报错：OOM Mixed GC：对新生代以及部分老年代GC。只有 G1 GC 会有这种行为，按照 region 小区域划分。 Full GC：对 Java 整个堆和元空间GC，在开发中应尽量避免。 调用System.gc()时，系统建议执行Full GC，但不是必然执行 老年代或元空间内存不足 晋升对象大小大于老年代的可用内存 为对象分配内存：TLAB（线程私有 ） 尽管不是所有对象实例都能在 TLAB 中成功分配内存，但 JVM 确实将 TLAB 作为内存分配的首选。 对象分配优先级：TLAB\u003eEden区其他空间\u003e老年代 堆不是分配对象的唯一选择 逃逸分析（Escape Analysis）：可以有效减少 Java 程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义， 在方法内部使用，则没有逃逸。——可能被优化成栈上分配。 被外部方法所引用，则发生逃逸（new 的对象实体在方法外被调用） 标量替换：用对多个标量的操作代替对聚合量的操作。 程序计数器（每个线程拥有） 全称程序计数寄存器（Program Counter Register），也称为程序钩子。用来存储指向下一条指令的地址，由执行引擎读取下一条指令。它是唯一一个在JVM 规范中没有规定任何OOM异常的区域。它并非广义上所指的物理寄存器，而是对物理 PC 寄存器的一种抽象模拟。 🌟虚拟机栈（每个线程拥有）——程序运行 栈帧（Stack Frame），一一对应执行的方法。 🌟局部变量表 Local Variables 🌟操作数栈 Operand Stack，表达式栈：32bit占一个深度（int），64bit占两个深度（double） 动态链接 Dynamic Linking，指向运行时常量池的方法引用：在 Java 源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用保存在 class 文件的常量池里。动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 方法返回地址 Return Address：方法正常退出（PC计数器的值，即该方法指令的下一条指令地址）或异常退出（查询异常表）的定义 附加信息 静态链接：编译期可知【早期绑定】 非虚方法：在编译期就确定了具体的调用版本，这个版本在运行时不可变 invokestatic：static invokespecial：private、final、实例构造器(\u003cinit\u003e 方法)、父类方法 动态链接：编译期未知，运行期可知【晚期绑定】 invokevitual：调用虚方法 invokeinterface：调用接口 invokedynamic：动态解析方法，然后执行（Lambda表达式的出现使得该指令可以直接生成） Java 是静态语言，但 Lambda 表达式使它具备动态语言的特点。（invokedynamic） 在字节码文件中，构造器和方法等价。Java 中任何一个puts的方法其实都具备c++中虚函数的特征（多态：允许父类指针指向子类实例），除非用 final 修饰。 变量类型： 成员变量 类变量，prepare阶段默认赋值，initial阶段显式赋值 实例变量，随着对象的创建，在堆空间中分配空间，进行赋值 局部变量：必须显式赋值，否则编译不通过 🌟本地方法栈（每个线程拥有） 与 Java 外面的环境交互。 本地方法：一个 Java 程序调用非 Java 代码（比如C）的接口。 ","date":"2023-08-08","objectID":"/jvm_draft_1/:0:2","tags":["JVM"],"title":"JVM 杂记（一）","uri":"/jvm_draft_1/"},{"categories":["draft"],"content":"Java 守护线程 虚拟机线程：JVM 达到安全点时出现。 周期任务线程 GC 线程 编译线程：将字节码编译成本地代码 信号调度线程：接收信号 ","date":"2023-08-08","objectID":"/jvm_draft_1/:0:3","tags":["JVM"],"title":"JVM 杂记（一）","uri":"/jvm_draft_1/"},{"categories":["interview"],"content":"Java 进阶必经之路","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"JVM 内存模型 堆（Heap）：线程共享。所有的对象实例以及数组都要在堆上分配。 GC 主要管理的对象。 新生代（1/3） eden（8/10），可以通过 -XXSurvivorRatio 调整，默认 8 from、to 区域（各 1/10） TLAB：Thread Local Allocation Buffer，为每个线程分配的一个私有缓存区域，否则，多线程同时分配内存时，为避免操作同一地址，可能需要使用加锁等机制，进而影响分配速度（start、top、end） 老年代 （2/3） 大对象 长生命周期对象，-XX:MaxTenuringThreshold调整，默认 15，取值 0～15 from+to 空间不足 OutOfMemoryError：堆空间不足，调整 Xmx -Xms 初始堆空间，-Xmx 堆空间上限，建议设置相等（物理内存的 1/4），在考虑其他项目的内存使用情况时，尽量大。 方法区（Method Area）：线程共享。存储类信息、常量、静态变量、即时编译器编译后的代码，JVM 启动时创建，关闭时释放 jdk 7 位于永久代，jdk8 移动到了 metaspace 元空间，防止内存溢出。 OutOfMemoryError：Metaspace 运行时常量池：常量池中的符号地址（#1、#2..）变为真实地址 虚拟机栈（JVM Stack）：线程私有，由多个栈帧组成。存储局部变量表、方法、对象指针。 如果局部变量引用的对象，并逃离方法的作用范围，需要考虑线程安全。 StackOverFlowError：递归调用导致栈帧过多 or 栈帧过大。 -Xss 128k（默认 1M，太大了） 本地方法栈（Native Method Stack）：线程私有。为虚拟机使用到的Native 方法服务。如Java使用c或者c++编写的接口服务时，代码在此区运行。 程序计数器（Program Counter Register）：线程私有，指向下一条要执行的（字节码）指令。 【不由 JVM 管理】直接内存：是 JVM 的系统内存，在 NIO 操作时，用于数据缓冲区，它分配回收成本高，读写性能高（少一次缓冲区复制）。 ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:1","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"STW为什么需要停顿所有的Java执行线程呢？ 确保一致性快照，让整个执行系统看起来像冻结在某个时间点上了，如果出现分析过程中对象引用关系还在不断变化，则分享结果的准确性是无法保证的。 STW事件和采用哪款GC无关，所有的GC都是有这个事件 越优秀，回收效率越高的垃圾回收器，尽可能地缩短暂停时间。 STW是JVM在后台自动发起和自动完成的。是在用户不可见的情况下，把用户正常的线程全部停掉，再去把不用的对象都干掉。 ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:2","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"G1 GC的好处？ -xx:+UseG1GC 优势： 并行与并发 - 多个垃圾收集线程同时工作（并行），垃圾收集线程与用户线程交替执行（并发），分成三个阶段： 年轻代垃圾回收 并发标记 混合收集（Eden + from -\u003e to，old -\u003e old’） 分代收集 - **G1**将堆空间划分为若干个区域(**Region**)，每个区域都可以充当 eden、s0、s1 和 humongous（大对象），它不要求年轻代，老年代是连续的。 mixed gc：部分区域，G1 特有 minor gc：新生代，时间短（STW） full gc：新生代+老年代，时间长（STW） 复制算法：G1内存回收使用Region作为基本单位，对内存空间进行整理。 可预测的停顿时间模型 - 可以让用户明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。 缺点：相比于CMS，G1还不具备全方位、压倒性优势。比如，G1为了垃圾收集产生的**内存占用**以及程序运行时的**额外执行负载**都比CMS要高。 ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:3","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"🌟Java常见的垃圾收集器和对应的GC 算法？ 可达性分析：以 GC Root 为起点，标记出所有要回收的对象，然后进行清除。 GC 新生代 老年代 特点 SerialGC（JDK1.2 默认） 复制算法 优点：高效、无碎片， 缺点：内存利用率低 标记-整理，无内存碎片 适用于单核 CPU ParallelGC（JDK1.3默认） 高吞吐量，新老年代GC 并行执行 CMS GC（JDK5 引入） 标记-清除，高效， 但出现内存碎片化问题 低延迟，因为使用了增量标记和并发标记来减少 STW 时间 G1（Garbage 1st，JDK9 默认） region 之间复制，不要求年轻代、老年代是连续的 兼顾吞吐量和延迟 ZGC（JDK11 引入） 复制 标记-整理，着色指针+读屏障技术 通过着色指针和读屏障技术，解决了转移过程中准确访问对象的问题，使得 GC 最大停顿时间不超过 10ms，但仅支持 Linux 64 位平台 ZGC 的**着色指针**使用一个额外的标记位来标记对象是否存活，将其存储在对象头中的 unused_bits 字段中，可以快速标记对象存活信息，实现内存压缩和快速的垃圾回收；**读屏障技术**在对象引用读取时进行内存屏障，可以减少STW 时间，提高应用程序的性能和可靠性。 Minor/Young + Major = Full GC GC过程 Java 应用不断创建对象，通常都是分配在 Eden 区域，当其空间占用达到一定阈值时，触发 minor GC。仍然被引用的对象存活下来，被复制到 JVM 选择的 Survivor 区域，而没有被引用的对象则被回收。 经过一次 Minor GC，Eden 就会空闲下来，直到再次达到 Minor GC 触发条件，这时候，另外一个 Survivor 区域则会成为 to 区域，Eden 区域的存活对象和 From 区域对象，都会被复制到 to 区域，并且存活的年龄计数会被加 1。 类似第二步的过程会发生很多次，直到有对象年龄计数达到阈值，这时候就会发生所谓的晋升（Promotion）过程，超过阈值的对象会被晋升到老年代。 ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:4","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"什么是内存溢出和内存泄漏？ 内存溢出 Out Of Memory，是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。可能原因： 虚拟机栈：StackOverFlowError，递归方法死循 元空间方法区：OutOfMemoryError：Metaspace，启动参数内存值设定得过小 堆：OutOfMemoryError，用 jmap + VisualVM 排查，或设置启动参数输出日志。 内存中加载的数据量过于庞大，如一次从数据库取出过多数据 集合类中有对对象的引用，使用完后未清空，使得JVM不能回收 内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，当一个对象不再被使用时，如果没有及时将其引用置为 null 或者手动释放，会导致应用程序崩溃、性能下降、数据丢失等严重后果。 e.g. ThreadLocal 没有及时调用 remove（） ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:5","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":"🌟JVM 调优工具 看堆信息 jmap -heap \u003cpid\u003e 生成快照 dump 文件（保存线程、堆栈调用、异常信息），可以用 VisualVM 去加载 xxx.hprof 文件 jmap -dump:format=b,file=heap.hprof \u003cpid\u003e 垃圾回收统计 jstat -gc \u003cpid\u003e ","date":"2023-08-08","objectID":"/jvm%E4%B8%8Egc/:0:6","tags":["Java"],"title":"🚩JVM与GC","uri":"/jvm%E4%B8%8Egc/"},{"categories":["interview"],"content":" JUC = java.util.concurrent ","date":"2023-08-08","objectID":"/juc/:0:0","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"进程 vs 线程？ 进程是正在运行程序的实例（多开软件 = 多实例进程），包含了线程 进程是分配资源的基本单位，同一进程下所有线程共享内存空间 切换线程成本 \u003c 切换进程 容器的本质是进程。 ","date":"2023-08-08","objectID":"/juc/:0:1","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"并行 vs 并发？ 并发：同一时间，多个线程轮流使用 CPU，Java 并发有三大特性 原子性：锁 可见性：一个线程修改共享变量对其他线程可见（volatile） 有序性：防止先读和后写（volatile） 并行：4 核 CPU 同时执行 4 个线程 ","date":"2023-08-08","objectID":"/juc/:0:2","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"🌟创建线程的方式？ extends Thread @override run() implements Runnable @override run() 无返回值，不可抛异常 implements Callable @ovverride call() 有返回值，可抛异常 FutureTask\u003c\u003e ft; ft.get() 异步获得结果 线程池 Executors 和 ThreadPoolExecutors 的区别和联系？ ThreadPoolExecutor 是线程池的核心实现，有 7 大参数： public ThreadPoolExecutor( int corePoolSize 核心线程数，对于 N 核（逻辑处理器） CPU IO 密集型：2N+1，读写请求任务时间长 CPU 密集型：N+1，计算、Bitmap 转换，高并发，减少切换上下文 int maximumPoolSize 最大（核心+救急）线程数 long keepAliveTime 救急线程生存时间值 TimeUnit unit 救急线程生存时间单位 BlockingQueue queue 阻塞队列，满了创建救急线程 ArrayBlockingQueue 数组 FIFO（饿加载），入队出队锁住整个队列 LinkedBlockingQueue 链表 FIFO（懒加载），收尾有两把锁，入队出队互不影响，效率高 DelayedWorkQueue 出队执行时间靠前的 SynchronousQueue：每次插入都等待移出，不存储元素 ThreadFactory threadFactory 定义线程名、是否是守护线程 RejectedExecutionHandler handler 当线程和阻塞队列都忙时，触发的拒绝策略 AbortPolicy 抛异常（默认） CallerRunsPolicy 调用者所在线程执行 DiscardOldestPolicy 丢弃阻塞队列最久任务 DiscardPolicy 丢弃当前任务 Executors 是一个工具类，提供了一些快速创建线程池的静态方法，但不推荐使用，原因： 线程池或请求队列长度 = Integer.MAX_VALUE 没做限制，会导致 OOM。 newCachedThreadPool()：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；用来处理大量短时间工作任务。其内部使用 SynchronousQueue 作为工作队列。 newFixedThreadPool(int nThreads)：固定线程数，用于任务量已知且相对耗时的场景。 newSingleThreadExecutor()：单线程，顺序执行任务 newSingleThreadScheduledExecutor() 和 newScheduledThreadPool(int corePoolSize)，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 线程池使用场景 数据迁移：MySQL -\u003e es 用到了 CountDownLatch + 线程池，初始值设为查询总页数，每页 ctl.countDown()，最终 countDownLatch.await() 等待计数归零 数据汇总：并行执行没有依赖关系的接口，Callable + Future 异步调用：上级方法不需依赖下级方法的返回值 e.g. 保存搜索历史 @EnableAsync class SpringBootApplication @Async(“taskExecutor”) ","date":"2023-08-08","objectID":"/juc/:0:3","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"Java 线程的状态 yield()：使线程进入就绪状态，但不释放锁资源，不会导致阻塞 t.join()：当前线程调用线程t，当前线程不释放锁，t会释放同步锁 **notify()**：随机唤醒在此监视器上等待的线程 🌟Object.wait() vs Thread.sleep(long) ？ wait() 方法是 Object 类的成员方法，而 sleep() 方法是 Thread 类的静态方法。在使用 wait() 方法时需要获取对象的监视器锁，而 sleep() 方法不需要。 wait() 方法会释放对象的监视器锁，使得其他线程可以获取该对象的监视器锁并执行同步代码块或同步方法。而 sleep() 方法不会释放任何锁，线程仍然持有之前获取的所有锁。 wait() 方法必须在 synchronized 块或方法内部调用，即在 synchronized 块或方法内部调用 wait() 方法时，当前线程会释放锁并进入等待状态。而 sleep() 方法可以在任何地方调用，不需要持有任何锁。 wait() 方法会一直等待，直到被 notify() 或 notifyAll() 唤醒，或者等待超时。而 sleep() 方法会暂停线程的执行，等待指定的时间后自动唤醒线程。 如何停止线程？ 正常执行完 run() stop() 不推荐 interrupt 打断（阻塞会抛异常 InterruptedException），成功后可用 isInterrupt() 判断 ","date":"2023-08-08","objectID":"/juc/:0:4","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"🌟synchronized synchronized 中的锁 偏向锁【无竞争】：只被一个线程持有，第一次 CAS（乐观锁），之后判断线程 id 🌟CAS = Compare and Swap（比较并替换）：当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为目标值B，否则就什么都不做。在无锁情况下，保证线程操作共享数据的原子性，通过自旋锁，调用操作系统底层 CAS 来实现（Unsafe 类），用于 AQS、AtomicXxx。缺点：循环时间长开销很大、只能保证一个共享变量的原子操作、ABA问题。 轻量级锁【有竞争】：多线程交替，每次修改都是 CAS 操作，保证原子性 重量级锁 Monitor【多线程竞争】：悲观锁，由 JVM 提供，C++实现。一旦发生锁竞争，都会编程重量级锁 waitSet：waiting 状态的线程集合 entrySet：BLOCKED 状态的线程集合 owner：当前获得锁的线程，只有一个 Hotspot JVM 中对象存储锁的方式 对象头 MarkWord（001 无锁，101 偏向锁，00 轻量级锁，10 重量级锁，11 GC） KlassWord 实例数据 Object body 对齐填充 Lock 和 synchronized 的区别？ synchronized 是Java 的一个内置关键字，只能实现非公平锁。 Lock 是一个类，有 ReentrantLock、ReentrantReadWriteLock 的实现，可以实现： 公平锁 中断试图获取锁的一个线程 设置超时 必须手动 unlock()释放，防止死锁 有竞争时，Lock 性能更好。 ","date":"2023-08-08","objectID":"/juc/:0:5","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"死锁诊断 jps：JVM 中进程状态 jstack：Java 进程内线程的堆栈 jstack -l \u003c进程 id\u003e jconsole：对 JVM 的内存、线程、类的监控 ","date":"2023-08-08","objectID":"/juc/:0:6","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"🌟volatile 功能： 保证线程间的可见性 禁止指令重排序（JIT 即时编译器自动优化，可通过 -Xint 关闭，但不推荐） 用法和原理： volatile 修饰的写变量放在最后，阻止上方写操作越过屏障 volatile 修饰的读变量放在最先，阻止下方读操作约过屏障 使用场景： DCL 单例 AQS.state ","date":"2023-08-08","objectID":"/juc/:0:7","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"🌟AQS = AbstractQueuedSynchronizer 抽象队列同步器 Q：和 synchronized 的区别？ A： 他们都是悲观锁。AQS 由 Java 实现，手动开启和关闭，处理竞争有多种方案，synchronized 由 C++ 实现，自动释放锁，遇到竞争就变成重量级锁，性能差。 AQS 有一个 FIFO 队列，head 指向最久元素，tail 指向最新元素，可以实现公平锁和非公平锁： 公平锁：FIFO 非公平锁：新元素与队列中元素竞争 使用场景： ReentrantLock 可重入锁 = CAS + AQS（extends） exclusiveOwnerThread 当前持有锁的线程，当其为 null 时，唤醒双向队列中的线程 Semaphore：控制某方法并发访问线程数 acquire() 请求 -1 release() 释放 +1 内部状态： volatile state = 0 无锁/1 有锁 ，修改通过 CAS，保证原子性。 ","date":"2023-08-08","objectID":"/juc/:0:8","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview"],"content":"ThreadLocal 为每个线程都分配一个独立的线程副本，解决多线程并发访问冲突问题，包含 set、get、remove 方法，key 是 ThreadLocal 自己，放入 ThreadLocalMap 中。 使用场景 在复杂的链式调用中传递参数：工作中可能会遇到很长的调用链，例如：在A-\u003eB-\u003eC-\u003eD的调用链中，D需要使用A的参数，而为了避免一层层传递参数，可以使用ThreadLocal将参数存储在A中，然后在D中取出使用。 为每个线程生成独立的随机数或者其他类似的资源：在多线程编程中，可能需要为每个线程生成独享的对象，避免多线程下的竞争条件。例如，给每个线程携带dateFormat。 static ThreadLocal\u003cSimpleDateFormat\u003e dateFormatThreadLocal = ThreadLocal.withInitial(() -\u003e new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")); // ...略 public static String date(int seconds) { //参数的单位是毫秒，从1970.1.1 00:00:00 GMT计时 Date date = new Date(1000 * seconds); SimpleDateFormat dateFormat = dateFormatThreadLocal.get(); return dateFormat.format(date); } ThreadLocal是一种避免共享的设计模式，它是一种无同步方案。其实把dateFormat写在date()里面也是无同步方案，只不过，它叫另一种名字——栈封闭 强、软、弱、虚引用 强引用：new，GC Root （可达性分析）找不到时回收 软引用：SoftReference，内存不足时回收 弱引用：WeakReference，始终回收 虚引用：ReferenceQueue，由 ReferenceHandler 线程释放 内存泄漏问题 static class Entry extends WeakReference\u003cThreadLocal\u003c?\u003e\u003e{ Object value; Entry(ThreadLocal\u003c?\u003e k,Object v){ super(k); // 弱引用，GC 会释放 value = v; // 强引用，内存泄漏 } } 解决方案：手动调用 remove() 方法 ","date":"2023-08-08","objectID":"/juc/:0:9","tags":["Java"],"title":"🚩JUC","uri":"/juc/"},{"categories":["interview","practice","ChatGPT"],"content":"学习 JUC 不得不掌握的代码...","date":"2023-08-08","objectID":"/juc-%E7%BB%8F%E5%85%B8%E4%BB%A3%E7%A0%81/","tags":["Java"],"title":"🚩JUC 经典代码","uri":"/juc-%E7%BB%8F%E5%85%B8%E4%BB%A3%E7%A0%81/"},{"categories":["interview","practice","ChatGPT"],"content":" package juc; import java.util.concurrent.*; /** * 一个线程1,3,5,7；一个线程 2,4,6,8；合作输出 1,2,3,4,5,6,7,8 * 核心思想： * 1. 使用synchronized关键字对lock对象进行同步 * 2. 使用wait和notify方法实现线程之间的交替输出 * 3. 输出的数字用 volatile 修饰，保证可见性 */ public class AlternatePrinting { private static final Object lock = new Object(); private static volatile int count = 1; public static void main(String[] args) { // 等价于newFixedThreadPool（2），但是阻塞队列上限不是 Integer.MAX_VALUE ExecutorService executorService = new ThreadPoolExecutor(2, 2, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003c\u003e(1), new CustomThreadFactory()); executorService.submit(new OddPrinter()); executorService.submit(new EvenPrinter()); executorService.shutdown(); } static class CustomThreadFactory implements ThreadFactory { private static final String THREAD_NAME_PREFIX = \"MyThread-\"; private int threadCount = 1; @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setName(THREAD_NAME_PREFIX + threadCount++); return thread; } } static class OddPrinter implements Runnable { @Override public void run() { while (count \u003c= 8) { synchronized (lock) { if (count % 2 != 0) { System.out.println(count++ + \" by \" + Thread.currentThread().getName()); lock.notify(); } else { try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } } } static class EvenPrinter implements Runnable { @Override public void run() { while (count \u003c= 8) { synchronized (lock) { if (count % 2 == 0) { System.out.println(count++ + \" by \" + Thread.currentThread().getName()); lock.notify(); } else { try { lock.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } } } } 以上代码由 ChatGPT 生成，经验证可以实现功能。 ","date":"2023-08-08","objectID":"/juc-%E7%BB%8F%E5%85%B8%E4%BB%A3%E7%A0%81/:0:0","tags":["Java"],"title":"🚩JUC 经典代码","uri":"/juc-%E7%BB%8F%E5%85%B8%E4%BB%A3%E7%A0%81/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/mq_draft/","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"MQ 四大作用 异步通信：减少线程等待 系统解耦 削峰填谷：压力大的时候，缓冲部分请求 可靠通信：提供多种消息模式、服务质量、顺序保障等 ","date":"2023-08-08","objectID":"/mq_draft/:0:1","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"消息处理模式 点对点：Queue 发布订阅：对应 Topic（Kafka) ","date":"2023-08-08","objectID":"/mq_draft/:0:2","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"Kafka Broker：集群中的服务器 Topic：消息类别，物理上不同 Topic 的消息分开存储【通过顺序写入达到高吞吐】 Partition：每个 Topic 包含 1～n 个 Partition【可扩展性】 Producer：生产者，负责发布消息到 Broker，有三种确认模式 ack = 0 只发送，不管是否成功写入 broker ack = 1 写入到 leader 就认为成功 ack = -1/all 写入到最小副本则认为成功 Consumer： 消费者，从 Broker 读取消息 Consumer Group：每个 Consumer 属于一个特定的 Consumer Group ","date":"2023-08-08","objectID":"/mq_draft/:0:3","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"RabbitMQ ","date":"2023-08-08","objectID":"/mq_draft/:0:4","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"26.2 RocketMQ ","date":"2023-08-08","objectID":"/mq_draft/:0:5","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["draft"],"content":"26.3 Pulsar ","date":"2023-08-08","objectID":"/mq_draft/:0:6","tags":["MQ"],"title":"MQ 杂记","uri":"/mq_draft/"},{"categories":["interview"],"content":"🌟RabbitMQ如何保证消息不丢失？ producer 生产者确认 持久化：交换机、队列、消息 consumer 消费者确认：由 spring 确认消息处理成功后 ack consumer 消费者失败重试机制，达到一定次数后转人工处理 监控和告警 高可用集群 ","date":"2023-08-08","objectID":"/mq/:0:1","tags":["MQ"],"title":"🚩MQ - 消息队列","uri":"/mq/"},{"categories":["interview"],"content":"🌟重复消费怎么解决？ 每条消息设置唯一 id 幂等性方案：分布式锁、数据库锁（乐观锁、悲观锁、唯一索引） ","date":"2023-08-08","objectID":"/mq/:0:2","tags":["MQ"],"title":"🚩MQ - 消息队列","uri":"/mq/"},{"categories":["interview"],"content":"延迟队列 延迟消费的消息：超时订单、限时优惠、定时发布（e.g. 零零播项目中的任务） 延迟队列 = 死信交换机 + TTL 死信交换机 DLX（Dead Letter Exchange）：声明一个交换机，delayed = true 死信 dead letter 定义： 消费者使用basic.reject声明消费失败，并且 requeue = false 消息过期，超时无人消费（发送消息时添加参数 x-delay = 超时时间） 队列已满，最早的消息可能成为死信 ","date":"2023-08-08","objectID":"/mq/:0:3","tags":["MQ"],"title":"🚩MQ - 消息队列","uri":"/mq/"},{"categories":["interview"],"content":"🌟大数据堆积在 MQ 怎么办？ 单个消费者-多线程消费 多个消费者消费 使用 RabbitMQ 惰性队列，支持百万条消息存储 将消息直接存入磁盘 消费者从磁盘读消息 ","date":"2023-08-08","objectID":"/mq/:0:4","tags":["MQ"],"title":"🚩MQ - 消息队列","uri":"/mq/"},{"categories":["读书笔记"],"content":"消息引擎系统 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"定义 持久化能力：TB级别数据保持O(1)的时间复杂度 高吞吐率：100kps 分布式消费：每个partition内消息顺序传输 支持离线和实时处理 支持在线水平扩展 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:1","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"基本概念 broker：kafka的服务器 topic：发布到kafka集群的消息的类别 partition：物理分区，每个topic包含1～n个partition producer：负责发布消息到kafka broker consumer：向kafka broker 读取消息的客户端 consumer group：每个consumer可以属于一个特定的group ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:2","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"特性 broker强依赖zookeeper 多partition支持水平扩展和并行处理，顺序写入提升吞吐性能 容错性：每个partition可以通过副本因子添加多个副本（3副本2确认，5副本3确认） Kafka 简介 服务器端：Broker集群，分散运行在不同机器以实现高可用 客户端： 生产者（Producer）：向主题发布新消息 消费者（Consumer）：向主题订阅新消息 点对点模型（Peer to Peer，P2P） 消费者组（Consumer Group）：多个消费者实例同时消费，加速整个消费端的吞吐量（TPS） 重平衡（Rebalance）：消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 发布订阅模型 备份机制（Replication）（v0.8） 领导者副本（Leader Replica）：唯一对外提供服务 追随者副本（Follower Replica）：追随领导者 分区（Partitioning） 主题（Topic）：主题是承载消息的逻辑容器，是发布订阅的对象，在实际使用中多用来区分具体的业务 分区（Partition）：一个有序不变的消息序列，将每个主题划分成多个分区（Partition）， 生产者生产的每条消息只会被发送到一个分区中。 Kafka 的三层消息架构 主题层：每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本 分区层：每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。 消息层：分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。 最后，客户端程序只能与分区的领导者副本进行交互。 位移 消费者位移（Consumer Offset）：消费者消费进度的指示器，可能是随时变化的； 分区位移：消息在分区中的位移，表征的是分区内的消息位置，它是单调递增且不变的。 Kafka Streams（v0.10） 🌟Apache Kafka 是消息引擎系统，也是一个分布式流处理平台（Distributed Streaming Platform） Kafka 与其他主流大数据流式计算框架相比的优势？ 更容易实现端到端的正确性（Correctness） 灵活性：Kafka Streams 是一个用于搭建实时流处理的客户端库而非是一个完整的功能系统 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:1:3","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"Kafka 在生产环境中的应用 Kafka 版本对比 Apache Kafka：开发人数最多、版本迭代速度最快，用户仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度；【更新快，把控度高】 Confluent Kafka：用户需要用到 Kafka 的一些高级特性，中文文档少；【高级特性】 CDH/HDP Kafka：所有的操作都可以在前端 UI 界面上完成，用户需要快速地搭建消息引擎系统，或者需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件。【节省运维成本】 磁盘选择 机械磁盘成本低且容量大，但易损坏；固态硬盘性能优势大，不过单价高。而 Kafka 多是顺序读写操作，一定程度上规避了机械磁盘随机读写操作慢的劣势，所以选择机械磁盘即可。 就 Kafka 而言，一方面 Kafka 自己实现了冗余机制来提供高可靠性；另一方面通过分区的概念，Kafka 也能在软件层面自行实现负载均衡，所以可以不搭建磁盘阵列（RAID） I/O模型 Kafka 客户端底层使用了 Java 的 selector，在不同平台上的 I/O 模型不同： Linux：epoll，介于I/O 多路复用和信号驱动 I/O之间； Windows：select，属于I/O 多路复用模型。 零拷贝（Zero Copy）：当数据在磁盘和网络进行传输时，避免昂贵的内核态数据拷贝，从而实现快速地数据传输。 Kafka 集群配置参数 以下主要为需要修改默认值的参数。 Broker 端参数 log.dirs：用逗号分割文件目录路径，最好保证这些目录挂载到不同的物理磁盘上，以提高性能、故障转移（Failover）。如：`/home/kafka1,/home/kafka2,/home/kafka3`` zookeeper.connect：chroot 是 ZooKeeper 的概念，类似于别名，只需要写一次，而且是加到最后的。如：zk1:2181,zk2:2181,zk3:2181/kafka1和zk1:2181,zk2:2181,zk3:2181/kafka2 listeners：监听器，告诉外部连接者要通过什么协议访问指定主机名和端口开放的 Kafka 服务，格式上是若干个逗号分隔的三元组，每个三元组的格式为\u003c协议名称，主机名，端口号\u003e，其中主机名建议使用域名，而不是IP地址。如果自定义协议名字，如：CONTROLLER://localhost:9092，就需要指定协议底层使用了哪种安全协议，如：listener.security.protocol.map=CONTROLLER:PLAINTEXT advertised.listeners：格式同上。主要是为外网访问用的，如果clients在内网环境访问不需要配置这个参数。 auto.create.topics.enable：是否允许自动创建 Topic。建议 false，方便管理。 🌟unclean.leader.election.enable：是否允许 Unclean Leader 选举。建议 false，防止落后进度太多的副本竞选 Leader。 auto.leader.rebalance.enable：是否允许定期进行 Leader 选举。建议 false，防止无意义地换 Leader 导致性能降低。 log.retention.{hour|minutes|ms}：控制一条消息数据被保存多长时间。从优先级上来说 ms 设置最高、minutes 次之、hour 最低。比如log.retention.hour=168表示默认保存 7 天的数据 log.retention.bytes：指定 Broker 为消息保存的总磁盘容量大小。默认值为 -1，不作限制，使用云上 Kafka 服务时，需要做一定限制。 message.max.bytes：控制 Broker 能够接收的最大消息大小。默认值为 1000012，不到1MB，建议设置一个较大值。 Topic 级别参数 Topic 级别参数会覆盖全局 Broker 参数的值。 retention.ms：规定了该 Topic 消息被保存的时长，默认是 7 天。 retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。 max.message.bytes：决定了 Kafka Broker 能够正常接收该 Topic 的最大消息大小 设置时机： 创建 Topic 时：bin/kafka-topics.sh --bootstrap -server localhost:9092 --create --topictransaction --partitions1 --replication -factor1 --configretention.ms=15552000000 --configmax.message.bytes=5242880 修改 Topic 时（建议）：bin/kafka-configs.sh --zookeeperlocalhost:2181 --entity -typetopics --entity -nametransaction --alter --add -configmax.message.bytes=10485760 JVM 参数 堆内存：建议 6G export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g GC：优先使用 G1 G1：JDK 8 直接使用，具有更少的 Full GC，需要调整的参数更少 -XX:+UseCurrentMarkSweepGC：JDK 8 以下在 Broker 所在机器的 CPU 资源非常充裕时使用 -XX:+UseParallelGC export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true 操作系统参数 文件描述符限制：设置一个超大上限，如：ulimit -n 1000000 文件系统类型：XFS \u003e ext4 swappniess 调优：建议配置成一个接近 0 但不为 0 的值，比如 1，这样能够观测到 Broker 性能开始出现急剧下降，从而给你进一步调优和诊断问题的时间。 提交时间：向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期（由提交时间决定）将页缓存上的“脏”数据落盘到物理磁盘上。默认值为 5 秒，建议调大提交间隔去换取性能。 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:2:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"Kafka 客户端 分区策略 如果指定了 Key，那么默认实现按消息键保序策略； 如果没有指定 Key，则使用轮询策略； 针对那些大规模的 Kafka 集群，可使用基于地理位置的分区策略 数据压缩算法 Producer 端压缩、Broker 端保持、Consumer 端解压缩。 然而，Broker 端仍然会发生消息格式转换时发生的解压缩，这种消息校验是必要的。 应当尽量规避掉为了兼容老版本而引入的解压缩。 GZIP Snappy LZ4：吞吐量最高 Zstandard （简写为 zstd）：从 Kafka 2.1.0 开始支持。它是 Facebook 开源的一个压缩算法，能够提供超高的压缩比（compression ratio） benchmark（基准测试）结果： 在吞吐量方面：LZ4 \u003e Snappy \u003e zstd 和 GZIP； 而在压缩比方面，zstd \u003e LZ4 \u003e GZIP \u003e Snappy 消息无丢失 🌟Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证。 设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。 设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries \u003e 0 的 Producer 能够自动重试消息发送，避免消息丢失。 对于 Broker 端的参数，设置消息保存份数 replication.factor \u003e= 3，设置消息写入副本数 min.insync.replicas \u003e 1，可以提升消息持久性，推荐设置成 replication.factor = min.insync.replicas + 1 Producer 端丢失数据 永远要使用带有回调通知的发送 API 不要使用 producer.send(msg)，这是“fire and forget”，即“发射后不管”； 而要使用 producer.send(msg, callback) Consumer 端丢失数据 保证先消费消息，后更新位移 如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移，即enable.auto.commit = false Kafka 拦截器 Producer 端 拦截器实现类都要继承接口： org.apache.kafka.clients.producer.ProducerInterceptor onSend：该方法会在消息发送之前被调用。 onAcknowledgement：该方法会在消息成功提交或发送失败之后被调用，早于 callback 的调用。这个方法处在 Producer 发送的主路径中，所以最好别放一些太重的逻辑进去，否则你会发现你的 Producer TPS 直线下降。 Consumer 端 拦截器实现类都要继承借口： org.apache.kafka.clients.consumer.ConsumerInterceptor onConsume：该方法在消息返回给 Consumer 程序之前（正式处理消息之前）调用。 onCommit：Consumer 在提交位移之后调用该方法。通常你可以在该方法中做一些记账类的动作，比如打日志等。 使用场景 端到端系统性能检测、消息审计 Kafka 采用 TCP 协议 Producer 启动时会首先创建与bootstrap.servers 中所有 Broker 的 TCP 连接，所以在实际使用过程中，并不建议把集群中所有的 Broker 信息都配置到 bootstrap.servers 中，通常你指定 3～4 台就足以了。因为 Producer 一旦连接到集群中的任一台 Broker，就能拿到整个集群的 Broker 信息，故没必要为 bootstrap.servers 指定所有的 Broker。 TCP 连接创建时机 在创建 KafkaProducer 实例时 在更新元数据后 当 Producer 尝试给一个不存在的主题发送消息时，Broker 会告诉 Producer 说这个主题不存在。此时 Producer 会发送 METADATA 请求给 Kafka 集群，去尝试获取最新的元数据信息。 Producer 通过metadata.max.age.ms 参数定期地去更新元数据信息。该参数的默认值是 300000，即 5 分钟，也就是说不管集群那边是否有变化，Producer 每 5 分钟都会强制刷新一次元数据以保证它是最及时的数据。 在消息发送时 TCP 连接关闭时机 用户主动关闭 producer.close() 或 KafkaConsumer.close() （推荐） kill -9 进程 Kafka 自动关闭 Producer 端参数 connections.max.idle.ms的值，默认9分钟，即如果在 9 分钟内没有任何请求“流过”某个 TCP 连接，那么 Kafka 会主动帮你把该 TCP 连接关闭。若改为 -1，TCP 连接将成为永久长连接。 TCP 连接是在 Broker 端被关闭的，但其实这个 TCP 连接的发起方是客户端，因此在 TCP 看来，这属于被动关闭的场景，即 passive close。被动关闭的后果就是会产生大量的 CLOSE_WAIT 连接，因此 Producer 端或 Client 端没有机会显式地观测到此连接已被中断。 Consumer 端管理 TCP 和生产者不同的是，构建 KafkaConsumer 实例时是不会创建任何 TCP 连接的。 TCP 连接是在调用 KafkaConsumer.poll 方法时被创建的： 发起 FindCoordinator 请求时：希望 Kafka 集群告诉它哪个 Broker 是管理它的协调者，单向选择待发送请求最少的 Broker； 连接协调者时：消费者知晓了真正的协调者后，会创建连向该 Broker 的 Socket 连接 消费数据时 消息交付可靠性保障 最多一次（at most once）：消息可能会丢失，但绝不会被重复发送。 至少一次（at least once）：消息不会丢失，但有可能被重复发送。 精确一次（exactly once）：消息不会丢失，也不会被重复发送。 幂等性 Producer props.put(“enable.idempotence”, true)：Producer 自动升级成幂等性 Producer，只能够保证某个主题的一个分区上不出现重复消息，但无法实现多个分区的幂等性；其次，它只能实现单会话上的幂等性，不能实现跨会话的幂等性。这里的会话，你可以理解为 Producer 进程的一次运行。当你重启了 Producer 进程之后，这种幂等性保证就丧失了。 事务型 Producer 事务提供的安全性保障是经典的 ACID 原子性（Atomicity） 一致性 (Consistency) 隔离性 (Isolation) ：并发执行的事务彼此相互隔离，互不影响 isolation.level 参数： read_uncommitted：这是默认值，表明 Consumer 能够读取到 Kafka 写入的任何消息，不论事务型 Producer 提交事务还是终止事务，其写入的消息都可以读取。很显然，如果你用了事务型 Producer，那么对应的 Consumer 就不要使用这个值。 read_committed：表明 Consumer 只会读取事务型 Producer 成功提交事务写入的消息。当然了，它也能看到非事务型 Producer 写入的所有消息。Kafka 主要是在已提交读（read committed）这一隔离级别上做事情。 持久性 (Durability) 设置事务型 Producer 的方法 和幂等性 Producer 一样，开启 enable.idempotence = true 设置 Producer 端参数 transctional. id producer.initTransactions(); try { producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction(); } catch (KafkaException e) { producer.abortTransaction(); } 消费者组 Consumer Group 点对点模型：下游多个 Consumer 都要抢夺共享消息队列的消息，伸缩性（scalability）差 发布/订阅模型：每个订阅者都必须要订阅主题的所有分区 消费者组：组内的每个实例不要求一定要订阅主题的所有分区。理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数。 点对点模型：所有消息都属于同一个 Group 发布/订阅模型：所有实例都属于不同的 Group 位移保存在 Broker 端的内部主题__consumer_offsets中 进度监控 滞后程度：Consumer Lag ，指消费者落后于生产者的程度。 使用 Kafka 自带的命令行工具 kafka-consumer-groups 脚本。（最简单） bin/kafka-consumer-groups.sh(bat) 使用 Kafka Java Consumer API 编程。 public static Map\u003cTopicPartition, Long\u003e lagOf(String groupID, String bootstrapServers) throws TimeoutException { Properties props = new Properties(); props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); try (AdminClient client = AdminClient.create(props)) { ListConsumerGroupOffsetsResult result = client.listConsumerGroupOffsets(groupID);// 🌟获取给定消费者组的最新消费消息的位移 try { Map\u003cTopicPartition, OffsetAndMetadata\u003e consumedOffsets = result.partitionsToOffsetAndMetadata().get(10, TimeUnit.SECONDS); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // 禁止自动提交位移 props.put(ConsumerConfig.GROUP_ID_CONFIG, groupID); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); try (final KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003c\u003e(props)) { Map\u003cTopicPartition, Long\u003e endOffsets = consumer.endOffsets(consumedOffsets.keySet());// 🌟获取订阅分区的最新消息位移 return endOffsets.entrySet().stream().collect(Collectors.toMap(entry -\u003e entry.getKey(), entry -\u003e entry.getValue() - consumedOffsets.get(entry.getKey()).offset()));// 🌟执行相应的减法操作，获取 Lag 值并封装进一个 Map 对象 } } catch (InterruptedException e) { Thread.currentThread().interrupt(); // 处理中断异常 // ... return Collections.emptyMap(); } catch (ExecutionException e) { // 处理 ExecutionException // ... return Collections.emptyMap(); } catch (TimeoutException e) { throw new TimeoutException(\"Timed out when getting lag for consumer group \" + groupID); } } } 使用 Kafka 自带的 JMX 监控指标。（集成性最好） Kafka 消费者提供了一个名为 kafka.consumer:type=consumer-fetch-manager-metrics,client-id=“{client-id}”的 JMX 指标，records-lag-max 和 records-lead-min 分别表示此消费者在测试窗口时间内曾经达到的最大的 Lag 值和最小的 Lead 值（ Lead 值是指消费者最新消费消息的位移与分区当前第一条消息位移的差值，Lag 与 Lead 的值呈负相关），Lag 越大，Lead 越小，就意味着消费者端要丢消息了。 位移主题 __consumer_offsets 核心数据结构 键值对，类似于Map\u003cTopicPartition, Long\u003e key：\u003cGroup ID，主题名，分区号 \u003e value：\u003c位移，时间戳，用户自定义数据\u003e，保存这些元数据是为了帮助 Kafka 执行各种各样后续的操作，比如删除过期位移消息等。 该内部主题下还有两种消息格式： 用来注册 Consumer Group 的消息； tombstone （墓碑）消息：消息体是 null，用于删除 Group 过期位移甚至是删除 Group 的消息。 分区 分区数配置：Broker 端参数 offsets.topic.num.partitions，默认值 50； 副本数：Broker 端参数 offsets.topic.replication.factor ，默认值 3。 Compaction 整理位移 过期消息：对于同一个 Key 的两条消息 M1 和 M2，如果 M1 的发送时间早于 M2，那么 M1 就是过期消息。 Log Cleaner 线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据。 Rebalance 所有 Broker 都有各自的 Coordinator 组件，具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的 Broker 提交位移。同样地，当 Consumer 应用启动时，也是向 Coordinator 所在的 Broker 发送各种请求，然后由 Coordinator 负责执行消费者组的注册、成员管理记录等元数据管理操作。 Kafka 为某个 Consumer Group 确定 Coordinator 所在的 Broker 的算法有 2 个步骤： 确定由位移主题的哪个分区来保存该 Group 数据：partitionId=Math.abs(groupId.hashCode() % offsetsTopicPartitionCount) 找出该分区 Leader 副本所在的 Broker，该 Broker 即为对应的 Coordinator 避免 Rebalance 方法（Consumer 端）： 检测心跳间隔： session.timeout.ms，默认值为 10 秒，推荐设为 6 秒 发送心跳间隔：heartbeat.interval.ms 越小，频率越高，会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启 Rebalance，推荐设为 2 秒 实际消费能力：max.poll.interval.ms，限定了应用程序两次调用 poll 方法的最大时间间隔，默认值为 5 分钟。最好将该参数值设置得大一点，比下游最大处理时间稍长一点。就拿 MongoDB 这个例子来说，如果写 MongoDB 的最长时间是 7 分钟，那么你可以将该参数设置为 8 分钟左右。 GC 参数：查看是否出现了频繁的 Full GC 消费者位移 它记录了 Consumer 要消费的下一条消息的位移，如果你提交了位移 X，那么 Kafka 会认为所有位移值小于 X 的消息你都已经成功消费了。 🌟自动提交：enable.auto.commit = true，提交间隔为auto.commit.interval.ms Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"true\"); props.put(\"auto.commit.interval.ms\", \"2000\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003c\u003e(props); consumer.subscribe(Arrays.asList(\"foo\", \"bar\")); while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(100); for (ConsumerRecord\u003cString, String\u003e record : records) System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } 手动提交：enable.auto.commit = false，使用 consumer.commitSync（阻塞）或 consumer.commitAsync（异步） // 同步：利用 commitSync 的自动重试来规避那些瞬时错误，比如网络的瞬时抖动，Broker 端 GC 等 while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 try { consumer.commitSync(); } catch (CommitFailedException e) { handle(e); // 处理提交失败异常 } } // 异步：常规性、阶段性的手动提交 while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 consumer.commitAsync((offsets, exception) -\u003e { if (exception != null) handle(exception); }); } // 二者结合：既实现了异步无阻塞式的位移管理，也确保了 Consumer 位移的正确性 try { while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 commitAysnc(); // 使用异步提交规避阻塞 } } catch (Exception e) { handle(e); // 处理异常 } finally { try { consumer.commitSync(); // 最后一次提交使用同步阻塞式提交 } finally { consumer.close(); } } // 更加细粒度的操作 // Kafka Consumer API 为手动提交提供了这样的方法： // commitSync(Map\u003cTopicPartition, OffsetAndMetadata\u003e) 和 // commitAsync(Map\u003cTopicPartition, OffsetAndMetadata\u003e)。 // 它们的参数是一个 Map 对象，键就是 TopicPartition，即消费的分区，而值是一个 OffsetAndMetadata 对象，保存的主要是位移数据。 private Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets = new HashMap\u003c\u003e(); int count = 0; …… while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord\u003cString, String\u003e record: records) { process(record); // 处理消息 offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)； if（count % 100 == 0）// 每累计 100 条消息就统一提交一次位移 consumer.commitAsync(offsets, null); // 回调处理逻辑是null count++; } } CommitFailedException 异常 解决办法： 缩短单条消息处理的时间，简化消费逻辑；（优先） 增加期望的时间间隔 max.poll.interval.ms 参数值； 减少下游系统一次性消费的消息总数，减少 poll 方法一次性返回的消息数量，即减少max.poll.records 参数值； 下游系统使用多线程来加速消费 让独立消费者与消费者组的 group.id 保持不同 Kafka Java Consumer 用户主线程 + 心跳线程，非阻塞式的消息获取 在消费消息这一层面，仍然是单线程设计。 KafkaConsumer 类不是线程安全的，不能在多个线程中共享同一个 KafkaConsumer 实例，否则程序会抛出 ConcurrentModificationException 异常，除了 KafkaConsumer.wakeup() 方法。 两种多线程方案： 消费者程序启动多个线程，每个线程维护专属的 KafkaConsumer 实例，负责完整的消息获取、消息处理流程。 优点：实现简单，能保证分区内的消费顺序 缺点：占用更多资源，可能会出现不必要的 Rebalance // 每个 KafkaConsumerRunner 类都会创建一个专属的 KafkaConsumer 实例 public class KafkaConsumerRunner implements Runnable { private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() { try { consumer.subscribe(Arrays.asList(\"topic\")); while (!closed.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(10000)); // 执行消息处理逻辑 } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } finally { consumer.close(); } } // Shutdown hook which can be called from a separate thread public void shutdown() { closed.set(true); consumer.wakeup(); } 单线程获取消息，多线程消费 优点：伸缩性强，消息的获取与处理解耦 缺点：实现难度大，无法保证分区内的消费顺序，可能出现重复消费（位移提交困难） // 当 Consumer 的 poll 方法返回消息后，由专门的线程池来负责处理具体的消息 // 调用 poll 方法的主线程不负责消息处理逻辑 private final KafkaConsumer\u003cString, String\u003e consumer; private ExecutorService executors; ... private int workerNum = ...; executors = new ThreadPoolExecutor( workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue\u003c\u003e(1000), new ThreadPoolExecutor.CallerRunsPolicy()); ... while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (final ConsumerRecord record : records) { executors.submit(new Worker(record)); } } .. ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:3:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"Kafka 核心设计原理 副本机制 副本的功能： 提供数据冗余（Kafka，以下两项未提供） 提供高伸缩性 改善数据局部性 副本（Replica）：本质上是一个只能追加写消息的提交日志。 追随者副本不对外提供服务（因此不能提供读操作横向扩展以及改善局部性），只向领导者副本异步拉取消息，并写入到自己的提交日志中。 Kafka 副本机制的好处： Read-your-writes：立即能读取自己写入的消息 单调读（Monitonic Reads）：对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。 ISR（In-sync Replicas） 指的是与 Leader 副本同步的副本，包括 Leader 本身，是一个动态集合。 同步与否，取决于 Broker 端参数 replica.lag.time.max.ms 参数值，表示 Follower 副本能够落后 Leader 副本的最长时间间隔，当前默认值是 10 秒。 Unclean 领导者选举 Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举，开启可能会造成数据丢失，关闭可能会使得 ISR 集合为空。 CAP 理论：以下三者只能满足其二： 一致性（Consistency）：关闭 Unclean 领导者选举，可避免消息丢失（建议） 可用性（Availability）：开启 Unclean 领导者选举，可保证对外提供服务（还可以通过增加副本数来提高可用性，所以建议在 Unclean 上选择一致性） 分区容错性（Partition tolerance） Kafka 请求处理 Reactor 模式 epoll 是一种 IO 模型，而 Reactor 是一种 IO 处理模式。 作者 Doug Lea 曾开发整个 java.util.concurrent 包。 Acceptor 线程：只是用于请求分发，不涉及具体的逻辑处理，非常得轻量级，因此有很高的吞吐量表现。 Broker 端参数 num.network.threads：用于调整该网络线程池的线程数。其默认值是 3，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求。 异步线程池 当网络线程拿到请求后，它不是自己处理，而是将请求放入到一个共享请求队列中。 IO 线程池：线程数由 Broker 端参数num.io.threads控制 PRODUCE 生产请求：将消息写入到底层的磁盘日志中 FETCH 请求：从磁盘或页缓存中读取消息 请求队列是所有网络线程共享的，而响应队列则是每个网络线程专属的，因为Dispatcher 只是用于请求分发而不负责响应回传，所以只能让每个网络线程自己发送 Response 给客户端。 Kafka Broker 启动后，会在后台分别创建两套网络线程池和 IO 线程池的组合，它们分别处理数据类请求和控制类请求，因此 listeners 配置可以配置两个端口号。 Purgatory 组件 “炼狱”组件，用来缓存延迟请求，比如设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。当请求不能立刻处理时，它就会暂存在 Purgatory 中。 🌟 Rebalance 重平衡 Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。触发条件： 组成员数发生变更； 订阅主题数发生变更； 订阅主题的分区数发生变更。 弊端： 影响 Consumer 端 TPS，类似 JVM 的 STW，所有 Consumer 实例都会停止消费； 需要重新创建连接其他 Broker 的 Socket 资源，Group 下的所有成员都要参与进来，而且通常不会考虑局部性原理，但局部性原理对提升系统性能是特别重要的； 速度很慢，几百个 Consumer 实例的 Rebalance 需要几个小时。 消费者端有消息处理线程和心跳线程，重平衡的通知机制是通过心跳线程来完成的。heartbeat.interval.ms既是心跳的间隔时间，也是控制重平衡通知的频率。 消费者组的 5 种状态 Empty：会进行过期位移删除：Removed ✘✘✘ expired offsets in ✘✘✘ milliseconds. Dead PreparingRebalance：等待成员加入 CompletingRebalance：老版本中叫 AwatingSync，等待分配方案 Stable：重平衡已完成 消费者端 2 类请求 JoinGroup 请求：第一个发送 JoinGroup 请求的成员自动成为领导者 此处的领导者不同于领导者副本，它们不是一个概念。这里的领导者是具体的消费者实例，它既不是副本，也不是协调者。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。 SyncGroup 请求 Broker 端 LeaveGroup 请求：主动离组 session.timeout.ms：崩溃离组 Kafka 控制器 zookeeper znode 是树形结构上的节点，分为持久性和临时性。 节点变更监听器 (ChangeHandler) ：监控（Watch） znode 变更行为，实现集群成员管理、分布式锁、领导者选举等功能。 Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个成功创建 /controller 节点的 Broker 会被指定为控制器。 控制器职责： 主题管理（创建、删除、增加分区）：执行 kafka-topic 脚本 分区重分配：执行 kafka-reassign-partitions 脚本 Preferred 领导者选举 集群成员管理（新增 Broker、Broker 主动关闭、Broker 宕机） 数据服务：在 ZooKeeper 中也保存了一份 控制器内部设计原理： 单线程+事件队列方案； 异步操作 Zookeeper； 赋予 StopReplica 请求更高的优先级，使它能够得到抢占式的处理。 控制器组件出问题的解决方案： 在 Zookeeper 中手动删除 /controller 节点：rmr /controller，这样既可以引发控制器的重选举，又可以避免重启 Broker 导致的消息处理中断。 高水位 示意图： ｜—— LEO —— ｜未提交消息，不可被消费 ｜—— 高水位 —— ｜已提交消息 高水位的作用： 定义消息可见性 完成副本同步 日志位移： LSO：Log Stable Offset， Kafka 事务通过 LSO来判断消息可见性 LEO：Log End Offset，日志末端位移，表示副本写入下一条消息的位移值。 Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。 Leader Epoch 数据结构： Epoch：版本号，小版本号被认为过期 Start Offset：起始位移 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:4:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"Kafka 运维与监控 主题管理 # 创建主题 bin/kafka-topics.sh --bootstrap-server broker_host:port --create --topic my_topic_name --partitions 1 --replication-factor 1 # 查询主题 bin/kafka-topics.sh --bootstrap-server broker_host:port --list bin/kafka-topics.sh --bootstrap-server broker_host:port --describe --topic \u003ctopic_name\u003e # 修改主题分区 bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic \u003ctopic_name\u003e --partitions \u003c 新分区数 \u003e # 修改主题级别 # 设置动态参数，使用 --bootstrap-server # 设置常规的主题级别参数，还是使用 --zookeeper。 bin/kafka-configs.sh --zookeeper zookeeper_host:port --entity-type topics --entity-name \u003ctopic_name\u003e --alter --add-config max.message.bytes=10485760 # 修改主题限制 bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config 'leader.replication.throttled.rate=104857600,follower.replication.throttled.rate=104857600' --entity-type brokers --entity-name 0 # Broker ID bin/kafka-configs.sh --zookeeper zookeeper_host:port --alter --add-config 'leader.replication.throttled.replicas=*,follower.replication.throttled.replicas=*' --entity-type topics --entity-name test # 主题分区迁移 bin/kafka-topics.sh --bootstrap-server broker_host:port --delete --topic \u003ctopic_name\u003e 内部主题： __consumer_offsets __transaction_state：为支持事务引入 动态 Broker 参数配置 初始化配置在config/server.properties中， Dynamic Update Mode 列（属性的三种级别）： read-only：只有重启 Broker，才能令修改生效。 per-broker：只会在对应的 Broker 上生效。 如 listeners cluster-wide：会在整个集群范围内生效，可对所有 Broker 都生效。 如 log.retention.ms # 配置 bin/kafka-configs.sh --bootstrap-server kafka-host:port --entity-type brokers --entity-default --alter --add-config unclean.leader.election.enable=true # 查看 bin/kafka-configs.sh --bootstrap-server kafka-host:port --entity-type brokers --entity-default --describe 可能被动态调整的参数： log.retention.ms：修改日志留存时间 num.io.threads 和 num.network.threads：两组线程池 ssl.keystore.type、ssl.keystore.location、ssl.keystore.password 和 ssl.key.password：创建那些过期时间很短的 SSL 证书，增加安全性 num.replica.fetchers：增加该参数值，可确保有充足的线程可以执行 Follower 副本向 Leader 副本的拉取 重设消费者位移 Kafka 优点：提供较高吞吐量，消息短，能保证消息的顺序。 重设策略： 位移维度 Earliest：最早位移处 Latest：最新位移处 Current：最新提交位移处 Specified-Offset：特定位移处（绝对数值） Shift-By-N：调整到当前位移+N处（相对数值） 时间维度 DateTime：时刻处 Duration：距离当前时间间隔处，格式为 PnDTnHnMnS，如 15 分钟前可表示为 PT0H15M0S。 在 Java 中，通过consumer.seek(key,value) 方法重设策略。 运维利器 AdminClient \u003cdependency\u003e \u003cgroupId\u003eorg.apache.kafka\u003c/groupId\u003e \u003cartifactId\u003ekafka-clients\u003c/artifactId\u003e \u003cversion\u003e2.3.0\u003c/version\u003e \u003c/dependency\u003e 在 Java 中的对象为org.apache.kafka.clients.admin.AdminClient // 创建主题 String newTopicName = \"test-topic\"; try (AdminClient client = AdminClient.create(props)) { NewTopic newTopic = new NewTopic(newTopicName, 10, (short) 3); CreateTopicsResult result = client.createTopics(Arrays.asList(newTopic)); result.all().get(10, TimeUnit.SECONDS); } // 查询消费者位移 String groupID = \"test-group\"; try (AdminClient client = AdminClient.create(props)) { ListConsumerGroupOffsetsResult result = client.listConsumerGroupOffsets(groupID); Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets = result.partitionsToOffsetAndMetadata().get(10, TimeUnit.SECONDS); System.out.println(offsets); } // 获取 Broker 磁盘占用 try (AdminClient client = AdminClient.create(props)) { DescribeLogDirsResult ret = client.describeLogDirs(Collections.singletonList(targetBrokerId)); // 指定 Broker id long size = 0L; for (Map\u003cString, DescribeLogDirsResponse.LogDirInfo\u003e logDirInfoMap : ret.all().get().values()) { size += logDirInfoMap.values().stream().map(logDirInfo -\u003e logDirInfo.replicaInfos).flatMap( topicPartitionReplicaInfoMap -\u003e topicPartitionReplicaInfoMap.values().stream().map(replicaInfo -\u003e replicaInfo.size)) .mapToLong(Long::longValue).sum(); } System.out.println(size); } Kafka 认证机制 authentication SASL 机制：提供认证和数据安全服务的框架 FSSAPI：Kerberos 使用的安全接口 PLAIN：用户名+密码 SCRAM ：改进 PLAIN，将认证用户信息保存在 ZooKeeper，避免了动态修改需要重启 Broker 的弊端 OAUTHBEARER：基于 OAuth 2 认证框架 Delegation Token：轻量级 授权机制 Authorization 权限模型： ACL：Access-Control List，访问控制列表。（用户直接映射权限） 开启方法：authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer RBAC：Role-Based Access Control，基于角色的权限控制。 ABAC：Attribute-Based Access Control，基于属性的权限控制。 PBAC：Policy-Based Access Control，基于策略的权限控制。 监控 Kafka JVM 调优 Full GC 发生频率和时长 开启 G1 的 -XX:+PrintAdaptiveSizePolicy 开关，让 JVM 告诉你到底是谁引发了 Full GC。 活跃对象大小 应用线程总数 Broker 集群监控 5 个方法： 查看 Broker 进程是否启动，端口是否建立 查看 Broker 端关键日志 查看 Broker 端 2 类关键线程的状态 Log Compaction 线程，这类线程是以 kafka-log-cleaner-thread 开头的 副本拉取消息的线程，通常以 ReplicaFetcherThread 开头 查看 Broker 端端关键 JMX 指标 监控 Kafka 客户端：首要关心网络往返时延（Round-Trip Time，RTT） 主流监控框架 Kafka Manager 框架适用于基本的 Kafka 监控 Grafana+InfluxDB+JMXTrans 的组合适用于已经具有较成熟框架的企业，可以在一套框架中同时监控企业的doge关键技术组件 调优 Kafka 目标：高吞吐量、低延迟 优化漏斗 层级越靠上，调优效果越明显： 应用程序层 不要频繁创建对象实例，多复用 用完及时关闭 合理利用多线程，Kafka 的 Java Producer 是线程安全的，Java Consumer 虽不是线程安全的，也有应对方案（见前文）。 框架层：尽力保持客户端版本和 Broker 端版本一致 JVM 层 堆设置：6～8 GB，或 Full GC 之后堆上存活对象的总大小的 1.5～2 倍 GC 收集器：选择 G1 收集器 配置-XX:+PrintAdaptiveSizePolicy，来探查一下到底是谁导致的 Full GC。 配置-XX:+G1HeapRegionSize=N，N越大，大对象数越少。 操作系统层 挂在文件系统时禁掉 atime 更新，避免 inode 访问时间的写入操作，减少文件系统的写操作数。命令：mount -o noatime 选择 ext4 或 XFS 文件系统 swap 空间设置成一个很小的值，比如 1～10 之间，以防止 Linux 的 OOM Killer 开启随意杀掉进程 页缓存大小预留出一个日志段大小，至少能保证 Kafka 可以将整个日志段全部放入页缓存 ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:5:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["读书笔记"],"content":"Kafka Streams 流处理：处理无限数据集 Kafka Streams 的特点 它是一个Java 客户端库（Client Library），更轻量级 应用部署方面：Kafka Streams 更倾向于将部署交给开发人员来做，而不是依赖于框架自己实现。 上下游数据源：Kafka Streams 只支持与 Kafka 集群进行交互，它没有提供开箱即用的外部数据源连接器。 协调方式：通过消费者组实现高伸缩性和高容错性 Kafka Streams 与 Kafka 的适配性最好 DSL ：Domain Specific Language，领域特定语言 流表二元性：流在时间维度上聚合之后形成表，表在时间维度上不断更新形成流，这就是所谓的流表二元性（Duality of Streams and Tables） 常见操作算子 无状态算子 filter：过滤，如.filter(((key, value) -\u003e value.startsWith(\"s\"))) map：kv对，如 KStream\u003cString, Integer\u003e transformed = stream.map( (key, value) -\u003e KeyValue.pair(value.toLowerCase(), value.length())); print 是终止操作，peek 还能继续处理 有状态算子：涉及聚合方面操作 金融领域应用 Avro： 是 Java 或大数据生态圈常用的序列化编码机制，能极大地节省磁盘占用空间或网络 I/O 传输量 识别用户身份信息： 身份证号 手机号 设备 ID 应用注册账号 Cookie ","date":"2023-08-08","objectID":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/:6:0","tags":["MQ"],"title":"《Kafka核心技术与实战》笔记","uri":"/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"},{"categories":["draft"],"content":"1. 3PC 相比 2PC 改进在哪？ 3PC在事务管理者和事务参与者中引入了超时机制，超过响应时间默认失败，以避免事务参与者一直阻塞占用资源。（性能下降）在2PC中直邮事务管理者才拥有超时机制。 实际应用广泛的：2PC+复制状态机 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:1","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"2. TCC 与 2PC/3PC 的区别？ TCC ：Try - Confirm - Cancel，一种柔性事务解决方案 2PC/3PC 依靠数据库或者存储资源层面的事务，TCC 主要通过修改业务代码来实现。 2PC/3PC 属于业务代码无侵入的，TCC 对业务代码有侵入。 2PC/3PC 追求的是强一致性，在两阶段提交的整个过程中，一直会持有数据库的锁。TCC 追求的 是最终一致性，不会一直持有各个业务资源的锁。 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:2","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"3. 分布式事务分类：柔性事务和刚性事务 柔性事务：TCC/FMT、Saga（状态机模式、Aop模式）、本地事务消息、消息事务（半消息） 满足 BASE 理论 有业务改造，最终⼀致性，实现补偿接⼝，实现资源锁定接⼝，⾼并发，适合⻓事务 细分为两种： 补偿型事务（同步）：TCC、Saga TCC 把事务运⾏过程分成 Try、Confirm / Cancel 两个阶段，每个阶段的逻辑由业务代码控制，避免了⻓事务，可以获取更⾼的性能。 通知型事务（异步）：MQ事务消息、最⼤努⼒通知型。 外部系统：最大努力通知型因为外部的⽹络环境更加复杂和不可信，所以只能尽最⼤努⼒去通知实现数据最终⼀致性，⽐如充值平台与运营商、⽀付对接等等跨⽹络系统级别对接； 内部系统：异步确保型，因为内部相对⽐较可控，如订单和购物⻋、收货与清算、⽀付与结算等等场景； 刚性事务：XA 协议（2PC、JTA、JTS）、3PC 满足 CP 理论 通常⽆业务改造，强⼀致性，原⽣⽀持回滚/隔离性，低并发，适合短事务 但由于同步阻塞，处理效率低，不适合⼤型⽹站分布式场景 最⼤努⼒通知事务 VS 异步确保型事务 最⼤努⼒通知事务在我认知中，其实是基于异步确保型事务发展⽽来适⽤于外部对接的⼀种业务实现。他们主要有的是业务差别，如下： 从参与者来说：最⼤努⼒通知事务适⽤于跨平台、跨企业的系统间业务交互；异步确保型事务更适⽤于同⽹络体系的内部服务交付。 从消息层⾯说：最⼤努⼒通知事务需要主动推送并提供多档次时间的重试机制来保证数据的通知；⽽异步确保型事务只需要消息消费者主动去消费。 从数据层⾯说：最⼤努⼒通知事务还需额外的定期校验机制对数据进⾏兜底，保证数据的最终⼀致性；⽽异步确保型事务只需保证消息的可靠投递即可，⾃身⽆需对数据进⾏兜底处理。 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:3","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"4. 刚性事务 - XA 模型 这个模型主要使⽤了 2PC 来保证分布式事务的完整性。 在X/Open **DTP(Distributed Transaction Process)**模型⾥⾯，有三个⻆⾊： AP: Application，应⽤程序。也就是业务层。哪些操作属于⼀个事务，就是AP定义的。 TM: Transaction Manager，事务管理器。接收AP的事务请求，对全局事务进⾏管理，管理事务分⽀状态，协调RM的处理，通知RM哪些操作属于哪些全局事务以及事务分⽀等等。这个也是整个事务调度模型的核⼼部分。 RM：Resource Manager，资源管理器。⼀般是数据库，也可以是其他的资源管理器，如消息队列(如JMS数据源)，⽂件系统等。 XA规范主要定义了(全局)事务管理器(Transaction Manager)和(局部)资源管理器(Resource Manager)之间的接⼝。XA接⼝是双向的系统接⼝，在事务管理器（Transaction Manager）以及⼀个或多个资源管理器（Resource Manager）之间形成通信桥梁，从⽽在多个数据库资源下保证ACID四个特性。⽬前知名的数据库，如Oracle, DB2,mysql等，都是实现了XA接⼝的，都可以作为RM。 XA之所以需要引⼊事务管理器是因为，在分布式系统中，从理论上讲（参考Fischer等的论⽂），两台机器理论上⽆法达到⼀致的状态，需要引⼊⼀个单点进⾏协调。事务管理器控制着全局事务，管理事务⽣命周期，并协调资源。资源管理器负责控制和管理实际资源（如数据库或JMS队列) ","date":"2023-08-08","objectID":"/distributed-transaction/:0:4","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"5. XA规范 以下的函数使事务管理器可以对资源管理器进⾏的操作： xa_open,xa_close：建⽴和关闭与资源管理器的连接。 xa_start,xa_end：开始和结束⼀个本地事务。 xa_prepare,xa_commit,xa_rollback：预提交、提交和回滚⼀个本地事务。 xa_recover：回滚⼀个已进⾏预提交的事务。 ax_开头的函数使资源管理器可以动态地在事务管理器中进⾏注册，并 可以对XID(TRANSACTION IDS)进⾏操作。 ax_reg,ax_unreg；允许⼀个资源管理器在⼀个TMS(TRANSACTION MANAGER SERVER)中动态注册或撤消注册。 XA各个阶段的处理流程 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:5","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"6. JTA规范 作为java平台上事务规范 JTA（Java Transaction API）也定义了对XA事务的⽀持，实际上，JTA是基于XA架构上建模的，在JTA 中，事务管理器抽象为javax.transaction.TransactionManager接⼝，并通过底层事务服务 （即JTS）实现。像很多其他的java规范⼀样，JTA仅仅定义了接⼝，具体的实现则是由供应商(如J2EE⼚商)负责提供。 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:6","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"7. TCC TCC(Try-Confirm-Cancel)分布式事务模型相对于 XA 等传统模型，其特征在于它不依赖资源管理器(RM)对分布式事务的⽀持，⽽是通过对业务逻辑的分解来实现分布式事务。 TCC 模型认为对于业务系统中⼀个特定的业务逻辑，其对外提供服务时，必须接受⼀些不确定性，即对业务逻辑初步操作的调⽤仅是⼀个临时性操作，调⽤它的主业务服务保留了后续的取消权。如果主业务服务认为全局事务应该回滚，它会要求取消之前的临时性操作，这就对应从业务服务的取消操作。⽽当主业务服务认为全局事务应该提交时，它会放弃之前临时性操作的取消权，这对应从业务服务的确认操作。每⼀个初步操作，最终都会被确认或取消。 TCC 分布式事务模型包括三部分： Try 阶段： 调⽤ Try 接⼝，尝试执⾏业务，完成所有业务检查，预留业务资源。 Confirm 或 Cancel 阶段： 两者是互斥的，只能进⼊其中⼀个，并且都满⾜幂等性，允许失败重试。 Confirm 操作： 对业务系统做确认提交，确认执⾏业务操作，不做其他业务检查，只使⽤ Try 阶段预留的业务资源。 Cancel 操作： 在业务执⾏错误，需要回滚的状态下执⾏业务取消，释放预留资源。 Try 阶段失败可以 Cancel，如果 Confirm 和 Cancel 阶段失败了怎么办？ TCC 中会添加事务⽇志，如果 Confirm 或者 Cancel 阶段出错，则会进⾏重试，所以这两个阶段需要⽀持幂等；如果重试失败，则需要⼈⼯介⼊进⾏恢复和处理等。 TCC事务模型的要求 可查询操作：服务操作具有全局唯⼀的标识，操作唯⼀的确定的时间。 幂等操作：重复调⽤多次产⽣的业务结果与调⽤⼀次产⽣的结果相同。 通过业务操作实现幂等性 系统缓存所有请求与处理的结果，最后是检测到重复请求之后，⾃动返回之前的处理结果。 TCC事务模型 VS DTP事务模型 ⽐较⼀下TCC事务模型和DTP事务模型，如下所示： 这两张图看起来差别较⼤，实际上很多地⽅是类似的! TCC模型中的 主业务服务 相当于 DTP模型中的AP，TCC模型中的从业务服务 相当于 DTP模型中的RM 在DTP模型中，应⽤AP操作多个资源管理器RM上的资源；⽽在TCC模型中，是主业务服务操作多个从业务服务上的资源。例如航班预定案例中，美团App就是主业务服务，⽽川航和东航就是从业务服务，主业务服务需要使⽤从业务服务上的机票资源。不同的是DTP模型中的资源提供者是类似于Mysql这种关系型数据库，⽽TCC模型中资源的提供者是其他业务服务。 TCC模型中，从业务服务提供的try、confirm、cancel接⼝ 相当于 DTP模型中RM提供的prepare、commit、rollback接⼝。XA协议中规定了DTP模型中定RM需要提供prepare、commit、rollback 接⼝给TM调⽤，以实现两阶段提交。⽽在TCC模型中，从业务服务相当于RM，提供了类似的try、confirm、 cancel接⼝。 事务管理器：DTP模型和TCC模型中都有⼀个事务管理器。不同的是：在DTP模型中，阶段1的(prepare)和阶段2的(commit、rollback)，都是由TM进⾏调⽤的。在TCC模型中，阶段1的try接⼝是主业务服务调⽤(绿⾊箭头)，阶段2的(confirm、cancel接⼝)是事务管理器TM调⽤(红⾊箭头)。这就是 TCC 分布式事务模型的⼆阶段异步化功能，从业务服务的第⼀阶段执⾏成功，主业务服务就可以提交完成，然后再由事务管理器框架异步的执⾏各从业务服务的第⼆阶段。这⾥牺牲了⼀定的隔离性和⼀致性的，但是提⾼了⻓事务的可⽤性。 TCC与2PC对⽐ XA是资源层⾯的分布式事务，强⼀致性，在两阶段提交的整个过程中，⼀直会持有资源的锁。基于数据库锁实现，需要数据库⽀持XA协议，由于在执⾏事务的全程都需要对相关数据加锁，⼀般⾼并发性能会⽐较差 TCC是业务层⾯的分布式事务，最终⼀致性，不会⼀直持有资源的锁，性能较好。但是对微服务的侵⼊性强，微服务的每个事务都必须实现try、confirm、cancel等3个⽅法，开发成本⾼，今后维护改造的成本也⾼为了达到事务的⼀致性要求，try、confirm、cancel接⼝必须实现幂等性操作由于事务管理器要记录事务⽇志，必定会损耗⼀定的性能，并使得整个TCC事务时间拉⻓。TCC它会弱化每个步骤中对于资源的锁定，以达到⼀个能承受⾼并发的⽬的（基于最终⼀致性）。 TCC 的使⽤场景 TCC是可以解决部分场景下的分布式事务的，但是，它的⼀个问题在于，需要每个参与者都分别实现Try，Confirm和Cancel接⼝及逻辑，这对于业务的侵⼊性是巨⼤的。 TCC ⽅案严重依赖回滚和补偿代码，最终的结果是：回滚代码逻辑复杂，业务代码很难维护。所以，TCC ⽅案的使⽤场景较少，但是也有使⽤的场景。 ⽐如说跟钱打交道的，⽀付、交易相关的场景，⼤家会⽤ TCC⽅案，严格保证分布式事务要么全部成功，要么全部⾃动回滚，严格保证资⾦的正确性，保证在资⾦上不会出现问题。 ","date":"2023-08-08","objectID":"/distributed-transaction/:0:7","tags":["Distributed"],"title":"Distributed Transaction - 分布式事务","uri":"/distributed-transaction/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/zookeeper_draft/","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zookeeper 是 chubby 的开源实现，使用 zab 协议，paxos 算法的变种。 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:0","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zookeeper提供了什么？ 文件系统：树状结构，每个节点存放数据上限为 1M 通知机制 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:1","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"四种数据节点 persistent 持久节点 ephemeral 临时节点 persistent sequential 持久顺序节点 ephemeral sequential 临时顺序节点 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:2","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"Watcher 机制 一次性：无论是服务端还是客户端，一旦一个 Watcher 被 触 发 ，Zookeeper 都会将 其从相应的存储中移除。 串行：客户端 Watcher 回调的过程是一个串行同步的过程。 轻量：Watcher 通知非常简单，只会告诉客户端发生了事件，而不会说明事件的 具体内容。客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象实体传递到服务端，仅仅是在客户端请求中使用 boolean 类型属性进行了 标记。 最终一致性：异步发送通知，无法保证强一致性。数据同步方式： diff：差异化同步 trunc+diff：先回滚，再差异化同步 trunc：回滚同步 snap：全量同步（快照） 客户端注册 watcher 调用 getData()/getChildren()/exist()三个 API，传入 Watcher 对象 标记请求 request，封装 Watcher 到 WatchRegistration 封装成 Packet 对象，发服务端发送 request 收到服务端响应后，将 Watcher 注册到 ZKWatcherManager 中进行管理 请求返回，完成注册。 服务端处理 watcher 封装 WatchedEvent：将通知状态（SyncConnected）、事件类型（NodeDataChanged）以及节点路径封装成一个 WatchedEvent 对象 查询 Watcher：从 WatchTable 中根据节点路径查找 Watcher 找到：提取并从 WatchTable 和 Watch2Paths 中删除对应 Watcher（从这里可以看出 Watcher 在服务端是一次性的，触发一次就失效了） 调用 process 方法来触发 Watcher：这里 process 主要就是通过 ServerCnxn 对应的 TCP 连接发送 Watcher 事件 通知。 客户端回调 watcher 客户端 SendThread 线程接收事件通知，交由 EventThread 线程回调 Watcher。 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:3","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"Server 工作状态 looking：寻找 leader following：当前服务器角色时 follower 处理客户端的非事务请求，转发事务给 leader 服务器 参与事务请求 proposal 的投票 参与 leader 选举投票 leading：当前服务器角色时 leader 事务请求的唯一调度和处理者，保证事务处理的顺序性 集群内部各服务的调度者 observing：当前服务器角色时 observer 处理客户端的非事务请求，转发事务给 leader 服务器 不参与任何形式的投票 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:4","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zookeeper 如何保证事务的顺序一致性？ 全局递增的事务ID，所有 proposal（提议）都在被提出时加上了 zxid，为 64 位数字： 高 32 位： epoch，用来标识 leader 周期 低 32 位：用来计数，新产生proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:5","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zk 节点宕机如何处理？ 集群规则为 2N+1 台，N\u003e0，至少 3 台。只要超过半数的节点正常，集群就能正常提供服务。 follower 宕机：数据不会丢失 leader 宕机：选举出新的 leader ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:6","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zk 负载均衡和 nginx 负载均衡的区别 zk：可以调控，吞吐量小 nginx：只能调权重，其他的需要自己写插件，吞吐量大 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:7","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["draft"],"content":"zk 的应用场景 分布式数据的发布和订阅： 数据发布/订阅：配置中心，动态获取数据，数据量通常较小（如数据库配置信息） 数据存储：存储在 zk 上的一个数据节点 数据获取：读取数据节点，并注册一个数据变更 watcher 数据变更：当变更数据时，zk 会将数据变更通知发到各客户端 负载均衡 命名服务：指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个 全局的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务 的地址，或者一个远程的对象等等。 分布式协调/通知 集群管理：所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的字节点变化信息，判断是否有机器退出和加入、选举 master 分布式锁： 独占锁：所有用户都去创建/distribute_lock 节点 控制时序：/distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选 master 一样，编号最小的获得锁，用完删除。 分布式队列 同步队列：当一个队列成员都聚齐时，队列才可用。在约定目录下创建临时目录节点 异步队列，先进先出：入列有编号，出列按编号。类似分布式锁中控制时序的实现。 ","date":"2023-08-08","objectID":"/zookeeper_draft/:0:8","tags":["Zookeeper"],"title":"Zookeeper 杂记","uri":"/zookeeper_draft/"},{"categories":["interview"],"content":"Raft 协议的基本原理是什么？ Raft 协议的基本原理是通过选举一个 leader 节点来管理整个集群，并通过 leader 节点向其他节点发送心跳信号和日志复制等信息来保持数据的一致性。当 leader 节点失效时，其他节点会重新选举出新的 leader 节点。 ","date":"2023-08-08","objectID":"/raft/:0:1","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Raft 协议中有哪些角色？ Raft 协议中有三种角色：leader、follower 和 candidate。其中，leader 负责管理整个集群，follower 和 candidate 则负责接收 leader 的指令和提交日志等操作。 ","date":"2023-08-08","objectID":"/raft/:0:2","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Raft 协议中的选举过程是怎样的？ Raft 协议的选举过程分为两个阶段：选举阶段和日志复制阶段。在选举阶段，follower 节点会向其他节点发送 RequestVote 请求，请求其他节点投票支持自己成为 candidate 节点。当一个 candidate 节点获得了大多数节点的投票时，就会成为新的 leader 节点，并开始向其他节点发送心跳信号和日志复制等操作。 ","date":"2023-08-08","objectID":"/raft/:0:3","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Raft 协议如何保证数据的一致性？ Raft 协议通过 leader 节点向其他节点发送心跳信号和日志复制等信息来保持数据的一致性。当 leader 节点接收到客户端的请求时，会将请求转换成一条日志，并将该日志复制到其他节点上，以保证所有节点上的数据都是一致的。 ","date":"2023-08-08","objectID":"/raft/:0:4","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Raft 协议中如何处理网络分区问题？ Raft 协议中通过选举机制来避免网络分区问题。当一个节点与其余节点失去联系时，其余节点会重新选举出新的 leader 节点，从而保证整个集群的正常运行。当网络分区问题解决后，失去联系的节点会重新加入集群中，同步最新的数据。 ","date":"2023-08-08","objectID":"/raft/:0:5","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Raft 和 zab 协议的区别？ 相同点： 一致性都由 quorum（仲裁集，少数服从多数）来保证，zookeeper实现了带权重的quorum 都由 leader 来发起写操作 心跳机制检测存货行 leader election 选举都采用先到先得都投票方式 不同点： zab 用的是 epoch(时代，纪元) 和 count 的组合来唯一表示一个值, 而 raft 用的是 term 和 index。 zab 的 follower 在投票给一个 leader 之前必须和 leader 的日志达成一致,而 raft 的 follower则简单地说是谁的 term 高就投票给谁。 raft 协议数据只有单向地从 leader 到 follower(成为 leader 的条件之一就是拥有最新的 log)。而 zab 协议在 discovery 阶段, 一个 prospective(潜在的) leader 需要将自己的 log 更新为 quorum 里面最新的 log,然后才好在 synchronization 阶段将 quorum 里的其他机器的 log 都同步到一致。 ","date":"2023-08-08","objectID":"/raft/:0:6","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"Paxos vs Raft？ Paxos算法和Raft协议都是分布式一致性算法，用于解决分布式系统中节点之间的数据一致性问题。它们的基本思想和目标是相同的，但是在实现细节和应用场景上有一些不同。 实现难度 Paxos算法相对于Raft协议更加复杂，更加难以理解和实现。Paxos算法的实现需要考虑多个复杂的情况，包括多个阶段的消息协议、多个角色的状态转换、多个副本之间的数据同步等。Raft协议相对来说更加简单易懂，实现起来也更加容易理解和维护。 可扩展性 Paxos算法在节点数量较大时，消息通信和数据同步的效率会降低，因此它的可扩展性相对较差。Raft协议在节点数量较大时，依然能够保持较好的性能和可扩展性。 选举机制 Paxos算法的选举机制是基于**提案（proposal）**的，每个节点都可以提出一个提案，并通过协商过程来选出一个最终的提案。Raft协议的选举机制是基于角色的，只有一个节点能够成为leader，其他节点则成为follower或candidate。 可读性 Raft协议相对于Paxos算法具有更好的可读性。Raft协议中的角色和消息通信比较直观和易懂，容易让人理解和记忆。 总的来说，Paxos算法和Raft协议都是分布式一致性算法，它们的目标和基本思想相同，但是在实现细节、可扩展性、选举机制和可读性等方面有所不同。在实际应用中，需要根据具体的场景和需求选择适合的算法。 ","date":"2023-08-08","objectID":"/raft/:0:7","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"如何选择一致性算法？ 在实际应用中，选择合适的分布式一致性算法需要考虑多个因素，包括数据一致性要求、系统规模、可用性、性能等方面。 数据一致性要求 首先需要根据实际需求来确定数据一致性的要求，包括强一致性、弱一致性或最终一致性等。如果数据一致性要求较高，需要选择能够提供强一致性的算法，如Paxos算法或Raft协议。如果数据一致性要求较低，可以选择提供最终一致性的算法，如Gossip协议或CRDT算法。 系统规模 其次需要考虑系统规模，包括节点数量、数据量、数据分布等方面。如果系统规模较小，可以选择一些简单易懂、易于实现的算法，如Raft协议。如果系统规模较大，需要选择能够提供高性能和可扩展性的算法，如Paxos算法或Gossip协议。 可用性 还需要考虑系统的可用性，即在节点故障、网络分区等情况下，系统是否能够继续正常运行。如果可用性要求较高，需要选择能够提供高可用性的算法，如Raft协议。如果可用性要求较低，可以选择一些性能更高的算法，如Paxos算法。 性能 最后需要考虑系统的性能，包括吞吐量、延迟、负载均衡等方面。如果性能要求较高，需要选择能够提供高性能的算法，如Paxos算法或Gossip协议。如果性能要求较低，可以选择一些简单易懂、易于实现的算法，如Raft协议。 总的来说，在实际应用中，选择合适的分布式一致性算法需要综合考虑数据一致性要求、系统规模、可用性和性能等方面，根据具体需求选择适合的算法。同时，需要根据实际情况进行测试和调优，以保证系统的稳定性和性能。 ","date":"2023-08-08","objectID":"/raft/:0:8","tags":["Distributed"],"title":"🚩Raft","uri":"/raft/"},{"categories":["interview"],"content":"分布式事务与分布式锁的区别？ 分布式锁解决的是分布式资源抢占的问题；分布式事务和本地事务是解决流程化提交问题。 ","date":"2023-08-08","objectID":"/distributed/:0:1","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"🌟如何解决分布式事务问题的？/Seata 架构？ 三种角色： TC 事务协调者 TM 事务管理器 RM 资源管理器 解决方案： Seata XA：RM 互相等待，强一致性【CP：银行】 Seata AT：利用 undolog 实现高可用，主要⽤于核⼼模块，例如交易/订单等【强一致性场景：商品交易】 Seata TCC：强调资源预留， 通过TC的协调，保证最终⼀致性，也可以业务解耦【弱一致性场景：通知回显】 MQ：异步确保型事务，将 MQ 和 MySQL 操作放在同一个事务内执行，实时性高【最终一致性场景：通知支付结果】 属性 2PC TCC Saga 异步确保型事务 尽最⼤努⼒通知 事务⼀致性 强 弱 弱 弱 弱 复杂性 中 ⾼ 中 低 低 业务侵⼊性 ⼩ ⼤ ⼩ 中 中 使⽤局限性 ⼤ ⼤ 中 ⼩ 中 性能 低 中 ⾼ ⾼ ⾼ 维护成本 低 ⾼ 中 低 中 如何使用？ maven 引入 seata-all 依赖 分布式事务处理的方法上添加@GlobalTransactional注解 根据需求选择方案，修改配置文件 XA：enableAutoDataSourceProxy = false，因为 XA 模式需要使用AtomikosDataSourceBean 代理数据源。 AT：enableAutoDataSourceProxy = true，因为 AT 模式需要使用 DataSourceProxy 代理数据源 TCC：enableAutoDataSourceProxy = true，在业务方法上添加 @TwoPhaseBusinessAction 注解，同时修饰三个方法：try（）、confirm（）、cancel（） ","date":"2023-08-08","objectID":"/distributed/:0:2","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"CAP定理 - 三选二 ⼀致性(Consistency) ： 客户端知道⼀系列的操作都会同时发⽣(⽣效) 强一致性：保证客户端看到的数据都是⼀致的 最终一致性：允许存在中间状态，只要求经过⼀段时间后，数据最终是⼀致的 弱一致性：存在部分数据不一致 可⽤性(Availability) ： 每个操作都在一定时间范围内，返回可预期的响应。 分区容错性(Partition tolerance) ： 分布式系统在遇到任何⽹络分区故障时，仍然需要能够保证对外提供满⾜⼀致性和可⽤性的服务，除⾮是整个⽹络环境都发⽣了故障。 AP系统：在互联⽹领域的绝⼤多数的场景中，都需要牺牲强⼀致性来换取系统的⾼可⽤性，系统往往只需要保证最终⼀致性。 对于涉及到钱财这样不能有⼀丝让步的场景，C 必须保证。⽹络发⽣故障宁可停⽌服务，这是保证 CA，舍弃 P。还有⼀种是保证 CP，舍弃 A。例如⽹络故障是只读不写。 ","date":"2023-08-08","objectID":"/distributed/:0:3","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"BASE定理 核⼼思想：即使⽆法做到强⼀致性，但每个应⽤都可以根据⾃身业务特点，采⽤适当的⽅式来使系统达到最终⼀致性。 CAP是分布式系统设计理论，BASE是CAP理论中AP⽅案的延伸，对于C我们采⽤的⽅式和策略就是保证最终⼀致性；BASE理论是对CAP理论的延伸，核⼼思想是即使⽆法做到强⼀致性（StrongConsistency，CAP的⼀致性就是强⼀致性），但应⽤可以采⽤适合的⽅式达到最终⼀致性（Eventual Consitency）。 Basically Available（基本可⽤）：响应时间增加 Soft state（软状态）：指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可⽤性。 **Eventually consistent（最终⼀致性）：**通过牺牲⼀致性来获得可⽤性，并允许数据在⼀段时间内是不⼀致的，但最终达到⼀致状态。 ","date":"2023-08-08","objectID":"/distributed/:0:4","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"BASE与ACID的区别和联系 BASE理论⾯向的是⼤型⾼可⽤可扩展的分布式系统，和传统的事物ACID 特性是相反的。 它完全不同于ACID的强⼀致性模型，⽽是通过牺牲强⼀致性来获得可⽤性，并允许数据在⼀段时间内是不⼀致的，但最终达到⼀致状态。 但同时，在实际的分布式场景中，不同业务单元和组件对数据⼀致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往⼜会结合在⼀起。 ","date":"2023-08-08","objectID":"/distributed/:0:5","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"CAP和ACID中的A和C是完全不⼀样的 A的区别 ACID中的A指的是原⼦性(Atomicity)，是指事务被视为⼀个不可分割的最⼩⼯作单元，事务中的所有操作要么全部提交成功，要么全部失败回滚； CAP中的A指的是可⽤性(Availability)，是指集群中⼀部分节点故障后，集群整体是否还能响应客户端的读写请求； C的区别 ACID的C是有关数据库规则，数据库总是从⼀个⼀致性的状态转换到另外⼀个⼀致性的状态；【数据库完整性】 CAP的C是分布式多服务器之间复制数据令这些服务器拥有同样的数据，由于⽹速限制，这种复制在不同的服务器上所消耗的时间是不固定的，集群通过组织客户端查看不同节点上还未同步的数据维持逻辑视图，这是⼀种分布式领域的⼀致性概念；【分布式节点的数据的⼀致性】 ","date":"2023-08-08","objectID":"/distributed/:0:6","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"🌟如何实现分布式接口幂等性？ 每个请求对应唯一 token，在处理前判断是否处理过，处理完成后保存业务状态到 Redis 中。 分布式锁 数据库悲观锁、乐观锁（version） 唯一索引（只针对新增请求） ","date":"2023-08-08","objectID":"/distributed/:0:7","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"如果插入数据库成功，但是更新 Redis 上的总数失败了怎么办？ 如果数据短时间不一致但是业务可以接受的话，那么就可以考虑异步刷新 Redis 上的总数。 使用 Canal 之类的工具监听 MySQL binlog，然后刷新 Redis 上的总数。 ","date":"2023-08-08","objectID":"/distributed/:0:8","tags":["Distributed"],"title":"🚩Distributed","uri":"/distributed/"},{"categories":["interview"],"content":"为什么选用 Consul？ CP系统：强一致性需求 Go 语言编写 API 文档丰富，社区活跃 提供了简易的事务能力，可以使用 Consul API 或 HTTP 请求将事务代码提交给 Consul Agent 执行 consul agent 与服务之间有心跳机制，基于 Gossip 协议，去中心化，支持多数据中心 提供三种服务发现方式 DNS服务发现是Consul最原生的服务发现机制，它通过将服务实例的IP地址注册为DNS记录，来实现服务发现。 HTTP API服务发现是一种基于RESTful API的服务发现机制，它通过HTTP请求来获取服务实例的地址信息。 gRPC服务发现 安全功能：TLS 加密和 ACL（访问控制列表）机制，可以对服务进行更细粒度的权限控制。 Consul \u003e Eureka ？ Consul - CP（Raft 协议），Eureka - AP Consul 支持多数据中心，Eureka 只支持单个数据中心部署 Consul 在每个服务器上都有 consul agent，故障时通知 leader；Eureka 每 30s 心跳检查，实现简单 Consul 由 Go 编写，安装启动即可，提供了3种服务发现机制（DNS、HTTP、gRPC）；Eureka 是个 servlet 程序，跑在 servlet 容器中，Eureka 只支持基于 HTTP API 的服务发现机制。 Consul 还提供了 ACL（访问控制列表）机制，可以对服务进行更细粒度的权限控制。 Consul \u003e Zookeeper ？ 更适合服务发现：Consul 是专门为服务发现而设计的，而 ZooKeeper 则是一个通用的分布式协调服务。Consul 提供了一些方便的功能，如健康检查、DNS 解析等，使得服务发现更加容易。 提供了原生的分布式锁（Raft 协议）、Leader 选举支持 Consul 健康检查基于 Gossip 协议，去中心化，支持多数据中心，；zookeeper 采用临时节点 Consul 提供了简易的事务能力，zookeeper 只提供了版本号检查能力 文档详尽、社区活跃 ","date":"2023-08-08","objectID":"/microservice/:0:1","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"微服务架构 单体架构：所有代码全都在一个项目中，架构简单、部署成本低，但耦合度高 垂直架构：MVC三层结构 SOA架构：Service Oriented Architecture 面向服务架构，关注点是服务，服务是最基本的业务功能单元，由平台中立性的接口契约来定义。 微服务架构：每个小微服务都运行在自己的进程中，一般采用HTTP来轻量化通信。单一职责、面向服务、自治（团队、技术、数据、部署都独立）、隔离性强 服务网格：是一种微服务架构的扩展，提供了更加丰富的服务发现、负载均衡、流量控制、故障恢复和安全管理等功能。服务网格可以帮助开发人员更加轻松地管理和监控微服务架构，提高应用程序的可靠性和可维护性。 ","date":"2023-08-08","objectID":"/microservice/:0:2","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"云原生 云原生应用程序通常采用分布式架构，将应用程序拆分为多个小型服务，每个服务都运行在独立的容器中，并通过轻量级的通信机制相互协作。云原生应用程序的主要特点包括： 容器化部署：应用程序以容器的形式进行部署和运行，容器可以快速启动、停止和迁移，提高了应用程序的可靠性和可扩展性。 微服务架构：应用程序采用微服务架构，将应用程序拆分为多个小型服务，每个服务都可以独立地进行开发、部署和扩展。 自动化运维：应用程序采用自动化的部署、监控和管理工具，可以自动化地进行应用程序的部署、扩展、升级和故障恢复。 弹性设计：应用程序采用弹性设计，能够自动感知负载变化和故障情况，并自动调整资源分配，保证应用程序的高可用性和高性能。 面向服务的架构：应用程序采用面向服务的架构，将应用程序的服务作为基本的构建块，通过服务间的协作来实现应用程序的功能。 微服务技术栈：Spring Cloud + Consul + APISIX + Prometheus + Grafana + Hystrix Spring Cloud和Consul用于服务注册和发现，APISIX用于服务网关，Prometheus和Grafana用于监控和可视化，Hystrix用于服务保护和容错。这些组件都可以在同一个应用程序中一起工作，互相协作，形成一个完整的微服务架构。 具体来说，Prometheus可以采集Spring Cloud和APISIX的指标数据，Grafana可以展示和分析这些数据，Hystrix可以保护和容错服务，Consul可以用于服务注册和发现，APISIX可以作为服务网关进行请求路由和负载均衡。这些组件可以通过REST API、JMX、Webhook等方式相互交互。 REST API：REST API是一种基于HTTP协议的Web服务标准，用于不同应用之间的数据交互和远程调用。 APISIX可以通过REST API从Consul中获取服务列表，并进行负载均衡和路由，Prometheus可以通过REST API从Spring Cloud和APISIX中获取指标数据，并存储和查询这些数据。 JMX：JMX是Java Management Extensions的缩写，是一种Java平台的管理和监控技术，用于在运行时管理和监控Java应用程序。 Hystrix可以通过JMX暴露服务熔断和降级的状态，并提供监控和管理API，Prometheus可以通过JMX抓取Hystrix的监控数据。 Webhook：Webhook是一种HTTP回调机制，用于在发生事件时向指定的URL发送HTTP请求。 Hystrix可以通过Webhook向外部系统发送警报和通知，Prometheus可以通过Webhook触发告警规则并发送通知。 ","date":"2023-08-08","objectID":"/microservice/:0:3","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"服务拆分原则 不同微服务单一职责，不要重复开发相同业务 假设我们正在构建一个在线商城系统，可以拆分为订单服务、支付服务、库存服务和用户服务等微服务 微服务数据独立，不要访问其他微服务的数据库 微服务可以将业务暴露为接口，供其他微服务使用 ","date":"2023-08-08","objectID":"/microservice/:0:4","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"Eureka - 注册中心 Q1：消费者如何获取服务提供者具体信息？ 服务提供者启动时向 eureka 注册自己的信息 消费者根据服务名称向 eureka 拉取提供者信息 Q2：如果有多个服务提供者，消费者该如何选择？ A：服务消费者利用负载均衡算法，从服务列表中挑选一个 Q3：消费者如何感知服务提供者健康状态？ A：服务提供者每隔 30s 向 EurekaServer 发送心跳请求，报告健康状态， eureka 会更新记录服务列表信息，心跳不正常会被剔除。 ","date":"2023-08-08","objectID":"/microservice/:0:5","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"Nacos vs Eureka ？ 消费者端，Nacos 增加了注册中心向 Consumer push 推送变更消息 两者默认AP ， Nacos 可配置临时实例spring.cloud.nacos.discovery.ephemeral，实现 CP Nacos 服务端：临时实例采用心跳监测，心跳不正常会被剔除；非临时实例主动询问，不会被剔除。 Eureka 只有注册中心，Nacos 还有配置中心 配置中心 - 热更新 在@Value注入的变量所在类上添加注解@RefreshScope 使用 @ConfigurationProperties 注解 注意：不是所有配置都适合放到配置中心，建议只放一些需要运行时调整的关键参数 ","date":"2023-08-08","objectID":"/microservice/:0:6","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"nginx 反向代理 - 搭建集群 http { upstream nacos-cluster { server 127.0.0.1:8845; server 127.0.0.1:8846; server 127.0.0.1:8847; } ··· } ","date":"2023-08-08","objectID":"/microservice/:0:7","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"Ribbon - 负载均衡 给RestTemplate添加@LoadBalanced注解，负载均衡底层流程： 调用接口发起请求 RibbonLoadBalancerClient 获取url中的服务id DynamicServerListLoadBalancer 拉取 eureka-server 中的服务列表 🌟IRule 接口决定负载均衡策略，并选择某个服务： ZoneAvoidanceRule（默认）：对Zone区域内多个服务轮询，zone值可配置 RoundRobinRule：轮询 WeightedResponseTimeRule：服务响应时间越长，服务器权重就越小，概率就越低 RandomRule：随机 修改 url，发起请求 大请求场景 Q：我们公司用的是轮询来作为负载均衡。不过因为轮询没有实际查询服务端节点的负载，所以难免会出现偶发性的负载不均衡的问题。 A1：（业务拆分）向账户中心查询 loginName 是否存在时，传的 list 限制在 100 以内，防止请求过大。 A2：（隔离角度）魔改了一下负载均衡算法，让大客户被打到几个专门的节点上 加权负载均衡的权重设定 权重代表节点的处理能力，成加败减，权重的调整要设置好上限和下限。 一致性哈希负载均衡 比如说针对用户的本地缓存，我们可以使用用户 ID 来计算哈希值，那么可以确保同一个用户的本地缓存必然在同一个节点上。不过即便是采用了一致性哈希负载均衡算法，依旧不能彻底解决数据一致性的问题（应用发布时）。 负载均衡策略配置 在 consumer 消费者端： 针对所有服务提供者：直接注入IRule类型的Bean配置策略 针对特定服务提供者：yml：userservice.ribbon.NFLoadBlancerRuleClassName: com.netflix.loadbalancer.RandomRule 加载策略：默认懒加载，除非配置yml：ribbon.eager-load.enabled=true 开启饥饿加载 ","date":"2023-08-08","objectID":"/microservice/:0:8","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"Feign - 远程调用 系统级配置写入 bootstrap.yml 基本使用 添加 pom 依赖 启动类添加@EnableFeignClients 注解 写接口： @FeignClient(\"userservice\") public interface UserClient{ @GetMapping(\"/user/{id}\") User findById(@PathVariable(\"id\") Long id); } 使用 FeignClient 中定义的方法取代 RestTemplate（并集成了 ribbon，实现了负载均衡） 自定义配置 修改日志级别：feign.Logger.Level，包含 NONE、BASIC（记录请求基本信息）、HEADERS（BASIC+请求头）、FULL（HEADERS+请求体） 四种级别，推荐使用NONE 或 BASIC 配置文件方式 Java 注解方式 配置一个 Logger.Level 这个 Bean public class DefaultFeignConfiguration{ @Bean public Logger.Level feignLogLevel(){ return Logger.Level.BASIC; } } 全局：@EnableFeignClients（defaultConfiguration = DefaultFeignConfiguration.class) 局部：@EnableFeignClients（value=“userservice”, defaultConfiguration = DefaultFeignConfiguration.class) 响应结果解析器：feign.codec.Decoder，Json 转换为 POJO 对象 请求参数编码：feign.codec.Encoder，对象转换为 Json 失败重试机制：feign.Retryer，默认没有，不过会使用Ribbon的重试 性能优化 Feign 底层的客户端实现： URLConnection：默认实现，不支持连接池 Apache HttpClient：支持连接池，feign.httpclient.enabled=true OKHttp：支持连接池 最佳实践 继承：给消费者的 FeignClient和提供者的controller定义统一的父接口作为标准。 服务【紧耦合】 父接口参数列表中的映射不会被继承 ✓抽取：将 FeignClient 抽取为独立模块，并把接口相关的POJO、默认的Feign配置都放到这个模块中，提供给消费者用【降低耦合】 Q：定义的FeignClient不在SpringBootApplication的扫描包范围时，FeignClient无法使用？ 指定FeignClient所在包：@EnableFeignClients(basePackages = “edu.zjut.feign.clients”) 指定FeignClient字节码：@EnableFeignClients(clients = {UserClient.class}) （推荐） ","date":"2023-08-08","objectID":"/microservice/:0:9","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"网关 功能 请求先经过网关，再进入微服务： 身份认证和权限校验 服务路由、负载均衡 请求限流 网关比较 gateway：基于Spring5中提供的WebFlux，属于响应式编程（推荐） zuul：基于Servlet实现，属于阻塞式编程 Apisix：Nginx+etcd，性能好 请求路由和负载均衡：APISIX支持基于请求内容、请求地址、请求头等多种方式进行请求路由，并提供多种负载均衡算法，如轮询、加权轮询、随机等。 流量控制和限流：APISIX支持基于QPS、连接数、并发数等多种方式进行流量控制和限流，可以保护后端服务免受突发流量的冲击。 认证和授权：APISIX支持多种认证和授权方式，如基于JWT、OAuth2、API Key等方式进行认证和授权，可以保护API服务的安全性。 ","date":"2023-08-08","objectID":"/microservice/:0:10","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"跨域问题 跨域：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题。 域名不同 域名相同，端口不同 ","date":"2023-08-08","objectID":"/microservice/:0:11","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"分布式服务化与 SOA/ESB 的区别 有状态部分，放到xx中心 配置中心：全局非业务参数 Nacos，运行期改变配置，避免不断重启 注册中心：运行期临时状态 主动报告+心跳：让消费者能动态知道生产者集群的状态变化 元数据中心：业务模型 定义了所有业务服务的模型 无状态部分，放到应用侧（框架和配置部分，尽量不影响业务代码） ","date":"2023-08-08","objectID":"/microservice/:0:12","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"熔断 目的：给服务端恢复的机会。 使用：Hystrix 采用@EnableCircuitBreaker 注解，10s 内失败 50%以上触发熔断，之后每 5s 重试。 设定合适的阈值—— 响应时间超过 1.2s，并且持续 30s 防止抖动 总的来说，在任何的故障处理里面，都要考虑恢复策略会不会引起抖动问题。 超过阈值后，持续一段时间后再熔断 等待一段时间后，再放开流量（根据结果调整负载均衡权重） 场景：当 Redis 崩溃 or 返回某个错误类型时，触发熔断策略： 开启一个线程，自旋地 ping Redis，直到 Redis 恢复。 将节点挪出负载均衡策略的可用列表，降低权重。一段时间后如果客户端请求成功，就增加调整负载均衡权重，流量就会增加，否则该节点就继续等待。 当所有节点都触发熔断了，就通过钉钉/企微机器人发起告警通知，人工介入处理。 ","date":"2023-08-08","objectID":"/microservice/:0:13","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"降级 自我保护，目的在于应对系统自身的故障，与 feign 接口整合，编写降级逻辑，去掉不必要的业务逻辑，只保留核心逻辑 e.g. 第一次接触降级：双十一淘宝无法查看全部订单等边缘业务 场景： 读写服务降级写服务 降级不重要的写数据：埋点数据、观测性数据 任务模块，主播读服务多，运营商写服务多，在qps过高时，降级写服务，降级非核心服务（不赚钱的） 快慢路径降级慢路径：系统的并发高时，将请求标记为降级请求，只查询缓存（快路径），不查询数据库（慢路径）。 水平分库：最热的数据放缓存，温数据放数据库，冷数据放硬盘（不提供实时查询） ","date":"2023-08-08","objectID":"/microservice/:0:14","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"限流 雪崩：一个服务失败，导致整条链路都失败 限流可以解决异常突发流量使系统雪崩的问题，方式： Tomcat 最大连接数 Nginx 漏桶算法 网关：令牌桶算法，固定速率生成令牌，放入桶中，每个请求拿到令牌才会被执行 确定阈值：线上高峰期 QPS，全链路压测测量瓶颈，可动态调整 响应时间 - 性能 资源利用率 - 高并发 吞吐量 自定义拦截器 IP 限流 获取用户真实 IP： 应用层方法：X-Forwarded-For，缺点:会被伪造、多个X-Forwarded-For头部、不能解决HTTP和SMTP之外的真实源IP获取的需求 传输层方法：利用 TCP Options 的字段来承载真实源 IP 信息、Proxy Protocol、NetScaler 的 TCP IP header 网络层：隧道 +DSR 模式 ","date":"2023-08-08","objectID":"/microservice/:0:15","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"隔离 目的：提升可用性、性能和安全性。 线程池隔离快慢任务，防止定时任务不能得到及时的调度，一个任务运行时间超过阈值，就转交给慢任务线程池。 线程池分成三级：独享，部分共享，完全共享，并且设置超时时间。 数据库水平分库：素材库和发布库分离，只有正式发布的时候，素材才同步到缓存，提高缓存命中率。 Redis 隔离：核心业务保证高可用并且 code review，非核心业务 可以容错，以防缓存大量大对象导致集群超时、宕机。 服务共享一部分基础设施，如部署两套 Redis ","date":"2023-08-08","objectID":"/microservice/:0:16","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"超时控制 目的： 用户体验 e.g. 首页接口限制在 100ms 内 及时释放资源。 常用指标： 99 线：99%请求都在阈值内 999 线：99.9%的请求都在阈值内 链路超时控制：将超时时间放在协议头（可以存放剩余超时时间或超时时间戳） ","date":"2023-08-08","objectID":"/microservice/:0:17","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"Dubbo ","date":"2023-08-08","objectID":"/microservice/:1:0","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"1. Dubbo 整体架构 config 配置层：对外配置接口，以 ServiceConfig、ReferenceConfig 为中心 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务端 Skeleton，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心 cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心 monitor 监控层：RPC调用次数和调用时间监控 Protocol 远程调用层：封装 RPC 调用，以 Result 为中心，扩展接口为 Protocol，Invoker exchange 信息交换层：封装请求响应模式，同步转异步，以 Request，Response 为中心 transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心 serialize 数据序列化层：可复用的一些工具，扩展接口为 ThreadPool ","date":"2023-08-08","objectID":"/microservice/:1:1","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"2. 应用场景 分布式服务化改造：垂直拆分 开放平台：开放模式、容器模式 直接作为前端使用的后端（BFF，Backend For Frontend），一般不建议 通过服务化建设中台：基于Dubbo实现业务中台 Q：服务如何暴露？ A：具体服务 -\u003e Invoker -\u003e Exporter ","date":"2023-08-08","objectID":"/microservice/:2:0","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["interview"],"content":"3. 如何设计幂等接口？ 用 bitmap 去重 乐观锁机制 ","date":"2023-08-08","objectID":"/microservice/:2:1","tags":["MicroService"],"title":"🚩MicroService","uri":"/microservice/"},{"categories":["生活"],"content":"哈哈哈哈哈哈哈","date":"2023-08-08","objectID":"/joke/","tags":["笑话"],"title":"程序员笑话","uri":"/joke/"},{"categories":["生活"],"content":" 一个测试工程师走进一家酒吧，要了一杯啤酒，一切 OK 一个测试工程师走进一家酒吧，要了一杯咖啡，一切 OK 一个测试工程师走进一家酒吧，要了 0.7 杯啤酒，一切 OK 一个测试工程师走进一家酒吧，要了 -1 杯啤酒，一切 OK 一个测试工程师走进一家酒吧，要了 2^32 杯啤酒，一切 OK 一个测试工程师走进一家酒吧，要了一杯洗脚水，一切 OK 一个测试工程师走进一家酒吧，要了一杯蜥蜴，一切 OK 一个测试工程师走进一家酒吧，要了一份 adfgar@24dg!\u0026*(@ ，一切 OK 一个测试工程师走进一家酒吧，什么也没要，一切 OK 一个测试工程师走进一家酒吧，又走出去又从窗户进来又从后门出去从下水道钻进来，一切 OK 一个测试工程师走进一家酒吧，又走出去又进来又出去又进来又出去，最后在外面把老板打了一顿，一切 OK END: 测试工程师们满意地离开了酒吧。然后一名顾客点了一份炒饭，酒吧炸了 ","date":"2023-08-08","objectID":"/joke/:0:0","tags":["笑话"],"title":"程序员笑话","uri":"/joke/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/spring_draft/","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"9.1 Spring框架设计 ","date":"2023-08-08","objectID":"/spring_draft/:0:1","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"9.2 AOP 详解 ","date":"2023-08-08","objectID":"/spring_draft/:0:2","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"9.3 Bean 核心原理 ","date":"2023-08-08","objectID":"/spring_draft/:0:3","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"9.4 XML 配置原理 ","date":"2023-08-08","objectID":"/spring_draft/:0:4","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"10.1 Spring Boot 核心原理 ","date":"2023-08-08","objectID":"/spring_draft/:0:5","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"10.2 ORM-Hibernate/MyBatis ","date":"2023-08-08","objectID":"/spring_draft/:0:6","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"10.3 Spring 集成 ORM/JPA ","date":"2023-08-08","objectID":"/spring_draft/:0:7","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"容器接口与实现 BeanFactory Spring 的核心容器，实际上控制反转、基本的依赖注入、直至 Bean 的生命周期的各种功能，都由它的实现类提供。 @SpringBootApplication public class DemoApplication { public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { ConfigurableApplicationContext context = SpringApplication.run(DemoApplication.class, args); // 单例存储在 singletonObjects Field singletonObjects = DefaultSingletonBeanRegistry.class.getDeclaredField(\"singletonObjects\"); singletonObjects.setAccessible(true); ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); Map\u003cString, Object\u003e map = (Map\u003cString, Object\u003e) singletonObjects.get(beanFactory); map.forEach((k, v) -\u003e { System.out.println(\"beanName:\" + k + \",bean:\" + v); }); } } 将常用后处理器添加到beanFactory AnnotationConfigUtils.registerAnnotationConfigProcessors(beanFactory); beanFactory 后处理器 org.springframework.context.annotation.internalConfigurationAnnotationProcessor：解析@Configuration、@Bean ConfigurationClassPostProcessor.class 可以解析 @ComponentScan @Bean @Import @ImportResource MapperScannnerConfigurer.class 可以解析 @MapperScanner beanFactory.getBeansOfType(BeanFactoryPostProcessor.class).values().forEach(beanFactoryPostProcessor -\u003e { beanFactoryPostProcessor.postProcessBeanFactory(beanFactory); }); bean 后处理器 org.springframework.context.annotation.internalAutowiredAnnotationProcessor：在依赖注入阶段，解析@Autowired 如果是 GenericApplicationContext，还需要 context.getDefaultListableBeanFactory().setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()) 才能解析 @Value org.springframework.context.annotation.internalCommonAnnotationProcessor：解析@Resource（依赖注入阶段）、@PostConstruct（实例化后，即初始化前）、@PreDestroy（销毁前） org.springframework.context.event.internalEventListenerProcessor org.springframework.context.event.internalEventListenerFactory beanFactory.getBeansOfType(BeanPostProcessor.class).values().forEach(beanFactory::addBeanPostProcessor); 另外，ConfigurationPropertiesBindingPostProcessor 用于解析 @ConfigurationProperties 单例对象默认在使用时才初始化，若要预先初始化单例对象： beanFactory.preInstantiateSingletons(); 综上，beanFactory不会主动做以下事情： 不会主动调用 BeanFactory 后处理器 不会主动添加 Bean 后处理器 不会主动初始化单例 不会解析 beanFactory、${}、#{} 注：beanFactory.addEmbeddedValueResolver(new StandardEnvironment()::resolvePlaceholders) 是 ${} 的解析器 Q：同时添加 @Autowired 和@Resource注解？ Autowired默认按照类型匹配（通过beanFactory.doResolveDependency(…)方法）， 有相同类型时再按照名称匹配，Resource默认按照名称匹配。 internalAutowiredAnnotationProcessor默认优先级更高，所以Autowired生效，可以通过比较器来改变优先级（order越小优先级越高）。 ApplicationContext 继承了 BeanFactory，进行了扩展。 MessageSource：国际化 ResourcePatternResolver：资源通配符 ApplicationEventPublisher：事件发布器（用于解耦） EnvironmentCapable：环境变量、配置信息 DispatcherServlet 称为前控制器，所有请求都先通过它。 ","date":"2023-08-08","objectID":"/spring_draft/:0:8","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"Aware 接口 是 spring 的内置功能 bean初始化优先级： @PostConstruct Aware implements InitializingBean 重写方法 @Bean(initMethod = “”) bean销毁优先级： @PreDestroy Aware implements DisposableBean 重写方法 @Bean(destroyMethod = “”) ","date":"2023-08-08","objectID":"/spring_draft/:0:9","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"Scope singleton 单例：Q：单例中注入多例，多例都是同一个对象，需要推迟其他 scope bean 的获取（2种代理，2种非代理） 在注入目标前面加 @Lazy 生成代理对象（推荐） 在被注入类上添加 @Scope(value=“prototype”,proxyMode = ScopedProxyMode.TARGET_CLASS) 生成代理对象 声明 ObjectFactory\u003c\u003e，通过getObject() 来实现 注入 ApplicationContext，通过 context.getBean(Xxx.class) prototype 原型 request 请求 session 会话：不同浏览器，不同会话 application 应用程序 Q：jdk\u003e=9时，反射调用 jdk 中方法报 IllegalAccessException？ A1:重写toString()方法，不让它调用 jdk 中的方法 A2:添加参数–add-opens java.base/java.lang=ALL-UNNAMED ","date":"2023-08-08","objectID":"/spring_draft/:0:10","tags":["Spring"],"title":"Spring 杂记","uri":"/spring_draft/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/springboot_draft/","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"配置 ","date":"2023-08-08","objectID":"/springboot_draft/:1:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"包扫描 com +- example +- myproject +- Application.java | +- domain | +- Customer.java | +- CustomerRepository.java | +- service | +- CustomerService.java | +- web | +- CustomerController.java | @ComponentScan @SpringBootApplication @ComponentScan(basePackages=\"com.example\") public class Bootstrap { public static void main(String[] args) { SpringApplication.run(Bootstrap.class, args); } } @Bean @SpringBootApplication public class Bootstrap { public static void main(String[] args) { SpringApplication.run(Bootstrap.class, args); } @Bean public CustomerController customerController() { return new CustomerController(); } } ","date":"2023-08-08","objectID":"/springboot_draft/:1:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"YAML /ˈjæməl/ spring: jpa: databaseplatform: mysql database-platform: mysql databasePlatform: mysql database_platform: mysql # 以上等价 推荐使用全小写配合**-**分隔符的方式来配置，比如：spring.jpa.database-platform=mysql ","date":"2023-08-08","objectID":"/springboot_draft/:1:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"List、Map类型 spring: my-example1: key: value url: - http://example.com - http://spring.io my-example2: url: http://example.com, http://spring.io '[key.me]': value 如果Map类型的key包含非字母数字和**-**的字符，需要用**[]**括起来 ","date":"2023-08-08","objectID":"/springboot_draft/:1:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"随机数 # 随机字符串 com.didispace.blog.value=${random.value} # 随机int com.didispace.blog.number=${random.int} # 随机long com.didispace.blog.bignumber=${random.long} # 10以内的随机数 com.didispace.blog.test1=${random.int(10)} # 10-20的随机数 com.didispace.blog.test2=${random.int[10,20]} ","date":"2023-08-08","objectID":"/springboot_draft/:1:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"命令行参数 java -jar xxx.jar --server.port=8888 等价于我们在application.properties中添加属性server.port=8888 ","date":"2023-08-08","objectID":"/springboot_draft/:1:5","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"多环境配置 application-dev.properties：开发环境 application-test.properties：测试环境 application-prod.properties：生产环境 通过spring.profiles.active属性来配置（如：spring.profiles.active=test） ","date":"2023-08-08","objectID":"/springboot_draft/:1:6","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"绑定 API 假设在propertes配置中有这样一个配置：com.didispace.foo=bar 我们为它创建对应的配置类： @Data @ConfigurationProperties(prefix = \"com.didispace\") public class FooProperties { private String foo; } 接下来，通过最新的Binder就可以这样来拿配置信息了： @SpringBootApplication public class Application { public static void main(String[] args) { ApplicationContext context = SpringApplication.run(Application.class, args); Binder binder = Binder.get(context.getEnvironment()); // 绑定简单配置 FooProperties foo = binder.bind(\"com.didispace\", Bindable.of(FooProperties.class)).get(); System.out.println(foo.getFoo()); } } ","date":"2023-08-08","objectID":"/springboot_draft/:1:7","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"关系型数据库 Spring Boot提供自动配置的嵌入式数据库有H2、HSQL、Derby，你不需要提供任何连接配置就能使用。 ","date":"2023-08-08","objectID":"/springboot_draft/:2:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"JPA = Java Persistence API 通过解析方法名创建查询 ","date":"2023-08-08","objectID":"/springboot_draft/:2:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"JTA = Java Transaction API JTA事务比JDBC事务更强大。一个JTA事务可以有多个参与者，而一个JDBC事务则被限定在一个单一的数据库连接。所以，当我们在同时操作多个数据库的时候，使用JTA事务就可以弥补JDBC事务的不足。 ","date":"2023-08-08","objectID":"/springboot_draft/:2:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"单元测试 插入一条name=AAA，age=20的记录，然后根据name=AAA查询，并判断age是否为20 测试结束回滚数据，保证测试单元每次运行的数据环境独立 @Slf4j @RunWith(SpringRunner.class) @SpringBootTest public class Chapter35ApplicationTests { @Autowired private UserMapper userMapper; @Test @Rollback public void test() throws Exception { userMapper.insert(\"AAA\", 20); User u = userMapper.findByName(\"AAA\"); Assert.assertEquals(20, u.getAge().intValue()); } } 通常我们单元测试为了保证每个测试之间的数据独立，会使用@Rollback注解让每个单元测试都能在结束时回滚。而真正在开发业务逻辑时，我们通常在service层接口中使用@Transactional来对各个业务逻辑进行事务管理的配置。 @Transactional注解不生效？ 注解修饰的函数中catch了异常，并没有往方法外抛。 @Transactional注解修饰的函数不是public类型 对应数据库使用的存储引擎不支持事务，比如：MyISAM。 数据源没有配置事务管理器 ","date":"2023-08-08","objectID":"/springboot_draft/:2:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"事务隔离级别 隔离级别是指若干个并发的事务之间的隔离程度，与我们开发时候主要相关的场景包括：脏读取、重复读、幻读。 我们可以看org.springframework.transaction.annotation.Isolation枚举类中定义了五个表示隔离级别的值： public enum Isolation { DEFAULT(-1), READ_UNCOMMITTED(1), READ_COMMITTED(2), REPEATABLE_READ(4), SERIALIZABLE(8); } DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是：READ_COMMITTED。 READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 指定方法：通过使用isolation属性设置，例如： @Transactional(isolation = Isolation.DEFAULT) ","date":"2023-08-08","objectID":"/springboot_draft/:2:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"传播行为 所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。 我们可以看org.springframework.transaction.annotation.Propagation枚举类中定义了6个表示传播行为的枚举值： public enum Propagation { REQUIRED(0), SUPPORTS(1), MANDATORY(2), REQUIRES_NEW(3), NOT_SUPPORTED(4), NEVER(5), NESTED(6); } REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于REQUIRED。 指定方法：通过使用propagation属性设置，例如： @Transactional(propagation = Propagation.REQUIRED) ","date":"2023-08-08","objectID":"/springboot_draft/:2:5","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Flyway Flyway是一个简单开源数据库版本控制器（约定大于配置），主要提供migrate、clean、info、validate、baseline、repair等命令。它支持SQL（PL/SQL、T-SQL）方式和Java方式，支持命令行客户端等，还提供一系列的插件支持（Maven、Gradle、SBT、ANT等）。 官方网站：https://flywaydb.org/ ","date":"2023-08-08","objectID":"/springboot_draft/:2:6","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Mybatis 使用@Param 在之前的整合示例中我们已经使用了这种最简单的传参方式，如下： @Insert(\"INSERT INTO USER(NAME, AGE) VALUES(#{name}, #{age})\") int insert(@Param(\"name\") String name, @Param(\"age\") Integer age); 这种方式很好理解，@Param中定义的name对应了SQL中的#{name}，age对应了SQL中的#{age}。 使用Map 如下代码，通过Map\u003cString, Object\u003e对象来作为传递参数的容器： @Insert(\"INSERT INTO USER(NAME, AGE) VALUES(#{name,jdbcType=VARCHAR}, #{age,jdbcType=INTEGER})\") int insertByMap(Map\u003cString, Object\u003e map); 对于Insert语句中需要的参数，我们只需要在map中填入同名的内容即可，具体如下面代码所示： Map\u003cString, Object\u003e map = new HashMap\u003c\u003e(); map.put(\"name\", \"CCC\"); map.put(\"age\", 40); userMapper.insertByMap(map); 包扫描 @MapperScan(\"com.didispace.chapter36.mapper\") ","date":"2023-08-08","objectID":"/springboot_draft/:2:7","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Lombok Val可以将变量申明是final类型。 public static void main(String[] args) { val setVar = new HashSet\u003cString\u003e(); val listsVar = new ArrayList\u003cString\u003e(); val mapVar = new HashMap\u003cString, String\u003e(); //=\u003e上面代码相当于如下： final Set\u003cString\u003e setVar2 = new HashSet\u003c\u003e(); final List\u003cString\u003e listsVar2 = new ArrayList\u003c\u003e(); final Map\u003cString, String\u003e maps2 = new HashMap\u003c\u003e(); } @NonNull注解能够为方法或构造函数的参数提供非空检查。 public void notNullExample(@NonNull String string) { //方法内的代码 } //=\u003e上面代码相当于如下： public void notNullExample(String string) { if (string != null) { //方法内的代码相当于如下： } else { throw new NullPointerException(\"null\"); } } @Cleanup注解能够自动释放资源。 public void jedisExample(String[] args) { try { @Cleanup Jedis jedis = redisService.getJedis(); } catch (Exception ex) { logger.error(“Jedis异常:”,ex) } //=\u003e上面代码相当于如下： Jedis jedis= null; try { jedis = redisService.getJedis(); } catch (Exception e) { logger.error(“Jedis异常:”,ex) } finally { if (jedis != null) { try { jedis.close(); } catch (Exception e) { e.printStackTrace(); } } } } @Synchronized注解类似Java中的Synchronized 关键字，但是可以隐藏同步锁。 public class SynchronizedExample { private final Object readLock = new Object(); @Synchronized public static void hello() { System.out.println(\"world\"); } @Synchronized(\"readLock\") public void foo() { System.out.println(\"bar\"); } //上面代码相当于如下： public class SynchronizedExample { private static final Object $LOCK = new Object[0]; private final Object readLock = new Object(); public static void hello() { synchronized($LOCK) { System.out.println(\"world\"); } } public void foo() { synchronized(readLock) { System.out.println(\"bar\"); } } } ","date":"2023-08-08","objectID":"/springboot_draft/:2:8","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"PostgreSQL PostgreSQL是一种特性非常齐全的自由软件的对象-关系型数据库管理系统（ORDBMS），是以加州大学计算机系开发的POSTGRES，4.2版本为基础的对象关系型数据库管理系统。 支持存储一些特殊的数据类型，比如：array、json、jsonb 对地理信息的存储与处理有更好的支持，所以它可以成为一个空间数据库，更好的管理数据测量和几何拓扑分析 可以快速构建REST API，通过PostgREST可以方便的为任何PostgreSQL数据库提供RESTful API的服务 支持树状结构，可以更方便的处理具备此类特性的数据存储 外部数据源支持，可以把MySQL、Oracle、CSV、Hadoop等当成自己数据库中的表来进行查询 对索引的支持更强，PostgreSQL支持 B-树、哈希、R-树和 Gist 索引。而MySQL取决于存储引擎。MyISAM：BTREE，InnoDB：BTREE。 事务隔离更好，MySQL 的事务隔离级别repeatable read并不能阻止常见的并发更新，得加锁才可以，但悲观锁会影响性能，手动实现乐观锁又复杂。而 PostgreSQL 的列里有隐藏的乐观锁 version 字段，默认的 repeatable read 级别就能保证并发更新的正确性，并且又有乐观锁的性能。 时间精度更高，可以精确到秒以下 字符支持更好，MySQL里需要utf8mb4才能显示emoji，PostgreSQL没这个坑 存储方式支持更大的数据量，PostgreSQL主表采用堆表存放，MySQL采用索引组织表，能够支持比MySQL更大的数据量。 序列支持更好，MySQL不支持多个表从同一个序列中取id，而PostgreSQL可以 增加列更简单，MySQL表增加列，基本上是重建表和索引，会花很长时间。PostgreSQL表增加列，只是在数据字典中增加表定义，不会重建表。 ","date":"2023-08-08","objectID":"/springboot_draft/:2:9","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"缓存 NoSQL ","date":"2023-08-08","objectID":"/springboot_draft/:3:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"引入缓存 第一步：在pom.xml中引入cache依赖，添加如下内容： \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-cache\u003c/artifactId\u003e \u003c/dependency\u003e 第二步：在Spring Boot主类中增加@EnableCaching注解开启缓存功能，如下： @EnableCaching @SpringBootApplication public class Chapter51Application { public static void main(String[] args) { SpringApplication.run(Chapter51Application.class, args); } } 第三步：在数据访问接口中，增加缓存配置注解，如： @CacheConfig(cacheNames = \"users\") public interface UserRepository extends JpaRepository\u003cUser, Long\u003e { @Cacheable User findByName(String name); } ","date":"2023-08-08","objectID":"/springboot_draft/:3:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Redis 发布订阅模式比观察者模式多了一个中间层。 ","date":"2023-08-08","objectID":"/springboot_draft/:3:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"MongoDB MongoDB是一个基于分布式文件存储的数据库，它是一个介于关系数据库和非关系数据库之间的产品，其主要目标是在键/值存储方式（提供了高性能和高度伸缩性）和传统的RDBMS系统（具有丰富的功能）之间架起一座桥梁，它集两者的优势于一身。 MongoDB支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型，也因为他的存储格式也使得它所存储的数据在Nodejs程序应用中使用非常流畅。 较常见的，我们可以直接用MongoDB来存储键值对类型的数据，如：验证码、Session等；由于MongoDB的横向扩展能力，也可以用来存储数据规模会在未来变的非常巨大的数据，如：日志、评论等；由于MongoDB存储数据的弱类型，也可以用来存储一些多变json数据，如：与外系统交互时经常变化的JSON报文。而对于一些对数据有复杂的高事务性要求的操作，如：账户交易等就不适合使用MongoDB来存储。 ","date":"2023-08-08","objectID":"/springboot_draft/:3:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"LDAP = Lightweight Directory Access Protocol 轻量级目录访问协议是实现提供被称为目录服务的信息服务。目录服务是一种特殊的数据库系统，其专门针对读取，浏览和搜索操作进行了特定的优化。目录一般用来包含描述性的，基于属性的信息并支持精细复杂的过滤能力。目录一般不支持通用数据库针对大量更新操作操作需要的复杂的事务管理或回卷策略。而目录服务的更新则一般都非常简单。这种目录可以存储包括个人信息、web链结、jpeg图像等各种信息。为了访问存储在目录中的信息，就需要使用运行在TCP/IP 之上的访问协议—LDAP。 o：organization（组织-公司） ou：organization unit（组织单元-部门） c：countryName（国家） dc：domainComponent（域名） sn：surname（姓氏） cn：common name（常用名称） ","date":"2023-08-08","objectID":"/springboot_draft/:3:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"时序数据库InfluxDB 时间序列数据库主要用于指处理带时间标签（按照时间的顺序变化，即时间序列化）的数据，带时间标签的数据也称为时间序列数据。 时间序列数据主要由电力行业、化工行业等各类型实时监测、检查与分析设备所采集、产生的数据，这些工业数据的典型特点是：产生频率快（每一个监测点一秒钟内可产生多条数据）、严重依赖于采集时间（每一条数据均要求对应唯一的时间）、测点多信息量大（常规的实时监测系统均有成千上万的监测点，监测点每秒钟都产生数据，每天产生几十GB的数据量） 几个重要名词： database：数据库 measurement：类似于关系数据库中的table（表） points：类似于关系数据库中的row（一行数据） time：时间戳 fields：记录的值 tags：索引的属性 ","date":"2023-08-08","objectID":"/springboot_draft/:3:5","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Web 开发 ","date":"2023-08-08","objectID":"/springboot_draft/:4:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Thymeleaf 它是一个XML/XHTML/HTML5模板引擎，可用于Web与非Web环境中的应用开发。它是一个开源的Java库，基于Apache License 2.0许可，由Daniel Fernández创建，该作者还是Java加密库Jasypt的作者。 Thymeleaf提供了一个用于整合Spring MVC的可选模块，在应用开发中，你可以使用Thymeleaf来完全代替JSP或其他模板引擎，如Velocity、FreeMarker等。Thymeleaf的主要目标在于提供一种可被浏览器正确显示的、格式良好的模板创建方式，因此也可以用作静态建模。你可以使用它创建经过验证的XML与HTML模板。相对于编写逻辑或代码，开发者只需将标签属性添加到模板中即可。接下来，这些标签属性就会在DOM（文档对象模型）上执行预先制定好的逻辑。 ","date":"2023-08-08","objectID":"/springboot_draft/:4:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"单元测试 入门例子 @SpringBootTest public class Chapter11ApplicationTests { private MockMvc mvc; @Before public void setUp() { mvc = MockMvcBuilders.standaloneSetup(new HelloController()).build(); } @Test public void getHello() throws Exception { mvc.perform(MockMvcRequestBuilders.get(\"/hello\").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(content().string(equalTo(\"Hello World\"))); } } 对于文件上传接口，本质上还是http请求的处理，所以MockMvc依然逃不掉，就是上传内容发生了改变，我们只需要去找一下文件上传的模拟对象是哪个，就可以轻松完成这个任务。具体写法如下： @SpringBootTest(classes = Chapter43Application.class) public class FileTest { @Autowired protected WebApplicationContext context; protected MockMvc mvc; @BeforeEach public void setUp() { mvc = MockMvcBuilders.webAppContextSetup(context).build(); } @Test public void uploadFile() throws Exception { MockMultipartFile file = new MockMultipartFile( \"file\", \"hello.txt\", MediaType.TEXT_PLAIN_VALUE, \"Hello, World!\".getBytes() ); final MvcResult result = mvc.perform( MockMvcRequestBuilders .multipart(\"/upload\") .file(file)) .andDo(print()) .andExpect(status().isOk()) .andReturn(); } } 可以看到MockMvc的测试主体是不变的，无非就是请求类型和请求内容发生了改变。 ","date":"2023-08-08","objectID":"/springboot_draft/:4:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"定时任务 Springboot自带的@Scheduled 在Spring Boot的主类中加入@EnableScheduling注解，启用定时任务的配置 @SpringBootApplication @EnableScheduling public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 创建定时任务实现类 @Component public class ScheduledTasks { private static final SimpleDateFormat dateFormat = new SimpleDateFormat(\"HH:mm:ss\"); @Scheduled(fixedRate = 5000) public void reportCurrentTime() { log.info(\"现在时间：\" + dateFormat.format(new Date())); } } 这种模式实现的定时任务缺少在集群环境下的协调机制。解决这样问题的方式很多种，比较通用的就是采用分布式锁的方式，让同类任务之前的时候以分布式锁的方式来控制执行顺序，比如：使用Redis、Zookeeper等具备分布式锁功能的中间件配合就能很好的帮助我们来协调这类任务在集群模式下的执行规则。 Elastic Job Elastic Job的前生是当当开源的一款分布式任务调度框架，而目前已经加入到了Apache基金会。 pom.xml中添加elasticjob-lite的starter \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.shardingsphere.elasticjob\u003c/groupId\u003e \u003cartifactId\u003eelasticjob-lite-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e3.0.0\u003c/version\u003e \u003c/dependency\u003e // ... \u003c/dependencies\u003e 创建一个简单任务 @Slf4j @Service public class MySimpleJob implements SimpleJob { @Override public void execute(ShardingContext context) { log.info(\"MySimpleJob start : didispace.com {}\", System.currentTimeMillis()); } } 编辑配置文件 (properties) elasticjob.reg-center.server-lists=localhost:2181 elasticjob.reg-center.namespace=${spring.application.name} elasticjob.jobs.my-simple-job.elastic-job-class=com.didispace.chapter72.MySimpleJob elasticjob.jobs.my-simple-job.cron=0/5 * * * * ? elasticjob.jobs.my-simple-job.sharding-total-count=1 这里主要有两个部分： 第一部分：elasticjob.reg-center开头的，主要配置elastic job的注册中心和namespace 第二部分：任务配置，以elasticjob.jobs开头，这里的my-simple-job是任务的名称，根据你的喜好命名即可，但不要重复。任务的下的配置elastic-job-class是任务的实现类，cron是执行规则表达式，sharding-total-count是任务分片的总数。我们可以通过这个参数来把任务切分，实现并行处理。由于我们设置了分片总数为1，所以这个任务启动之后，只会有一个实例接管执行。这样就避免了多个进行同时重复的执行相同逻辑而产生问题的情况。同时，这样也支持了任务执行的高可用。 在整个实现过程中，我们并没有自己手工的去编写任何的分布式锁等代码去实现任务调度逻辑，只需要关注任务逻辑本身，然后通过配置分片的方式来控制任务的分割，就可以轻松的实现分布式集群环境下的定时任务管理了。 ","date":"2023-08-08","objectID":"/springboot_draft/:4:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"异步任务 为了让@Async注解能够生效，需要在Spring Boot的主程序中配置@EnableAsync @Async 所修饰的函数不要定义为static类型，这样异步调用不会生效 异步回调 @Test public void test() throws Exception { long start = System.currentTimeMillis(); CompletableFuture\u003cString\u003e task1 = asyncTasks.doTaskOne(); CompletableFuture\u003cString\u003e task2 = asyncTasks.doTaskTwo(); CompletableFuture\u003cString\u003e task3 = asyncTasks.doTaskThree(); CompletableFuture.allOf(task1, task2, task3).join(); long end = System.currentTimeMillis(); log.info(\"任务全部完成，总耗时：\" + (end - start) + \"毫秒\"); } 默认线程池参数 spring.task.execution.pool.core-size：线程池创建时的初始化线程数，默认为8 spring.task.execution.pool.max-size：线程池的最大线程数，默认为int最大值 spring.task.execution.pool.queue-capacity：用来缓冲执行任务的队列，默认为int最大值 spring.task.execution.pool.keep-alive：线程终止前允许保持空闲的时间 spring.task.execution.pool.allow-core-thread-timeout：是否允许核心线程超时 spring.task.execution.shutdown.await-termination：是否等待剩余任务完成后才关闭应用 spring.task.execution.shutdown.await-termination-period：等待剩余任务完成的最大时间 spring.task.execution.thread-name-prefix：线程名的前缀，设置好了之后可以方便我们在日志中查看处理任务所在的线程池 线程池隔离 第一步：初始化多个线程池，比如下面这样： @EnableAsync @Configuration public class TaskPoolConfig { @Bean public Executor taskExecutor1() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(2); executor.setQueueCapacity(10); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(\"executor-1-\"); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; } @Bean public Executor taskExecutor2() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(2); executor.setQueueCapacity(10); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(\"executor-2-\"); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; } } 注意：这里特地用executor.setThreadNamePrefix设置了线程名的前缀，这样可以方便观察后面具体执行的顺序。 第二步：创建异步任务，并指定要使用的线程池名称 @Slf4j @Component public class AsyncTasks { public static Random random = new Random(); @Async(\"taskExecutor1\") public CompletableFuture\u003cString\u003e doTaskOne(String taskNo) throws Exception { log.info(\"开始任务：{}\", taskNo); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); log.info(\"完成任务：{}，耗时：{} 毫秒\", taskNo, end - start); return CompletableFuture.completedFuture(\"任务完成\"); } @Async(\"taskExecutor2\") public CompletableFuture\u003cString\u003e doTaskTwo(String taskNo) throws Exception { log.info(\"开始任务：{}\", taskNo); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); log.info(\"完成任务：{}，耗时：{} 毫秒\", taskNo, end - start); return CompletableFuture.completedFuture(\"任务完成\"); } } 这里@Async注解中定义的taskExecutor1和taskExecutor2就是线程池的名字。由于在第一步中，我们没有具体写两个线程池Bean的名称，所以默认会使用方法名，也就是taskExecutor1和taskExecutor2。 线程池的拒绝策略 AbortPolicy策略：默认策略，如果线程池队列满了丢掉这个任务并且抛出RejectedExecutionException异常。 DiscardPolicy策略：如果线程池队列满了，会直接丢掉这个任务并且不会有任何异常。 DiscardOldestPolicy策略：如果队列满了，会将最早进入队列的任务删掉腾出空间，再尝试加入队列。 CallerRunsPolicy策略：如果添加到线程池失败，那么主线程会自己去执行该任务，不会等待线程池中的线程去执行。 而如果你要自定义一个拒绝策略，那么可以这样写： executor.setRejectedExecutionHandler(new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { // 拒绝策略的逻辑 } }); 当然如果你喜欢用Lamba表达式，也可以这样写： executor.setRejectedExecutionHandler((r, executor1) -\u003e { // 拒绝策略的逻辑 }); ","date":"2023-08-08","objectID":"/springboot_draft/:4:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Spring 简化数据访问的Spring Data 提供批处理能力的Spring Batch 用于保护应用安全的Spring Security ","date":"2023-08-08","objectID":"/springboot_draft/:5:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"自动装配原理 ","date":"2023-08-08","objectID":"/springboot_draft/:6:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"主启动器 @SpringBootApplication @SpringBootConfiguration @Configuration @Component @EnableAutoConfiguration 自动导入包🌟 @AutoConfigurationPackage @Import(AutoConfigurationPackages.Registrar.class) @Import(AutoConfigurationImportSelector.class) 自动注册包 getAutoConfigurationEntry(…) 获取自动配置的实体 org.springframework.boot:springboot-auto-configure/META-INF/spring.factories所有的自动配置类都在这里🌟 XXXAutoConfiguration：向容器中自动配置组件 xxxProperties：自动配置类，装配配置文件中自定义的一些内容 @ConditionalOnXXX 选择性生效 @ComponentScan 扫描当前主启动类同级的包 结论：SpringBoot所有的自动配置都在启动的时候扫描并加载，只有导入对应的 start ，通过启动器自动装配才会生效。xxxConfiguration 是用于扩展的。 ","date":"2023-08-08","objectID":"/springboot_draft/:6:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"实体类映射 yaml @Component @ConfigurationProperties(prefix = \"spring.datasource\") 松散绑定：last-name可以映射为 lastName 在yml中能配置的文件，都存在xxxProperties文件，在对应的xxxAutoConfiguration存在默认值 ","date":"2023-08-08","objectID":"/springboot_draft/:6:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"静态资源 localhost:8080/webjars/xxx localhost:8080/xxx resources\u003estatic(默认)\u003epublic ","date":"2023-08-08","objectID":"/springboot_draft/:6:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Thymeleaf 简单的表达： 变量表达式： ${…} 选择变量表达式： *{…} \u003cdiv th:object=\"${session.user}\"\u003e \u003cp\u003eName: \u003cspan th:text=\"*{firstName}\"\u003eSebastian\u003c/span\u003e.\u003c/p\u003e \u003cp\u003eSurname: \u003cspan th:text=\"*{lastName}\"\u003ePepper\u003c/span\u003e.\u003c/p\u003e \u003cp\u003eNationality: \u003cspan th:text=\"*{nationality}\"\u003eSaturn\u003c/span\u003e.\u003c/p\u003e \u003c/div\u003e 消息表达： #{…} [[#{login.btn}]] i18n配置页面国际化 i18n = internationalization k8s = kubernetes 链接 URL 表达式： @{…} 所有页面的静态资源都需要使用 thymeleaf 接管 片段表达式： ~{…} \u003cdiv th:replace=\"~{dashboard::topbar}\"\u003e\u003c/div\u003e \u003cdiv th:insert=\"~{dashboard::sidebar(active='list.html')}\"\u003e\u003c/div\u003e 其他示例： \u003cp th:if=\"${not #strings.isEmpty(msg)}\"\u003e123\u003c/p\u003e \u003ctr th:each=\"emp:${emps}\"\u003e \u003ctd th:text=\"${emp.getId()}\"/\u003e \u003ctd\u003e[[${emp.getLastName()}]]\u003c/td\u003e \u003ctd th:text=\"${emp.getGender()==0?'女':'男'}\"/\u003e \u003ctd th:text=\"${#dates.format(emp.getBirth(),'yyyy-MM-dd')}\"/\u003e \u003ctd\u003e \u003cbutton class=\"btn btn-sm btn-primary\"\u003e编辑\u003c/button\u003e \u003cbutton class=\"btn btn-sm btn-danger\"\u003e删除\u003c/button\u003e \u003c/td\u003e \u003c/tr\u003e ","date":"2023-08-08","objectID":"/springboot_draft/:6:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"其他 软件开发： 前端 设计数据库 接口对接：json、对象 前后端联调 开发要求： 有一套自己熟悉的后台模板（推荐x-admin） 前端界面：至少自己通过前端框架，组合出来一个网站页面 index about blog post user 让这个网站能独立运行 ","date":"2023-08-08","objectID":"/springboot_draft/:6:5","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"SpringSecurity SecurityConfig.java @EnableWebSecurity public class SecurityConfig extends WebSecurityConfigurerAdapter { /** * 认证 * 在 Spring Security 5.0+ 新增了加密方法，必须使用，否则报错： There is no PasswordEncoder mapped for the id \"null\" */ @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().passwordEncoder(new BCryptPasswordEncoder()) .withUser(\"josh\").password(new BCryptPasswordEncoder().encode(\"josh\")).roles(\"vip2\", \"vip3\") .and() .withUser(\"root\").password(new BCryptPasswordEncoder().encode(\"123456\")).roles(\"vip1\", \"vip2\", \"vip3\") .and() .withUser(\"guest\").password(new BCryptPasswordEncoder().encode(\"123456\")).roles(\"vip1\"); } /** * 授权 */ @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() // 首页所有人可以访问 .antMatchers(\"/\").permitAll() // 功能页有权限的人才能访问 .antMatchers(\"/level1/**\").hasRole(\"vip1\") .antMatchers(\"/level2/**\").hasRole(\"vip2\") .antMatchers(\"/level3/**\").hasRole(\"vip3\") .and() // 没有权限跳转登录页面 .formLogin().loginPage(\"/toLogin\").loginProcessingUrl(\"/login\") .usernameParameter(\"user\").passwordParameter(\"pwd\") .and() // 开启注销功能 .logout().logoutSuccessUrl(\"/\") .and() // 开启记住我功能 cookie 默认保存两周 .rememberMe().rememberMeParameter(\"remember\") .and() //关闭 csrf 跨站请求脚本攻击 .csrf().disable(); } } ","date":"2023-08-08","objectID":"/springboot_draft/:7:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Shiro Tutorial.java public class Tutorial { private static final transient Logger log = LoggerFactory.getLogger(Tutorial.class); public static void main(String[] args) { log.info(\"My First Apache Shiro Application\"); Factory\u003cSecurityManager\u003e factory = new IniSecurityManagerFactory(\"classpath:shiro.ini\"); SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); /** * 1、获取当前用户对象 */ Subject currentUser = SecurityUtils.getSubject(); /** * 2、 通过当前用户拿到 session 并对其操作 */ Session session = currentUser.getSession(); session.setAttribute(\"someKey\", \"aValue\"); String value = (String) session.getAttribute(\"someKey\"); /** * 3、 判断当前的用户是否被认证 */ final boolean authenticated = currentUser.isAuthenticated(); /** * 4、 用户信息 */ final Object principal = currentUser.getPrincipal(); /** * 5、 测试角色 */ final boolean schwartz = currentUser.hasRole(\"schwartz\"); /** * 6-1、 测试权限 - 粗力度 */ final boolean permitted = currentUser.isPermitted(\"lightsaber:wield\"); /** * 6-2、 测试权限 - 细力度 */ final boolean permitted2 = currentUser.isPermitted(\"winnebago:drive:eagle5\"); /** * 7、 注销 */ currentUser.logout(); System.exit(0); } } Shiro 三大对象： Subject：用户 SecurityManager：管理所有用户 Realm：连接数据 ShiroConfig.java @Configuration public class ShiroConfig { // 1、创建 realm 对象，需要自定义 @Bean(name = \"userRealm\") public UserRealm userRealm() { return new UserRealm(); } //2、 DefaultWebSecurityManager @Bean(name = \"defaultWebSecurityManager\") public DefaultWebSecurityManager getDefaultWebSecurityManager(@Qualifier(\"userRealm\") UserRealm userRealm) { DefaultWebSecurityManager defaultWebSecurityManager = new DefaultWebSecurityManager(); // 关联 userRealm defaultWebSecurityManager.setRealm(userRealm); return defaultWebSecurityManager; } //3、 ShiroFilterFactoryBean @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Qualifier(\"defaultWebSecurityManager\") DefaultWebSecurityManager defaultWebSecurityManager) { ShiroFilterFactoryBean bean = new ShiroFilterFactoryBean(); // 设置安全管理器 bean.setSecurityManager(defaultWebSecurityManager); // 添加 Shiro 的内置过滤器 /** * anon：无需认证就可以访问 * authc：必须认证才能访问 * user：必须拥有\"记住我\"功能才能访问 * perms：必须拥有对某个资源对权限才能访问 * role：必须拥有某个角色权限才能访问 */ Map\u003cString ,String \u003e filterChainDefinitionMap = new HashMap\u003c\u003e(); filterChainDefinitionMap.put(\"/userList\",\"authc\"); filterChainDefinitionMap.put(\"/user/add\",\"perms[user:add\"); bean.setFilterChainDefinitionMap(filterChainDefinitionMap); // 登录对请求 bean.setLoginUrl(\"/toLogin\"); // 未授权页面 bean.setUnauthorizedUrl(\"/noauth\"); return bean; } // 4. 整合 shiro 和 thymeleaf @Bean public ShiroDialect getShiroDialect(){ return new ShiroDialect(); } } UserRealm public class UserRealm extends AuthorizingRealm { @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { System.out.println(\"执行了授权=======》doGetAuthorizationInfo\"); final SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermission(\"user:add\"); final String permission = (String) SecurityUtils.getSubject().getPrincipal(); info.addStringPermission(permission); return info ; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { System.out.println(\"执行了认证=======》Override\"); String name = \"root\"; String permission = \"user:add\"; String password = \"123456\"; Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); session.setAttribute(\"name\",name); UsernamePasswordToken usernamePasswordToken = (UsernamePasswordToken)token; if (!usernamePasswordToken.getUsername().equals(name)){ // 自动抛出异常 return null; } // 密码认证 shiro 自动做 return new SimpleAuthenticationInfo(permission,password,\"\"); } } ","date":"2023-08-08","objectID":"/springboot_draft/:8:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Swagger2 SwaggerConfig.java @Configuration @EnableSwagger2 public class SwaggerConfig { @Bean public Docket docket(Environment environment) { // 设置要显示的 Swagger 环境 Profiles profiles = Profiles.of(\"dev\", \"test\"); // 获取项目的环境 final boolean flag = environment.acceptsProfiles(profiles); 1 return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .groupName(\"A\") .enable(flag) .select() // basePackage：指定要扫描的包 .apis(RequestHandlerSelectors.basePackage(\"edu.zjut.demospringboot.controller\")) // withMethodAnnotation：只扫描限定方法注解 .apis(RequestHandlerSelectors.withMethodAnnotation(RestController.class)) // ant：只扫描指定路由路径 .paths(PathSelectors.ant(\"/hello/**\")) .build(); } @Bean public Docket docket2(Environment environment) { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .groupName(\"B\"); } @Bean public Docket docket3(Environment environment) { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .groupName(\"C\"); } private ApiInfo apiInfo() { return new ApiInfo( \"xxxxAPI文档\", \"文档描述\", \"v1.0\", \"zjut\", new Contact(\"josh\", \"\", \"dujianghui537885@163.com\"), \"Apach 2.0\", \"http://www.apache.org/licenses/LICENSE-2.0\", new ArrayList\u003c\u003e()); } } 注意：在正式发布的时候，关闭Swagger！ ","date":"2023-08-08","objectID":"/springboot_draft/:9:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"邮件发送 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-mail\u003c/artifactId\u003e \u003c/dependency\u003e @Autowired private JavaMailSenderImpl mailSender; /** * 简单邮件发送 */ @Test void simpleMailMessage() { SimpleMailMessage simpleMailMessage = new SimpleMailMessage(); simpleMailMessage.setSubject(\"你好呀～\"); simpleMailMessage.setText(\"测试一下邮箱\"); simpleMailMessage.setTo(\"2869314652@qq.com\"); simpleMailMessage.setFrom(\"2869314652@qq.com\"); mailSender.send(simpleMailMessage); } /** * 复杂邮件发送 * @throws MessagingException */ @Test void mimeMessage() throws MessagingException { final MimeMessage mimeMessage = mailSender.createMimeMessage(); final MimeMessageHelper helper = new MimeMessageHelper(mimeMessage,true); helper.setSubject(\"你好呀～plus\"); helper.setText(\"\u003cp style='color:red'\u003e测试一下邮箱\u003c/p\u003e\",true); helper.setTo(\"2869314652@qq.com\"); helper.setFrom(\"2869314652@qq.com\"); mailSender.send(mimeMessage); } ","date":"2023-08-08","objectID":"/springboot_draft/:10:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"任务调度 TaskScheduler：任务调度者 TaskExecutor：任务执行者 EnableScheduling：开启定时功能的注解 Scheduled：什么时候执行 Cron 表达式 @Service public class ScheduledService { // 秒 分 时 日 月 周 年 @Scheduled(cron = \"0/3 * * * * ?\") public void hello(){ System.out.println(\"Scheduled.......\"); } } ","date":"2023-08-08","objectID":"/springboot_draft/:11:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Redis 底层从 jedis 变为 lettuce jedis：采用的直连，不安全，除非使用 jedis pool 线程池，类似 BIO lettuce：采用 netty，实例可以在多个线程中共享，安全，可以减少线程数据，类似 NIO pom.xml \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e application.yml spring: redis: host: 127.0.0.1 port: 6379 所有 pojo 都要序列化 implements Serializable，否则就得通过以下方法序列化 // 序列化 final String jsonUser = new ObjectMapper().writeValueAsString(user); 配置RedisTemplate @Configuration public class RedisConfig { @Bean @SuppressWarnings(\"all\") public RedisTemplate\u003cString, Object\u003e redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate\u003cString, Object\u003e template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); // key、hash 的 key 都采用 String 的序列化方式 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); template.setKeySerializer(stringRedisSerializer); template.setHashKeySerializer(stringRedisSerializer); // value、hash 的 value 都采用 json 的序列化方式 Jackson2JsonRedisSerializer\u003cObject\u003e jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer\u003c\u003e(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); template.setValueSerializer(jackson2JsonRedisSerializer); template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; } } RedisUtil.java package edu.zjut.demospingboot.utils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.stereotype.Component; import java.util.List; import java.util.Map; import java.util.Set; import java.util.concurrent.TimeUnit; /** * 操作 Redis 的工具类 * @author dujianghui-zjut * @date 2021/12/6 09:01 * @email dujianghui537885@163.com */ @Component public class RedisUtil { @Autowired private RedisTemplate\u003cString,Object\u003e redisTemplate; /** * 指定缓存失效时间 * @param key 键 * @param time 时间 * @return */ public boolean expire(String key,long time){ try{ if (time\u003e0){ redisTemplate.expire(key,time, TimeUnit.SECONDS); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 根据 key 获取过期时间 * @param key 不能为null * @return 时间（秒），返回 0 表示永久有效 */ public long getExpire(String key){ return redisTemplate.getExpire(key,TimeUnit.SECONDS); } /** * 判断 key 是否存在 * @param key 键 * @return true 存在 false 不存在 */ public boolean hasKey(String key){ try{ return redisTemplate.hasKey(key); } catch (Exception e){ e.printStackTrace(); return false; } } /** * 普通缓存获取 * @param key 键 * @return 值 */ public Object get(String key){ return key==null?null:redisTemplate.opsForValue().get(key); } /** * 普通缓存放入 * @param key 键 * @param value 值 * @return true 成功 false失败 */ public boolean set(String key,Object value){ try{ redisTemplate.opsForValue().set(key,value); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 普通缓存放入并设置时间 * @param key 键 * @param value 值 * @param time 时间（秒） time要大于0 否则讲设置无限期 * @return true 成功 false失败 */ public boolean set(String key,Object value,long time){ try{ if (time\u003e0){ redisTemplate.opsForValue().set(key,value,time,TimeUnit.SECONDS); }else { set(key,value); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 递增 * @param key 键 * @param delta 要增加几（大于0） * @return */ public long incr(String key,long delta){ if (delta\u003c0){ throw new RuntimeException(\"递增因子必须大于0\"); } return redisTemplate.opsForValue().increment(key,delta); } /** * 递增 * @param key 键 * @param delta 要减少几（小于0） * @return */ public long desr(String key,long delta){ if (delta\u003e0){ throw new RuntimeException(\"递减因子必须小于0\"); } return redisTemplate.opsForValue().increment(key,-delta); } /** * HashGet * @param key 键 notnull * @param item 项 notnull * @return */ public Object hget(String key,String item){ return redisTemplate.opsForHash().get(key,item); } /** * 获取 hashKey 对应的所有键值 * @param key 键 * @return 对应的多个键值 */ public Map\u003cObject,Object\u003e hmget(String key){ return redisTemplate.opsForHash().entries(key); } /** * HashSet * @param key 键 * @param map 对应多个键值 * @return */ public boolean hmset(String key,Map\u003cString,Object\u003e map){ try{ redisTemplate.opsForHash().putAll(key,map); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * HashSet 并设置时间 * @param key 键 * @param map 对应多个键值 * @param time 时间（秒） * @return true 成功 false 失败 */ public boolean hmset(String key,Map\u003cString,Object\u003e map,long time){ try{ redisTemplate.opsForHash().putAll(key,map); if (time\u003e0){ expire(key,time); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 向一张 hash 表中放入数据，如果不存在将创建 * @param key 键 * @param item 项 * @param value 值 * @return true 成功 false 失败 */ public boolean hset(String key,String item,Object value){ try{ redisTemplate.opsForHash().put(key,item,value); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 向一张 hash 表中放入数据，如果不存在将创建，并设置时间 * @param key 键 * @param item 项 * @param value 值 * @param time 时间（秒），如果已存在的 hash 表有时间，这里将会替换原有时间 * @return */ public boolean hset(String key,String item,Object value,long time){ try{ redisTemplate.opsForHash().put(key,item,value); if (time\u003e0){ expire(key,time); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 删除 hash 表中的值 * @param key 键 notnull * @param item 项 notnull */ public void hdel(String key,Object... item){ redisTemplate.opsForHash().delete(key,item); } /** * 判断 hash 表中是否有该项的值 * @param key 键 notnull * @param item 项 notnull * @return true 存在 false 不存在 */ public boolean hHasKey(String key,String item){ return redisTemplate.opsForHash().hasKey(key,item); } /** * hash 递增，如果不存在就会创建一个，并把新增后的值返回 * @param key 键 * @param item 项 * @param by 要增加几（大于0） * @return */ public double hincr(String key,String item,double by){ return redisTemplate.opsForHash().increment(key,item,by); } /** * hash 递减，如果不存在就会创建一个，并把新增后的值返回 * @param key 键 * @param item 项 * @param by 要增加几（大于0） * @return */ public double hdecr(String key,String item,double by){ return redisTemplate.opsForHash().increment(key,item,-by); } /** * 根据 key 获取 Set 中的所有值 * @param key 键 * @return */ public Set\u003cObject\u003e sGet(String key){ try{ return redisTemplate.opsForSet().members(key); }catch (Exception e){ e.printStackTrace(); return null; } } /** * 根据 value，从一个 set 中查询，是否存在 * @param key 键 * @param value 值 * @return true 存在 false 不存在 */ public boolean sHasKey(String key,Object value){ try{ return redisTemplate.opsForSet().isMember(key,value); }catch (Exception e){ e.printStackTrace(); return false; } } /** * 将数据放入 set 缓存 * @param key 键 * @param values 值 可以是多个 * @return 成功个数 */ public long sSet(String key,Object... values){ try{ return redisTemplate.opsForSet().add(key,values); } catch (Exception e){ e.printStackTrace(); return 0; } } /** * 将数据放入 set 缓存，并设置时间 * @param key 键 * @param time 时间（秒） * @param values 值 可以是多个 * @return 成功个数 */ public long sSetAndTime(String key,long time,Object... values){ try{ final Long count = redisTemplate.opsForSet().add(key, values); if (time\u003e0){ expire(key,time); } return count; } catch (Exception e){ e.printStackTrace(); return 0; } } /** * 获取 set 缓存的长度 * @param key 键 * @return */ public long sGetSetSize(String key){ try{ return redisTemplate.opsForSet().size(key); }catch (Exception e){ e.printStackTrace(); return 0; } } /** * 移除值为 value 的 * @param key 键 * @param values 值，可以是多个 * @return 移除的个数 */ public long setRemove(String key,Object... values){ try { return redisTemplate.opsForSet().remove(key, values); }catch (Exception e){ e.printStackTrace(); return 0; } } /** * 获取 list 缓存的内容 * @param key 键 * @param start 开始 * @param end 结束，0 到 -1 表示所有值 * @return */ public List\u003cObject\u003e lGet(String key,long start,long end){ try{ return redisTemplate.opsForList().range(key,start,end); }catch (Exception e){ e.printStackTrace(); return null; } } /** * 获取 list 缓存的长度 * @param key 键 * @return */ public long lGetlistSize(String key){ try { return redisTemplate.opsForList().size(key); }catch (Exception e){ e.printStackTrace(); return 0; } } /** * 通过索引获取 list 中的值 * @param key 键 * @param index 索引，index\u003e=0时，0 表头，1 第二个元素；index\u003c0时，-1 表尾，-2 倒数第二个元素。 * @return */ public Object lGetIndex(String key,long index){ try{ return redisTemplate.opsForList().index(key,index); }catch (Exception e){ e.printStackTrace(); return null; } } /** * 将 list 放入缓存 * @param key 键 * @param value 值 * @return */ public boolean lSet(String key,Object value){ try{ redisTemplate.opsForList().rightPush(key,value); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 将 list 放入缓存 * @param key 键 * @param value 值 * @return */ public boolean lSet(String key,List\u003cObject\u003e value){ try{ redisTemplate.opsForList().rightPushAll(key,value); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 将 list 放入缓存，并设置时间 * @param key 键 * @param value 值 * @param time 时间（秒） * @return */ public boolean lSet(String key,Object value,long time){ try{ redisTemplate.opsForList().rightPush(key,value); if (time\u003e0){ expire(key,time); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 将 list 放入缓存，并设置时间 * @param key 键 * @param value 值 * @param time 时间（秒） * @return */ public boolean lSet(String key,List\u003cObject\u003e value,long time){ try{ redisTemplate.opsForList().rightPushAll(key,value); if (time\u003e0){ expire(key,time); } return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 根据索引修改 list 中的某条数据 * @param key 键 * @param index 索引 * @param value 值 * @return */ public boolean lUpdateIndex(String key,long index,Object value){ try{ redisTemplate.opsForList().set(key,index,value); return true; }catch (Exception e){ e.printStackTrace(); return false; } } /** * 移除 N 个值为 value * @param key 键 * @param count 移除多少个 * @param value 值 * @return 移除的个数 */ public long lRemove(String key,long count,Object value){ try{ return redisTemplate.opsForList().remove(key,count,value); }catch (Exception e){ e.printStackTrace(); return 0; } } } ","date":"2023-08-08","objectID":"/springboot_draft/:12:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"分布式开发：Dubbo（RPC）+zookeeper Registry 注册中心 Monitor 监控中心：Dubbo 四个问题： API 网关，服务路由 HTTP、RPC 框架，异步调用 服务注册与发现，高可用 熔断机制，服务降级 ","date":"2023-08-08","objectID":"/springboot_draft/:13:0","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Spring Cloud Netflix（2018年停止维护） Eureka：服务注册与发现 Hystrix：熔断机制 ","date":"2023-08-08","objectID":"/springboot_draft/:13:1","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Apache Dubbo + zookeeper Dubbo：高性能的基于 Java 实现的 RPC 通信框架 Zookeeper：管理 Hadoop、Hive ","date":"2023-08-08","objectID":"/springboot_draft/:13:2","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Springcloud Alibaba 一站式解决方案 ","date":"2023-08-08","objectID":"/springboot_draft/:13:3","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["draft"],"content":"Server Mesh+istio 服务网格 ","date":"2023-08-08","objectID":"/springboot_draft/:13:4","tags":["SpringBoot"],"title":"SpringBoot 杂记","uri":"/springboot_draft/"},{"categories":["interview"],"content":"🌟AOP（Aspect Oriented Programming）面向切面编程 作用：在不惊动原始设计的基础上为其进行功能增强。（无侵入式） AOP 基本概念 在 pom.xml 中导入 aspectjweaver ，配置 @EnableAspectJAutoProxy 连接点（JoinPoint）：方法【可利用的机会】 切入点（PointCut）：是连接点的子集，要追加功能的方法 【解决了切面编程中的 Where 问题，让程序可以知道哪些机会点可以应用某个切面动作】 [@Pointcut(“execution(void ](/Pointcut() com.dao.xxxDao.update())”) 修饰的方法 pt() 通知（Advice）：有共性的功能 【明确了切面编程中的 What，也就是做什么；同时通过指定 Before、After 或者 Around，定义了 When，也就是什么时候做。】 @Before(“pt()”) 形参：JoinPoint @After(“pt()”) @Around (“pt()”) public void around(ProceedingJoinPoint pjp) throws Throwable{ pjp.proceed();} （重点，常用） 形参：ProceedingJoinPoint（必须放在参数的第一位） 要注意返回值和原始方法一致。 如果不执行pjp.proceed()，可以实现对原始方法的隔离（权限校验） @Component @Aspect public class ProjectAdvice{ @Pointcut(\"execution(* com.edu.zjut.service.*Service.*(..))\") private void servicePt(){} @Around(\"ProjectAdvice.servicePt()\") public void runSpeed(ProceedingJoinPoint pjp) throws Throwable{ Signature signature = pjp.getSignature(); String className = signature.getDeclaringTypeNmae(); String methodName = signature.getName(); long start = System.currentTimeMillis(); for(int i=0;i\u003c10000;i++){ pjp.proceed(); } long end = System.currentTimeMillis(); System.out.println(\"业务层接口万次执行\"+ className + \".\" + methodName + \"------\u003e\" + (end-start)+\"ms\"); } } AfterReturning(“pt()”) 只有不抛异常才运行 AfterThrowing(“pt()\")只有抛异常才运行 切面（Aspect）：通知和切入点的联系（多对多） 通知类：定义通知的类 @Aspect AOP 工作流程 AOP的本质是代理模式 Spring容器启动 读取切面中配置的切入点 初始化bean，若类中方法匹配到切入点，则创建代理对象 获取bean执行方法 切入点表达式 标准格式：execution(返回值 包.类.接口（推荐）/实现类.方法()) 通配符：*匹配任意符号，..匹配多个连续的任意符号，如：execution(_ _..Service+.(..))`匹配业务层所有接口 书写技巧： 所有类命名需规范，否则以下失效 描述切入点通常描述接口，而不描述实现类 增删改类使用精准类型加速匹配，查询类使用*通配快速描述 方法名以动词进行精准匹配 包名书写尽量不使用 .. 匹配，效率太低，常用 * 做单个包描述匹配 不使用异常作为匹配规则 ","date":"2023-08-08","objectID":"/spring/:0:1","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"什么是动态代理？ 动态代理是一种方便运行时动态构建代理、动态处理代理方法调用的机制，很多场景都是利用类似机制做到的，比如用来包装 RPC 调用、面向切面的编程（AOP），有两种实现： JDK：实现接口，运用反射。如果目标对象实现了至少一个接口，Spring AOP将使用JDK动态代理来生成代理对象。代理对象实现了与目标对象相同的接口，并在方法调用前后织入切面逻辑。JDK动态代理是通过java.lang.reflect.Proxy类和java.lang.reflect.InvocationHandler接口实现的。 cglib：修改字节码，生成子类。如果目标对象没有实现任何接口，Spring AOP将使用CGLIB库生成代理对象。CGLIB代理通过继承目标对象的子类来生成代理对象，然后在子类中织入切面逻辑。CGLIB代理可以处理没有接口的目标对象。 基于 ASM，可以直接产生二进制 class 文件，也可以在类被加载入 Java 虚拟机之前动态改变类行为 ","date":"2023-08-08","objectID":"/spring/:0:2","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"Spring 事务 编程式：TransactionTemplate 对业务有侵入性，少用。 声明式：@EnableTransactionManagement + @TranSactional 通过 AOP 将事务处理的功能编织到拦截的方法中（方法前开启事务，方法后酌情回滚） 事务失效场景 异常捕获 =\u003e 在 catch 中 throw 异常 抛出检查异常 =\u003e 指定 rollbackfor，因为默认只回滚非检查异常 非 public 方法 ","date":"2023-08-08","objectID":"/spring/:0:3","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"循环依赖如何解决？三级缓存 or @lazy 三级缓存只能解决初始化过程中的循环依赖： 一级 singletonObjects：单例池，存放已初始化完成的 bean 二级 earlySingletonObjects：存放生命周期未走完的 bean 三级 singletonFactories：对象工厂，用来创建对象 对于构造方法中的循环依赖，采用 @lazy 进行懒加载 ","date":"2023-08-08","objectID":"/spring/:0:4","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"Spring Cloud 是如何调用的？ RestTemplate方式调用 注入 RestTemplate 的bean，添加@Bean注解 调用 .getForObject() 或 .postForObject() 方法 Feign方式调用 添加 pom 依赖 启动类添加 @EnableFeignClients 注解 写接口： @FeignClient(\"userservice\") public interface UserClient{ @GetMapping(\"/user/{id}\") User findById(@PathVariable(\"id\") Long id); } 使用 FeignClient 中定义的方法取代 RestTemplate（并集成了 ribbon，实现了负载均衡） IRule 接口决定负载均衡策略，并选择某个服务： 1. ZoneAvoidanceRule（默认）：对Zone区域内多个服务轮询，zone值可配置 2. RoundRobinRule：轮询 3. WeightedResponseTimeRule：服务响应时间越长，服务器权重就越小，概率就越低 4. RandomRule：随机 ","date":"2023-08-08","objectID":"/spring/:0:5","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"SpringMVC 执行流程？ 与 DispatcherServlet（前端控制器） 依次交互的有： HandlerMapping （处理器映射器）：找到类名#方法名 HandlerAdaptor（适配器）：处理参数、返回值 ViewResolver（视图解析器，用于 JSP）：逻辑视图 -\u003e 视图 对于前后端分离项目，在方法上添加 @ResponseBody，通过 HttpMessageConverter 响应 JSON 数据 @RestController = @Controller + @ResponseBody ","date":"2023-08-08","objectID":"/spring/:0:6","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"JWT JWT 数据结构？ head payload 用户信息 加密信息 过期时间 signature JWT 和 token 的区别？ token 需要查数据库，JWT 不需要。 security和jwt怎么融合？ 认证：主要 jwt 的 token 实现 鉴权：springSecurity实现，RBAC = Role-Based Access Control 从cookie到jwt解决了什么？ 可以转化为 json 串，可读性强； RESTful 设计原则，避免在服务端保存会话状态，降低了服务器端的负担； 签名机制，安全性强。 token 泄露怎么处理？ 采用非对称加密：给前端一个公钥，加密数据传到后台，用私钥解密后处理数据。 ","date":"2023-08-08","objectID":"/spring/:0:7","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"Springboot的自动装配原理？ @SpringbootApplication @SpringBootConfiguration 声明当前类为配置类 @ComponentScan 组件扫描 @EnableAutoConfiguration 自动化配置 @Import 读取了项目和项目引用的 jar 包，位于 classpath 路径下 META-INF/spring.factories 文件 @ConditionalOnClass 有则加载 注入方式 对于引用类型： 按类型 byType（推荐）：**@Autowired **** ** 使用的是暴力反射，不需要 setter方法 按名称 byName（变量名与配置耦合，不推荐） @Autowired + @Qualifier(“beanName “) @Resource(“名称”) 按构造方法（不常用） 对于简单属性：使用 @Value(“xxx”)注入，可以来自外部 property，写法：@Value(\"${name}\") 加载外部properties 使用 @PropertySource(“classpath:jdbc.properties”)，不可以使用星号* ","date":"2023-08-08","objectID":"/spring/:0:8","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"谈谈Spring Bean的生命周期和作用域？ 生命周期 实例化（Instantiation）：在这个阶段，Spring 会使用配置元数据（XML、注解或Java配置）来创建 Bean 的实例。这通常是通过调用构造函数来完成的。 属性赋值（Population）：在实例化之后，Spring 将会通过依赖注入（Dependency Injection）或属性赋值的方式来设置 Bean 的属性和依赖。这可以通过 setter 方法注入或使用字段注入来实现。 初始化（Initialization）：在属性赋值完成后，Spring 将会调用 Bean 的初始化回调方法（如果有的话）。开发者可以在 Bean 中定义初始化方法，使用 @PostConstruct 注解或实现 InitializingBean 接口来指定初始化逻辑。 使用（In Use）：在初始化完成后，Bean 可以被正常使用。此时，Bean 处于活动状态，可以被其他组件或者应用程序使用。 销毁（Destruction）：当应用程序关闭或者不再需要某个 Bean 时，Spring 会调用销毁回调方法来进行资源释放或清理操作。开发者可以在 Bean 中定义销毁方法，使用 @PreDestroy 注解或实现 DisposableBean 接口来指定销毁逻辑。 使用模版方法设计模式实现： 实例化 Bean：首先，Spring会实例化 Bean，即创建 Bean 的对象。 Aware接口回调：如果 Bean 实现了任何 Aware 接口（例如 BeanNameAware、BeanFactoryAware、ApplicationContextAware），在实例化之后但在属性赋值之前，Spring会调用相应的 Aware 接口回调方法，将相关的引用或资源传递给 Bean。 属性赋值：在 Aware 接口回调之后，Spring会进行属性赋值（依赖注入），将配置或注入的属性值设置到 Bean 中。 BeanPostProcessor前置处理：在属性赋值之后，Spring会调用所有注册的 BeanPostProcessor 的 postProcessBeforeInitialization 方法，允许开发者在 Bean 的初始化之前进行自定义处理。 初始化方法调用：如果 Bean 定义了初始化方法（例如通过 @PostConstruct 注解或实现 InitializingBean 接口），在前置处理之后，Spring会调用初始化方法。 BeanPostProcessor后置处理：在初始化方法调用之后，Spring会再次调用所有注册的 BeanPostProcessor 的 postProcessAfterInitialization 方法，允许开发者在 Bean 的初始化之后进行自定义处理。 Bean 可用：此时，Bean已经完成了初始化过程，可以正常使用。 五个作用域 其中最基础的有下面两种： Singleton，这是 Spring 的默认作用域，也就是为每个 IOC 容器创建唯一的一个 Bean 实例。【适合无状态】 Prototype，针对每个 getBean 请求，容器都会单独创建一个 Bean 实例。【适合有状态】 如果是 Web 容器： Request，为每个 HTTP 请求创建单独的 Bean 实例。 Session，很显然 Bean 实例的作用域是 Session 范围。 GlobalSession，用于 Portlet 容器，因为每个 Portlet 有单独的 Session，GlobalSession 提供一个全局性的 HTTP Session。 ","date":"2023-08-08","objectID":"/spring/:0:9","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"IoC（Inversion of Control）控制反转 使用对象时，不再通过 new 产生对象，创建控制权由程序转移到外部的IoC容器（ApplicationContext，接口） bean 之间的依赖注入方式 setter注入（建议）：更加灵活，可以在对象创建以后动态更改依赖关系 构造方法注入：一旦创建完成，依赖关系就不能再改变 ","date":"2023-08-08","objectID":"/spring/:0:10","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"Q：拦截器（Interceptor）与过滤器（Filter）的区别？ A： 归属不同：Filter属于Servlet技术，Interceptor属于SpringMVC技术 拦截内容不同：Filter对所有访问进行增强，Interceptor仅针对SpringMVC的访问进行增强 执行流程 preHandle（return true 才走下一步，false结束） controller postHandle AfterCompletion 拦截器（Interceptor） 是一种动态拦截方法调用的机制，作用： 在指定的方法调用前后执行预先设定的代码 组织原始方法的执行 // preHandle 方法中可以根据返回值决定是否继续执行方法 // 并且可以通过反射拿到拦截方法，进行更多操作 HandlerMethod hm = (HandlerMethod) handler; hm.getMethod(); 拦截器链 preHandle在前的拦截器，postHandle 和 afterCompletion 在后。 123 都为 true：pre 123 -\u003e controller -\u003e post 321 -\u003e after 321 12 true 3 false：pre12 -\u003e after 21 13 true 2 false： pre1 -\u003e after 1 23 true 1 false：都不执行 ","date":"2023-08-08","objectID":"/spring/:0:11","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"全局异常处理 表现层通过AOP处理 @RestControllerAdvice public class GlobalExceptionHandler { @ExceptionHandler(value = Exception.class) public CommonResult handleUnknownException(Exception e) { return CommonResult.failed(ResultCode.UNKN0OWN_FAILED,\"系统繁忙，请稍后再试！\"); } } 业务异常：提醒用户规范操作 系统异常：安抚用户，通知运维，记录日志 ","date":"2023-08-08","objectID":"/spring/:0:12","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"REST风格（Representational State Transfer）表现形式状态转换 资源名称采用复数，如users、books GET：查询 POST：新增 PUT：修改 DELETE：删除 ","date":"2023-08-08","objectID":"/spring/:0:13","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"Q：如何避免Spring加载SpringMVC已加载的Controller？ A：加载Spring控制的bean的时候，排除掉SpringMVC控制的bean。 @ComponentScan(value = \"edu.zjut\",excludeFilters=@ComponentScan.Filter(type=FilterType.ANNOTATION,classes=Controller.class))按照注解排除 @ComponentScan({\"edu.zjut.service\",\"edu.zjut.dao\"}) 不区分Spring和SpringMVC的环境，都加载 ","date":"2023-08-08","objectID":"/spring/:0:14","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"参数传递 @RequestParam(“xxx”)：请求参数和形参不同时需配置，不写的话默认取相同。 @RequestParam 修饰 List 就可以实现集合传参。 参数对象中有对象时采用 address.city=beijing 这种形式。 @RequestBody：设置controller方法返回值为响应体。传字符串（json）用 HttpMessageConverter 日期参数：接收可以用 Date 接，加上 @DateTimeFormat(pattern = “yyyy-MM-dd”) @PathVariable 从路径中取值 ","date":"2023-08-08","objectID":"/spring/:0:15","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"SpringMVC 基于 Java 实现 MVC 模型的轻量级 web 框架。 后端服务器： 表现层：SpringMVC（取代 Servlet），封装数据（code、msg、data），用postman测试 业务层：Spring，用JUnit测试 @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = SpringConfig.class) 数据层：Mybatis 返回 json 数据，需要加 @ResponseBody 注解。 ","date":"2023-08-08","objectID":"/spring/:0:16","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["interview"],"content":"🌟Q：如何保证无论转账是否成功都记录日志留痕？ A：日志对应的事务不要加入转账的事务中，在插入日志的方法上加上 @Transactional(propagation = Propagation.REQUIRES_NEW) 事务传播行为：作为协调员， REQUIRED（默认）：事务管理员有事务就加入，没事务就新建 REQUIRED_NEW：无论如何都新建事务 SUPPORTS：事务管理员有事务就加入，没有就算了 NOT_SUPPORTS：无乱如何都不支持事务 MANDATORY：事务管理员有事务就加入，没有就报错 NEVER：事务管理员没事务没关系，有事务就报错 NESTED：设置 savePoint ","date":"2023-08-08","objectID":"/spring/:0:17","tags":["Spring"],"title":"🚩Spring","uri":"/spring/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-08","objectID":"/mysql_draft/","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"范式 1NF：消除重复数据，每一列都是原子的；所有属性都不能再分解为更基本的数据单位； 2NF：消除部分依赖，每一行都被主键唯一标识；所有非主属性都依赖于关键属性，所有列都依赖于任意一组候选关键字； 3NF：消除传递依赖，表中每一列只依赖主键而不是其他列；每一列都与任意候选关键字直接相关而不是间接相关； BCNF：消除主属性对于码的部分与传递函数依赖。 ","date":"2023-08-08","objectID":"/mysql_draft/:0:1","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"MySQL 存储 独占模式： 日志组文件：默认5M【ib_logfile0】【ib_logfile1】 表结构文件：【*.frm】 独占表空间文件：【*.ibd】 字符集和排序规则文件：【db.opt】 binlog 二进制日志文件：记录主数据库服务器的 DDL 和 DML 操作 DDL：Data Define Language，数据定义语言，如 CREATE、ALTER、DROP DML：Data Manipulation Language，数据操作语言，如 INSERT、UPDATE、DELETE 二进制日志索引文件：【master-bin.index】 共享模式： innodb_file_per_table = 1，数据都在 ibdata1 12.1.5 MySQL 执行流程 查询缓存：通过一个对大小写敏感的哈希查找实现的。 解析器生成解析树：语法层面，验证关键字是否拼写正确、顺序正确、引号对应。 预处理器优化解析树：逻辑层面，检查数据表和数据列是否存在，名字是否有歧义，验证权限 查询优化器： 最重要的一部分是关联查询优化； 用 distinct 和 group by 会产生临时中间表； 静态优化：“编译时优化”，不依赖于特别的数值，在第一次完成后就一直有效； 动态优化：“运行时优化”，和查询的上下文有关。 查询执行引擎，结果存入查询缓存 调用存储引擎接口 执行引擎的状态： sleep：线程正在等待客户端发送新请求； query：正在执行查询或正在将结果发送给客户端； locked：mysql 服务器层，线程正在等待表锁； analying and statistics：线程正在收集存储引擎的统计信息，并声称查询的执行计划。 copying to temp table：线程正在执行查询，并将结果复制到临时表中，一般为 GROUP BY 操作，要么是文件排序，或 UNION； on disk：将内存中的临时表放到磁盘上； sorting result：线程正在对结果集进行排序； sending data：线程在多个状态之间传递数据，或生成结果集，或向客户端返回数据。 执行顺序（并不绝对，比如在 select 筛选出找到的数据集）： 关系结构：FROM，ON，JOIN 筛选数据：WHERE，GROUP BY，HAVING，SELECT 展示数据：ORDER BY，LIMIT 分组： WHERE 过滤行（在数据分组前），HAVING 过滤分组 UNION 自动去除重复行，如果想返回所有，使用UNION ALL UNION 后的 ORDER BY 会排序所有结果 having和where的区别？ WHERE 子句用来筛选 FROM 子句中指定的操作所产生的行。 GROUP BY 子句用来分组 WHERE 子句的输出。 HAVING 子句用来从分组的结果中筛选行。 优先级：where\u003e 聚合语句(sum,min,max,avg,count) \u003ehaving子句 LIMIT LIMIT a,b：返回从行 a 开始的 b 行，a从0开始。 等同于：LIMIT b OFFSET a REGEXP 在列值内匹配，头尾加上^ $ 才与 LIKE 等价。 |：表示 or [123]：匹配 1 或 2 或 3 ，可以写成[1-3] [^123]：匹配除了 1 或 2 或 3 之外的字符 .：匹配任意字符 ？:前一个字符出现 0 或 1 次 {n}：出现 n 次 ^：文本的开始 $：文本的结尾 [[:\u003c:]]：词的开始 [[:\u003e:]]：词的结尾 函数 Concat()：连接 LTrim()、RTrim()：去掉左边或右边的所有空格 Upper()、Lower()：转换大小写 Soundex()：返回类似发音的结果 Date()：日期，格式为 yyyy-mm-dd Time()：时间，格式为 hh:mm:ss AVG()：求平均数，忽略列值为null 的行 count：计数，count(*)统计行数，包括 null 值；count(column) 忽略 null 值 count（） count（col）：统计有值的结果数，不包括null count（*）：统计结果集的行数 1. MyISAM 在 COUNT(*) 全表非常快 -- 优化前：要扫描全表 select count(*) from world.city where id \u003e5; -- 优化后：只扫描5行 select (select count(*) from world.city) - count(*) from world.city where id \u003c= 5; 2. 用 count 也可以实现 sum -- sum select sum(if(color='blue',1,0)) as blue,sum(if(color='red',1,0)) as red from items -- count select count(color = 'blue' or null) as blue,count(color='red' or null) as red from items Match+Against # 结果无序 WHERE column LIKE '%word%' # 结果有序 WHERE Match(column) Against('word') Match() 指定被搜索的列，Against() 指定要使用的搜索表达式。 12.1.6 数据结构 （1）整数类型 tinyint：8位 smallint：16位 mediumint：24位 int：32位 bigint：64位 默认允许负值：-2(n-1) ~ 2(n-1)-1 ，添加 unsigned 属性不允许负值，范围为 0 ~ 2n-1 int(11)：不会限制值的合法范围，只会控制显示字符的个数。 IPv4地址用 unsigned int 来存储，因为它实际上是 32 位无符号整数，不是字符串， INET_ATON() ：将一个字符串IP地址转换为一个32位的网络序列IP地址 INET_NTOA：讲一个32位的网络序列IP地址转换为一个字符串IP地址 （2）实数类型 float：4 字节 double：8 字节 （3）字符串类型 字符方式（有排序规则和字符集）/二进制方式（没有排序规则或字符集） varchar/varbinary：需要额外空间存储长度 char/binary：会截断最后的空格。适合存储以下类型： 短字符串 等长字符串（如 MD5 值） 经常变更的数据，因为不易产生碎片 tinytext/tinyblob smalltext/smallblob text/blob mediumtext/mediumblob longtext/longblob （4）🌟时间类型 datetime：8 字节 timestamp 时间戳：不容易产生歧义。4 字节，从1970年1月1日午夜（格林尼治标准时间）以来的秒数，智能表示从1970年到2038年，默认为 NOT NULL 12.1.7 表设计原则 避免过度设计 使用小而简单的合适数据类型，尽量避免使用 NULL 值（除非必要） 尽量使用相同的数据类型存储相似或相关的值 注意可变长字符串 尽量使用整型定义标识列 小心使用 ENUM 和 SET，避免使用 BIT alter table 大部分情况下会锁表并重建整张表，可以在备机执行 alter 并在完成后把它切换为主库 ","date":"2023-08-08","objectID":"/mysql_draft/:0:2","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"MySQL 配置优化 连接请求的变量 max_connections back_log（缓存中尚未处理的连接数量） wait_timeout（非交互式连接）、 interactive_timeout（交互式连接） 缓冲区变量 key_buffer_size query_cache_size（查询缓存，简称QC） max_connect_errors sort_buffer_size max_allowed_packet = 32M join_buffer_size = 2M thread_cache_size = 300 Innodb 相关变量 innodb_buffer_pool_size innodb_flush_log_at_trx_commit innodb_thread_concurrency = 0 innodb_log_buffer_size innodb_log_file_size = 50M innodb_log_files_in_group = 3 read_buffer_size = 1M read_rnd_buffer_size = 16M bulk_insert_buffer_size = 64M binary log ","date":"2023-08-08","objectID":"/mysql_draft/:0:3","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"如何选择恰当数据类型？ 明确、尽量小。 char、varchar 的选择 varchar适用的场景： 字符串列的最大长度比平均长度要大很多； 字符串列的更新很少时，因为没有或很少有内存碎片问题； 使用了UTF-8这样复杂的字符集，每个字符都使用不同的字节数进行存储； char适用的场景： 列的长度为定值时适合适用，比如：MD5密文数据 text/blob/clob 的使用问题？ text 只能保存字符数据，比如一遍文章或日记； clob指代的是字符大对象，通常用来存储大量的文本数据，即存储字符数据； blob指代的是二进制大对象，用于存储二进制数据或文件，常常为图片或音频。 数值的精度问题？ decimal的存储结果没有精度丢失问题。因为decimal内部以字符形式存储小数，属于准确存储。而float和 double等则属于浮点数数字存储，所以没有办法做到准确，只能尽可能近似。 是否使用外键 外键提供的几种在更新和删除时的不同行为都可以帮助我们保证数据库中数据的一致性和引用合法性，但是外键的使用也需要数据库承担额外的开销，在大多数服务都可以水平扩容的今天，高并发场景中使用外键确实会影响服务的吞吐量上限。 如果不使用外键牺牲了数据库中数据的一致性，但是却能够减少数据库的负载；如果使用外键保证了数据库中数据的一致性，也将全部的计算任务全部交给了数据库。 ","date":"2023-08-08","objectID":"/mysql_draft/:0:4","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"数据库碎片问题？ 查看碎片是否产生： show table status from table_name\\G; 清理办法： MyISAM： optimize table 表名； # （OPTIMIZE 可以整理数据文件,并重排索引） Innodb： # 1. 重建表存储引擎，重新组织数据 ALTER TABLE tablename ENGINE=InnoDB; # 2. 进行一次数据的导入导出 ","date":"2023-08-08","objectID":"/mysql_draft/:0:5","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"OLTP 和 OLAP OLTP = online transaction processing 在线事务处理 OLAP = online analytic processing 在线分析处理 ETL = Extract-Transform-Load，提取-转换-加载 属性 OLTP OLAP 读 基于key，每次查询返回少量记录 对大量记录进行汇总 写 随机访问，低延迟写入 批量导入（ETL）或事件流 使用场景 终端用户，通过网络应用程序 内部分析师，支持决策 数据表征 最新的数据状态 随着时间变化的所有事件历史 数据规模 GB到TB TB到PB ","date":"2023-08-08","objectID":"/mysql_draft/:0:6","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"锁 加锁情况列举 select：一致性读，读快照，仅当 serializable 隔离级别才上临键锁或索引记录锁。 update：排他临键锁或索引记录锁。当修改聚集索引记录时，将对二级索引记录进行隐式锁定，对受影响的二级索引记录设置共享锁。 delete：排他临键锁或索引记录锁。 insert：插入意向间隙锁+对插入的行设置排他锁，仅索引记录锁，没有间隙锁，不会阻止其他会话插入新行到前面的间隙中。如果键重复，则在重复的索引记录上设置一个共享锁，遇到排他锁有死锁风险。 insert ... on duplicate key update：键重复时设置排他锁 表级锁（MyISAM）： intention lock：意向锁，表明当前事务稍后要对表中的行进行哪种类型的锁定 IX 锁：排他意向锁 IS 锁：共享意向锁 auto-inc lock：自增锁，innodb_autoin_lock_mode 可以控制是否打开自增锁 开启：保证自增序列 关闭：insert 操作高并发 X锁与任何锁都冲突，IX锁与S锁冲突，其他锁之间是兼容的。 锁类型的兼容性 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 行级锁（InnoDB）： 记录锁 record lock：始终锁定索引记录，即使没有定义索引的表也是如此。 间隙锁 gap lock： 对索引记录前后的间隙进行锁定。“纯抑制性的”，唯一目的是阻止其他事务在间隙中插入，不同事务可以在同一个间隙上持有冲突锁。 RC隔离级别会显式禁用间隙锁。在 WHERE 条件计算完成后, 会⽴即释放不匹配⾏的记录锁。 对于 UPDATE 语句, InnoDB执⾏“半⼀致性读(semi-consistent)”, 将最新的提交版本返回给MySQL, 以便MySQL确定该⾏是否与 UPDATE 的 WHERE 条件匹配。 间隙锁在搜索和索引扫描时会被禁⽤, 只⽤于外键约束检查和重复键检查。 临键锁 = 记录锁 + 间隙锁（记录之前的间隙）next-key lock 插入意向锁 insert intention lock 谓词锁：空间索引使用，对包含 MBR（minimum bounding rectangle, 最⼩边界矩形）的索引强制执⾏⼀致性读，其他事务不能插入或修改。 死锁： 循环等待 互斥 请求和保持 不可剥夺 减少死锁： 优先使用事务 让执行 insert 或 update 的事务足够小 为 update 相关的列创建索引，也可以使用explain select 来确定默认使用哪个索引 用查看最近发生的死锁：show engine innodb status 打印死锁相关信息：innodb_print_all_deadlocks 选项 如果经常发生死锁，使用 Read Committed 隔离级别，这样同一事务中的每次一致性读，都是从自己的新快照中读取 实在不行，使用 LOCK TABLES 语句，表级锁可以防止其他会话对这张表进行并发更新 SET autocommit=0; LOCK TABLES t1 WRITE, t2 READ; ... do something with tables t1 and t2 here ... COMMIT; UNLOCK TABLES; 禁用死锁检测：innodb_deadlock_detect 选项 超时事务回滚：innodb_lock_wait_timeout 选项 ","date":"2023-08-08","objectID":"/mysql_draft/:0:7","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"14.1 MySQL 主从复制 主从复制的局限性： 主从延迟问题 应用侧需要配合读写分离框架 不解决高可用问题 ","date":"2023-08-08","objectID":"/mysql_draft/:0:8","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"14.2 MySQL 高可用 14.2.1 高可用 更少的不可服务时间 1年 = 365天 = 8760小时 99 -\u003e 8760*1% = 87.6 小时 99.9 -\u003e 8.76小时 99.99 -\u003e 0.876 小时 = 52.6分钟 99.999 -\u003e 5.26分钟 14.2.2 MGR：MySQL Group Replication 特点： 高一致性：基于 Paxos 协议实现组复制 高容错性：自动检测机制，内置防脑裂保护机制 高扩展性：节点的增加与移除会自动更新组成员信息，新节点加入后，自动从其他节点同步增量数据，直到与其他节点数据一致 高灵活性：提供单主模式（宕机后能自动选主，在主节点进行写入）和多主模式（支持多节点写入） 搭配中间件： MySQL Shell：配置管理 InnoDB Cluster MySQL Router：提供负载均衡和应用连接的故障转移 orchestrator：高可用和复制拓扑管理工具，两种部署方式： 一致性由 raft 协议保证，数据库之间不通信 一致性由数据库集群保证，数据库结点之间通信 ","date":"2023-08-08","objectID":"/mysql_draft/:0:9","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"15.0 数据库拆分 分库分表的选择：如果数据本身的读写压力较大，磁盘IO成为瓶颈，选择分库，使用不同磁盘可以并行提升整个集群的并行数据处理能力；相反，选择分表，减少单表操作的时间。 单机 MySQL 的技术演进 读写压力：多机集群，主从复制 高可用性：故障转移：MHA（Master High Availability）/MGR/Orchestrator 容量问题：数据库拆分，分库分表 一致性问题：分布式事务，XA/柔性事务 全部数据：数据复制，主从复制、备份与高可用 业务分类数据：垂直分库分表，分布式服务化、微服务 任意数据：水平分库分表，分布式结构、任意扩容 ","date":"2023-08-08","objectID":"/mysql_draft/:0:10","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"15.1 垂直拆分 优点： 单表变小，便于管理和维护 提升性能 数据复杂度降低 作为微服务改造的基础 缺点： 库变多，管理复杂 对业务系统有较强的侵入性 改造过程复杂，容易出故障 拆分有限度 做法： 梳理拆分范围和影响范围 评估影响到的服务 准备新的数据库集群复制数据 修改系统配置，发布新版上线 ","date":"2023-08-08","objectID":"/mysql_draft/:0:11","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"15.2 水平拆分 优点： 解决容量问题 对系统影响小 部分提升性能和稳定性 缺点： 集群规模大，管理复杂 复杂SQL支持问题 数据迁移问题 一致性问题 数据的分类管理： 时间 标签 存储 一周内下单但未支付 热数据 数据库和内存 三个月内 温数据 数据库，提供查询操作 三个月到三年 冷数据 从数据库删除，归档到便宜的磁盘，用压缩方式（tokuDB引擎）存储，邮件查询 三年以上 冰数据 备份到磁盘介质，不提供任何查询操作 ","date":"2023-08-08","objectID":"/mysql_draft/:0:12","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"16.1 分布式事务 强一致：XA 弱一致： 不用事务，业务侧补偿冲正 柔性事务，保证最终一致性 ","date":"2023-08-08","objectID":"/mysql_draft/:0:13","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"16.2 XA 分布式事务 又叫两阶段事务（2PC） 主流开源XA解决方案 Atomikos narayana seata TM 去中心化，高性能 去中心化，高性能 中心化，低性能，bug多 日志存储 仅文件 文件，数据库 文件，数据库 扩展性 好 中 中 事务恢复 仅单机事务恢复 支持集群模式恢复 问题多 XA实现 标准 标准 非标准 ","date":"2023-08-08","objectID":"/mysql_draft/:0:14","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"16.3 BASE 柔性事务： 对于互联网应用而言，随着访问量和数据量的激增，传统的单体架构模式将无法满足业务的高速发展。这时，开发者需要把单体应用拆分为多个独立的小应用，把单个数据库按照分片规则拆分为多个库和多个表。 数据拆分后，如何在多个数据库节点间保证本地事务的ACID特性则成为一个技术难题，并且由此而衍生出了CAP和BASE经典理论。 CAP理论指出，对于分布式的应用而言，不可能同时满足C（一致性），A（可用性），P（分区容错性），由于网络分区是分布式应用的基本要素，因此开发者需要在C和A上做出平衡。 由于C和A互斥性，其权衡的结果就是BASE理论。 对于大部分的分布式应用而言，只要数据在规定的时间内达到最终一致性即可。我们可以把符合传统的ACID叫做刚性事务，把满足BASE理论的最终一致性事务叫做柔性事务。 一味的追求强一致性，并非最佳方案。对于分布式应用来说，刚柔并济是更加合理的设计方案，即在本地服务中采用强一致事务，在跨系统调用中采用最终一致性。如何权衡系统的性能与一致性，是十分考验架构师与开发者的设计功力的。 本地事务 -\u003e XA(2PC) -\u003e BASE 本地事务 2PC、3PC事务 柔性事务BASE 业务改造 - - 实现相关接口 一致性 不支持 支持 最终一致 隔离型 不支持 支持 业务方保证 并发性能 无影响 严重衰退 略微衰退 适合场景 业务方处理不一致 短事务\u0026低并发 长事务\u0026高并发 BASE： 基本可用（Basically Available）：分布式事务参与方不一定同时在线 柔性状态（Soft state）：允许系统更新有一定延时 最终一致性（Eventually consistent）：消息传递方式保证系统最终一致性 ACID 是在资源层面隔离事务，BASE 是在业务逻辑层面实现互斥。BASE通过放宽强一致性要求，来换取系统吞吐量的提升。 BASE 柔性事务常见模式： TCC：通过手动补偿处理 AT：通过自动补偿处理 ","date":"2023-08-08","objectID":"/mysql_draft/:0:15","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["draft"],"content":"16.4 TCC/AT 以及相关框架 TCC 模式：对业务的侵入强，改造的难度大。 Try 准备：尝试执行业务，完成所有业务检查（一致性），预留业务资源（准隔离性）； Confirm 确认：执行业务逻辑，只使用Try阶段预留的业务资源，操作满足幂等性，保证事务能且只能成功一次； Cancel 取消：释放 Try 阶段预留的业务资源，操作也满足幂等性。 TCC需要注意的几个问题： 允许空回滚 防悬挂控制 幂等设计 AT模式：两阶段提交，自动生成反向SQL ","date":"2023-08-08","objectID":"/mysql_draft/:0:16","tags":["MySQL"],"title":"MySQL 杂记","uri":"/mysql_draft/"},{"categories":["practice"],"content":"基操要练6","date":"2023-08-08","objectID":"/mysql-50-%E9%A2%98/","tags":["MySQL"],"title":"MySQL 50 题","uri":"/mysql-50-%E9%A2%98/"},{"categories":["practice"],"content":"数据源 -- 创建数据库exercise create database exercise; -- 使用数据库exercise use exercise; -- 创建学生表student create table student (Sno varchar(10) not null, Sname varchar(10) , Sage date , Ssex varchar(10) , primary key (Sno)); start transaction; insert into student values ('01', '赵雷', '1990-01-01', '男'); insert into student values ('02', '钱电', '1990-12-21', '男'); insert into student values ('03', '孙风', '1990-05-20', '男'); insert into student values ('04', '李云', '1990-08-06', '男'); insert into student values ('05', '周梅', '1991-12-01', '女'); insert into student values ('06', '吴兰', '1992-03-01', '女'); insert into student values ('07', '郑竹', '1989-07-01', '女'); insert into student values ('08', '王菊', '1990-01-20', '女'); commit; -- 创建科目表course create table course (Cno varchar(10) not null, Cname varchar(10) , Tno varchar(10) , primary key (Cno)); start transaction; insert into course values ('01', '语文', '02'); insert into course values ('02', '数学', '01'); insert into course values ('03', '英语', '03'); commit; -- 创建教师表teacher create table teacher (Tno varchar(10) not null, Tname varchar(10) , primary key (Tno)); strat transaction; insert into teacher values ('01', '张三'); insert into teacher values ('02', '李四'); insert into teacher values ('03', '王五'); commit; -- 创建成绩表 sc create table sc (Sno varchar (10) , Cno varchar (10) , score decimal(18,1), primary key (Sno, Cno)); start transaction; insert into SC values('01' , '01' , 80); insert into SC values('01' , '02' , 90); insert into SC values('01' , '03' , 99); insert into SC values('02' , '01' , 70); insert into SC values('02' , '02' , 60); insert into SC values('02' , '03' , 80); insert into SC values('03' , '01' , 80); insert into SC values('03' , '02' , 80); insert into SC values('03' , '03' , 80); insert into SC values('04' , '01' , 50); insert into SC values('04' , '02' , 30); insert into SC values('04' , '03' , 20); insert into SC values('05' , '01' , 76); insert into SC values('05' , '02' , 87); insert into SC values('06' , '01' , 31); insert into SC values('06' , '03' , 34); insert into SC values('07' , '02' , 89); insert into SC values('07' , '03' , 98); commit; ","date":"2023-08-08","objectID":"/mysql-50-%E9%A2%98/:0:1","tags":["MySQL"],"title":"MySQL 50 题","uri":"/mysql-50-%E9%A2%98/"},{"categories":["practice"],"content":"问题 查询课程编号 01 比 02 高的学生的学号和姓名 查询所有学生的学号、姓名、选课数和总成绩 查询[没]学过’张三’老师课的学生的学号、姓名 查询学过’张三’老师[所有课]的学生的学号、姓名 查询没有学全所有课的学生的学号、姓名 查询和'01’号同学所学课程完全相同的其他同学的学号 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩 查询各科成绩最高分、最低分、平均分，展示：课程号、课程名、最高分、最低分、平均分、及格率、中等率、优良率、优秀率 按各科成绩排序，并显示排名 80 85 85 90 查询所有课程的成绩第 2 名到第 3 名的学生信息和该课程成绩 使用分段(100-85,85-70,70-60,\u003c60)来统计各科成绩，分别统计各分段人数，课程 id 和课程名称 查询学生的平均成绩及其名次 查询出只有两门课程的全部学生的学号和姓名 查询男女生人数 查询 1990 年出生的学生名单 查询所有学生的课程和分数情况 查询选修张三老师所教课程的学生中最高的学生的姓名和成绩 查询所有课程成绩相同的学生的学号、课程号、成绩 查询选修了全部课程的学生信息 查询各学生的年龄 查询下周过生日的学生 查询下个月过生日的学生 ","date":"2023-08-08","objectID":"/mysql-50-%E9%A2%98/:0:2","tags":["MySQL"],"title":"MySQL 50 题","uri":"/mysql-50-%E9%A2%98/"},{"categories":["practice"],"content":"答案 -- 1. 查询课程编号 01 比 02 高的学生的学号和姓名 select a.s_id '学号', c.s_name '姓名' from (select s_id, s_score 's01' from Score where c_id = '01') a inner join (select s_id, s_score 's02' from Score where c_id = '02') b on a.s_id = b.s_id inner join Student as c on c.s_id = a.s_id where s01 \u003e s02; -- 3. 查询所有学生的学号、姓名、选课数和总成绩 select a.s_id '学号', a.s_name '姓名', sum(ifnull(b.s_score, 0)) '总成绩', count(b.c_id) '选课数' from Student as a left join Score b on a.s_id = b.s_id group by a.s_id, a.s_name; -- 5. 查询[没]学过'张三'老师课的学生的学号、姓名 select s_id '学号', s_name '姓名' from Student where s_id not in (select s_id from Score where c_id = (select c_id from Course where t_id = (select t_id from Teacher where t_name = '张三'))); select s_id '学号', s_name '姓名' from Student where s_id not in (select s_id from Score as s inner join Course as c on s.c_id = c.c_id inner join Teacher t on c.t_id = t.t_id where t.t_name = '张三'); -- 6. 查询学过'张三'老师[所有课]的学生的学号、姓名 select s.s_id '学号', s.s_name '姓名' from Student as s inner join Score s2 on s.s_id = s2.s_id inner join Course c on s2.c_id = c.c_id inner join Teacher t on c.t_id = t.t_id where t.t_name = '张三' group by s.s_id having count(*) = (select count(*) from Course c inner join Teacher t on c.t_id = t.t_id where t_name = '张三' group by t.t_id, t.t_name); -- 10. 查询没有学全所有课的学生的学号、姓名 select s.s_id '学号', s.s_name '姓名' from Student as s left join Score s2 on s.s_id = s2.s_id group by s.s_id having count(distinct c_id) \u003c (select count(distinct c_id) from Course); -- 12. 查询和'01'号同学所学课程完全相同的其他同学的学号 select distinct s.s_id '学号', s.s_name '姓名' from Student as s inner join Score s2 on s.s_id = s2.s_id -- 去除选了 01 学生所学课程之外课程的学生 where s2.s_id not in (select s_id from Score where c_id not in (select c_id from Score where s_id = '01')) and s.s_id != '01' group by s.s_id, s.s_name having count(distinct c_id) = (select count(*) from Score where s_id = '01'); -- 17. 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩 select distinct s.s_id '学号', st.s_name '姓名', c_01_score '语文', c_02_score '数学', c_03_score '英语', c_04_score '奥数', avg_score '平均成绩' from Score s inner join (select s_id, max(case when c_id = '01' then s_score end) 'c_01_score', max(case when c_id = '02' then s_score end) 'c_02_score', max(case when c_id = '03' then s_score end) 'c_03_score', max(case when c_id = '04' then s_score end) 'c_04_score', avg(s_score) 'avg_score' from Score group by s_id) s2 on s.s_id = s2.s_id inner join Student st on s.s_id = st.s_id order by avg_score desc; -- 18. 查询各科成绩最高分、最低分、平均分，展示：课程号、课程名、最高分、最低分、平均分、及格率、中等率、优良率、优秀率 select sc.c_id '课程号', c.c_name '课程名', max(sc.s_score) 'max', min(sc.s_score) 'min', avg(sc.s_score) 'avg', sum(IF(sc.s_score \u003e= 60, 1, 0)) / count(sc.s_id) '及格率', sum(IF(sc.s_score \u003e= 70 and sc.s_score \u003c 80, 1, 0)) / count(sc.s_id) '中等率', sum(IF(sc.s_score \u003e= 80 and sc.s_score \u003c 90, 1, 0)) / count(sc.s_id) '优良率', sum(IF(sc.s_score \u003e= 90, 1, 0)) / count(sc.s_id) '优秀率' from Score sc inner join Course c on sc.c_id = c.c_id group by sc.c_id; -- 19. 按各科成绩排序，并显示排名 80 85 85 90 -- MySQL 8.0 窗口函数：不会改变记录条数 -- row_number() 排序不会重复 1 2 3 4 没有并列 -- dense_rank() 有并列，1 2 2 3 最后一名不一定等于总人数 -- rank() 有并列， 1 2 2 4 月考排名 select *, rank() over (partition by c_id order by s_score desc) 'rank', dense_rank() over (partition by c_id order by s_score desc) 'dense_rank', row_number() over (partition by c_id order by s_score desc) 'row_number' from Score; # where c_id ='01'; -- 测试最左匹配原则 CREATE INDEX idx_test2 ON Student (s_birth, s_name, s_sex); explain select s_name from Student where s_birth = '12345' # and s_name = '赵雷' and s_sex = '男'; -- 22. 查询所有课程的成绩第 2 名到第 3 名的学生信息和该课程成绩 -- row_number() 排序不会重复 1 2 3 4 没有并列 (见 19 题) select * from (select sc.c_id '课程号', C.c_name '课程', sc.s_score '分数', row_number() over (partition by sc.c_id order by s_score desc) 'sort', st.* from Score sc inner join Student st on sc.s_id = st.s_id join Course C on sc.c_id = C.c_id) a where sort in (2, 3); -- 23. 使用分段(100-85,85-70,70-60,\u003c60)来统计各科成绩，分别统计各分段人数，课程 id 和课程名称 select c.c_id '课程 id', c.c_name '课程名称', sum(IF(sc.s_score \u003c= 100 and sc.s_score \u003e 85, 1, 0)) '100-85', sum(IF(sc.s_score \u003c= 85 and sc.s_score \u003e 70, 1, 0)) '85-70', sum(IF(sc.s_score \u003c= 70 and sc.s_score \u003e 60, 1, 0)) '70-60', sum(IF(sc.s_score \u003c 60, 1, 0)) '\u003c60' from score as sc inner join Course c on sc.c_id = c.c_id group by c.c_id, c.c_name; -- 24. 查询学生的平均成绩及其名次 -- 可以没有 partition by select s.s_id '学号', st.s_name '姓名', avg(s_score) '平均分', rank() over (order by avg(s.s_score) desc) 'rank' from Score s join Student st on s.s_id = st.s_id group by s.s_id; -- 27. 查询出只有两门课程的全部学生的学号和姓名 select s_id '学号', s_name '姓名' from Student where s_id in (select s.s_id from Score s group by s.s_id having count(distinct s.c_id) = 2); -- 28. 查询男女生人数 -- 两行 select s_sex, count(s_sex) from Student group by s_sex; -- 列转行，同一行展示 select sum(if(s_sex = '男', 1, 0)) '男生数', sum(if(s_sex = '女', 1, 0)) '女生数' from Student; -- 31. 查询 1990 年出生的学生名单 -- 日期 varchar 存储时（不推荐） select * from Student where s_birth like '1990%'; -- year() 函数，兼容多种格式: -- YYYY-MM-DD -- YYYYMMDD -- YYYY/MM/DD -- YYMMDD select * from Student where year(s_birth) = 1990; -- 35. 查询所有学生的课程和分数情况 select s.s_id '学号', s2.s_name '姓名', max(if(c.c_name = '数学', s.s_score, null)) '数学成绩', max(if(c.c_name = '语文', s.s_score, null)) '语文成绩', max(if(c.c_name = '英语', s.s_score, null)) '英语成绩', max(if(c.c_name = '奥数', s.s_score, null)) '奥数成绩' from Score s inner join Course c on s.c_id = c.c_id inner join Student s2 on s.s_id = s2.s_id group by s.s_id, s2.s_name; -- 40. 查询选修张三老师所教课程的学生中最高的学生的姓名和成绩 explain select * from (select sc.c_id '课程号', c.c_name '课程名', sc.s_score '成绩', st.s_id '学号', st.s_name '姓名', rank() over (partition by sc.c_id order by s_score desc) 'rank' from Score sc inner join Course c on sc.c_id = c.c_id inner join Student st on sc.s_id = st.s_id inner join Teacher t on c.t_id = t.t_id where t.t_name = '张三') a where `rank` = 1; -- 41. 查询所有课程成绩相同的学生的学号、课程号、成绩 select s_id '学号', c_id '课程号', s_score '成绩' from Score where s_id = (select a.s_id as_id from (select s_id, s_score, count(*) count from Score group by s_id, s_score) a where count \u003e 1); -- 45. 查询选修了全部课程的学生信息 select * from Student where s_id = (select s_id from Score group by s_id having count(distinct c_id) = (select count(*) from Course)); -- 46. 查询各学生的年龄 -- datediff（a,b）两个时间相差几天 -- round 四舍五入取整 -- floor 向下取整 select s_id, s_name, s_birth, floor(DATEDIFF(now(), s_birth) / 365) from Student; -- 48. 查询下周过生日的学生 select *, week('1990-05-20', 1), week('2019-05-20', 1), concat(year(now()), substring(s_birth, 5, 6)) from Student -- week函数对于不同年份会有一点问题 # where week(s_birth,1) = week(now(),1)+1 where week(now(), 1) + 1 = week(concat(year(now()), substring(s_birth, 5, 6)), 1); -- 50. 查询下个月过生日的学生 select * from Student # where month(s_birth) = if(month(now()) = 12, 1, month(now()) + 1) where month(s_birth) = month(date_add(now(),interval 1 month )); ","date":"2023-08-08","objectID":"/mysql-50-%E9%A2%98/:0:3","tags":["MySQL"],"title":"MySQL 50 题","uri":"/mysql-50-%E9%A2%98/"},{"categories":["interview"],"content":"是用得最多的数据库了","date":"2023-08-08","objectID":"/mysql/","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"MySQL 中的锁 InnoDB 引擎加锁的本质是锁住索引记录（可以理解为 B+树的叶子节点）。在可重复读 RR 的隔离级别下， 一般都加临键锁，左开右闭 （防止幻读） 上界无穷间隙锁 唯一索引等值查询记录锁 ","date":"2023-08-08","objectID":"/mysql/:0:1","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"Innodb 和 MyISAM 的区别？ innodb 支持事务和外键 innodb 默认表锁，使用索引检索条件时是行锁，而 myisam 是表锁（每次更新增加删除都会锁住表） innodb 和 myisam 的索引都是基于 b+树，但 innodb 的 b+树的叶子节点是存放数据的，myisam 的 b+树的叶子节点是存放指针的 innodb 是聚簇索引，必须要有主键，一定会基于主键查询，但是辅助索引就会查询两次，myisam 是非聚簇索引，索引和数据是分离的，索引里保存的是数据地址的指针，主键索引和辅助索引是分开的。 innodb 不存储表的行数，所以 select count( _ )的时候会全表查询，而 myisam 会存放表的行数，select count(_）的时候会查的很快。 MyISAM 存储限制 256TB，更适合低并发、弱一致性场景；Innodb 存储限制 64TB，更适合高并发、更新频繁的场景。 ","date":"2023-08-08","objectID":"/mysql/:0:2","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"结构化查询语言的 6 个部分 DDL：Data Define Language，数据定义语言，如 CREATE、ALTER、DROP DML：Data Manipulation Language，数据操作语言，如 INSERT、UPDATE、DELETE DQL：Data Query Language，数据查询语言，如 SELECT、WHERE、ORDER BY、GROUP BY、HAVING TCL：Transactional Control Language，事务控制语言，确保 DML 影响的所有行及时得以更新，如 COMMIT、SAVEPOINT、ROLLBACK DCL：Data Control Language，数据控制语言，实现权限控制，如 GRANT、REVOKE CCL：Cursor Control Language，指针控制语言，如 DECLARE CURSOR、FETCH INTO、UPDATE WHERE CURRENT ","date":"2023-08-08","objectID":"/mysql/:0:3","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"binlog 、redolog 和 undolog 的区别？ binlog： 所有引擎增量记录逻辑日志（二进制） 数据库还原 主从同步（Canal 类中间件本质是把自己伪装成从节点）： 主库提交，将 DDL 与 DML 数据写入 binlog； 从库将主库的 binlog 写入自己的中继日志 relay log 从库执行 relay log 中的事件 sync_binlog = 0：写入 page cache 就算成功（默认） sync_binlog = N：每提交 N 次就刷新到磁盘，N 越小性能越差。 redolog：InnoDB 循环记录物理日志（某个页的修改），大小固定，写到结尾时，会回到开头循环写日志，保证了持久性。 先更新 buffer pool，再写 redo log，等事务结束后刷盘到磁盘 顺序写，性能优于随机写 undolog： 记录逻辑相反操作逻辑日志，用于回滚 delete：记录主键逻辑删除 insert：记录主键 update 没有更新主键：主键+原值 更新主键：delete+insert 事务执行过程 宕机情形： 在 redo log 刷新到磁盘（事务提交）之前，都是回滚 undo log。 如果在 redo log 已经刷新到磁盘，在 MySQL 重启之后就会回放这个 redo log，以纠正数据库里的数据。 ","date":"2023-08-08","objectID":"/mysql/:0:4","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"分库分表 - sharding-sphere 中间件 水平分库：应对大数据，对性能要求不同的数据进行分库 sync_binlog = 0/1 垂直分库：根据业务进行拆分微服务 垂直分表：冷热数据分离（零零播项目中的 user、streamer、owner） ","date":"2023-08-08","objectID":"/mysql/:0:5","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"索引分类 聚簇/非聚簇：叶子节点是否存储行数据 覆盖索引：索引包含某次查询的所有列 唯一索引 unique：可以为空，但最多一个空 主键索引 primary key：不能为空 前缀索引：可以选择在 varchar(128)的列上选择前 32 个字符作为索引 联合索引：最左匹配原则 全文索引 full text ：支持文本模糊查询用于 char、varchar、text 哈希索引：Innodb 和 MyISAM 都不适用 ","date":"2023-08-08","objectID":"/mysql/:0:6","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"如何使用慢 SQL 优化？ # 开启慢查询日志 SET GLOBAL slow_query_log = 'ON'; # 蛮查询日志存放位置 SET GLOBAL slow_query_log_file = 'var/lib/mysql/slow_query.log' # 未被索引的记录 SET GLOBAL log_queries_not_using_indexes = 'ON'; # 慢查询阈值（秒） SET SESSION long_query_time = 1; # 慢查询记录扫描行数阈值 SET SESSION min_examined_row_limit = 100; 找到慢 SQL 之后，用 EXPLAIN 命令分析： possible_key：可能用到的索引 key：实际命中的索引 key_len：索引占用的大小 rows：扫描的行数 filtered：所需数据占 rows 的比例 extra：额外的优化建议 Using where；Using Index 用到索引，不用回表 Using index** condition** 用到索引，需要回表 type：性能从好到差 system：mysql 自带的表 const：根据主键查询 eq_ref：主键索引/唯一索引 ref：索引查询 range：范围查询 index：索引树扫描 all：全盘扫描 选择索引列 数据量大、查询频繁的表，涉及 where、orderby、groupby 频繁的字段 区分度高的字段建立唯一索引或放在左边 字符串类型优先使用前缀索引，占用空间更小 not null 约束 不选择索引情况 被频繁更新的字段 长期未使用的索引：sys 库的schema_unused_indexes视图 ","date":"2023-08-08","objectID":"/mysql/:0:7","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"索引失效情况？ 没有覆盖索引，如 select * 联合索引未遵循最左前缀原则 Mysql 从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分 例如索引是 key index （a,b,c）。 可以支持 a 、a,b 、a,b,c 3 种组合进行查找，但不支持 b,c 进行查找。 范围查询右边的列会失效，范围过大也会导致全表扫描 在索引列上运算操作 如果查询中必须使用函数或表达式，而且这些函数或表达式无法使用索引加速查询，那么可以考虑使用计算列（Computed Column）来加速查询。计算列是一种特殊的列，它的值是通过计算得到的，而不是直接存储在表中。在 MySQL 中，可以使用虚拟列（Virtual Column）或存储列（Stored Column）来实现计算列。虚拟列是一种只存在于查询结果中的计算列，它不会在表中存储任何数据。虚拟列可以使用表达式来计算，例如：ALTER TABLE table ADD COLUMN virtual_col INT AS (YEAR(date_col)) VIRTUAL; like ‘%abc’ or 的前后条件中有一个列没有索引，涉及的索引也失效 in()查询时，应该尽可能减少值列表的长度，避免查询转化成过长的 OR 子句，以及避免在查询中使用 NULL 值。 ","date":"2023-08-08","objectID":"/mysql/:0:8","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"🌟 什么是聚簇索引？ 每个表在 MySQL 中对应着一个聚簇索引和多个辅助索引，每个索引都对应一个独立的 B+ 树。每个 B+ 树索引都包含了相应索引类型的所有索引键值，但并不是一个单独的 B+ 树索引包含了所有的索引键值。所以，可以说在 MySQL 中，每个表对应多棵 B+ 树索引。 聚簇索引：叶子节点保存了行数据，有且只有一个聚簇索引。 主键 pk \u003e 唯一索引 unique \u003e 自动生成 row_id 非聚簇索引：叶子节点关联主键，指向了数据的对应行，若没有覆盖索引则需要回表走聚簇索引。 聚簇索引的优点： 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了 Buffer 中，再次访问的时候，会在内存中完成访问，不必访问磁盘。这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键 Id 来组织数据，获得数据更快。 辅助索引使用主键作为\"指针\"而不是使用地址值作为指针的好处是，减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是 InnoDB 在移动行时无须更新辅助索引中的这个\"指针\"。也就是说行的位置（实现中通过 16K 的 Page 来定位）会随着数据库里数据的修改而发生变化（前面的 B+树节点分裂以及 Page 的分裂），使用聚簇索引就可以保证不管这个主键 B+树的节点如何变化，辅助索引树都不受影响。 聚簇索引适合用在排序的场合，非聚簇索引不适合 取出一定范围数据的时候，使用用聚簇索引 二级索引需要两次索引查找，而不是一次才能取到数据，因为存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据 可以把相关数据保存在一起。例如实现电子邮箱时，可以根据用户 ID 来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每封邮件都可能导致一次磁盘 I/O。 ","date":"2023-08-08","objectID":"/mysql/:0:9","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"🌟SQL 优化经验？ 避免 select *，主查询聚簇索引，子查询覆盖索引 **用 union all **代替 union（去重、排序消耗性能） where 语句中不使用表达式，防止索引失效 能用 inner join（优先以小表驱动大表） 就不用 left/right join。 将 HAVING 中部分条件提前到 WHERE 里 用 EXPLAIN 返回的预估行数 SELECT rows FROM (EXPLAIN SELECT * FROM xxx WHERE uid = 123) a; 将 RR 降低为 RC，避免临键锁引发死锁，提高性能。 可以通过缓存第一次数据，来避免第二次查询，来保证几乎全部事务内部对某一数据都只读取一次 可以单独指定 Session 或者事务的隔离级别。 ","date":"2023-08-08","objectID":"/mysql/:0:10","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"🌟 并发事务问题 隔离级别 脏读：读到其他事务未提交的数据 不可重复读：同一事务内，先后读取的数据不同，因为其他事务 update 或 delete 会影响结果集。 幻读：同一事务内，先后读取的 count 不同。例如，查询时没有某 id，插入时 id 已存在。 原因：加锁后，不锁定间隙，其他事务可以 INSERT，导致相同的查询在不同的时间得到不同的结果。 解决方案：使用临键锁，对于使用了唯一索引等唯一查询条件，InnoDB 只锁定索引记录，不锁定间隙；对于其他查询条件，InnoDB 会锁定扫描到的索引范围，通过临键锁来阻止其他会话在这个范围中插入新值。 RU 读未提交：脏读、不可重复读、幻读都会出现 RC 读已提交：解决脏读，每一次快照读时生成** ReadView 读取自己的新快照** 多数数据库默认 仅支持基于行的 bin log 当一个事务启动时，数据库会为该事务创建一个版本链，该版本链包含了数据库中所有数据对象的所有版本。当事务读取数据时，数据库会根据事务的启动时间戳选择相应的版本，并且在版本链中添加一个新的版本，以保证该事务所读取的数据不会被其他事务修改。 RR 可重复读：解决不可重复读，仅第一次生成 ReadView 的活跃事务 m_ids，后续复用（一致性快照） 聚簇索引更新 = 替换，非聚簇索引更新 = 删除+新建 MVCC = multiple version concurrent control 多版本并发控制 目的：高并发场景下，锁的性能差，MVCC 可以避免读写阻塞 DB_ROW_ID：隐藏列 DB_ROLL_PIR：回滚指针，指向这条记录的上一版本 版本链存储在 undo log 中 DB_TRX_ID：最近修改的事务 id 理论上 RR 存在幻读，但 MySQL 临键锁 next key lock 解决了幻读问题 问：如何保证 REPEATABLE READ 级别绝对不产⽣幻读？ 答：在 SQL 中加⼊** for update (排他锁) 或 lock in share mode (共享锁)**语句实现。就是锁住了可能造成幻读的数据，阻⽌数据的写⼊操作。 Serializable 串行化 加锁读：在每一行数据上都加锁，导致大量的超时和锁争用的问题，只有在非常需要确保数据一致性并且可以接受没有并发的情况下，才考虑采用该级别 事务特性 Atomic 原子性 一个事务要么全部提交，要么全部回滚 通过 undo log 记录逻辑日志，回滚时通过逆操作来恢复 保存位置：system tablespace(mysql5.7) 或 undo tablespaces(mysql8.0) Consistent 一致性 事务总是由一种状态转换为另一种状态，而不会停留在执行中的某个状态 A、I、D 同时作用，保证了 C（一致性） Isolate 隔离性 读写隔离：MVCC，undo log 版本链（从最新记录依次指向最旧记录的链表），ReadView 在 RC 和 RR 的不同 写写隔离：锁 Ddurable 持久性 事务只要提交了，那么数据库中的数据也永久的发生了变化 通过 redo log 增量记录数据页的物理变化，服务宕机时用来同步数据，先写日志，再写磁盘 日志文件：【ib_logfile0】【ib_logfile1】 日志缓冲：innodb_logh_buffer_size 强刷：fsync() ","date":"2023-08-08","objectID":"/mysql/:0:11","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"delete/truncate/drop 的区别？ DELETE 删除表中的行，不删除表本身 项目中一般使用逻辑删除，增加 deleted 字段 TRUNCATE 删除表，再创建一个空表 初始化数据 DROP 删除表 ","date":"2023-08-08","objectID":"/mysql/:0:12","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"数据迁移 mysqldump -u root -p db_name \u003e 'Desktop/test.sql' 加快导入速度 关闭唯一性检查 开启 extended-insert 将多行合并为一个 INSERT 语句 关闭 binlog，迁移后再开启 调整 redolog 时机 数据校验 先读 binlog，不一致再读源表，以源表为准。 先读从库，不一致再读主库，以主库为准。 增量同步数据 时间戳 binlog ","date":"2023-08-08","objectID":"/mysql/:0:13","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"难点 Situation：出现离职人员被企业微信机器人 at 的问题 Action-Wrong：在触发机器人消息的时候查询人员最新状态是否离职，离职则不 at【治标不治本】 Task：用户登录以后需要更新几个字段的数据，如果存在则更新，不存在则插入，最新数据来自于其他部门员工账户中心的微服务，（经过适配器设计模式，有些字段需要转化，有些字段直接复制，比如离职时间为 null 代表未离职，更新为 0），信息需要通过用户 id 查询，一个接口只能查询 100 条。 Action-Wrong：会有死锁的可能性：并发+业务耗时 = 两个临键锁发生死锁 BEGIN; // for update 为悲观锁 SELECT * FROM biz WHERE id = ? FOR UPDATE // 中间有很多业务操作 INSERT INTO biz(id, data) VALUE(?, ?); COMMIT; Action： ✓ 不管有没有数据，直接插入，规避死锁。 调整隔离级别为 RC（影响太大） 乐观锁（数据库的 CAS 操作，引入 version 列，或者使用 update_time 来确保数据没有发生变更） Result 解决了问题，同时杜绝了死锁的发生，如果使用乐观锁还能提升性能。 ","date":"2023-08-08","objectID":"/mysql/:0:14","tags":["MySQL"],"title":"🚩MySQL","uri":"/mysql/"},{"categories":["interview"],"content":"五种基础数据结构？ string：最大存储512M hash：hget、hmget、hset、hmset list 可以作为队列，rpush 生产消息，lpop消费消息，当lpop没有消息时，要适当sleep一会再重试，或者使用blpop，它在没有消息的时候会阻塞住直到消息到来。 如果想生产一次消费多次，就使用 pub/sub 主题订阅者模式，可以实现1：N的消息队列。但是消费者下线的情况下，生产的消息会丢失，就需要采用更专业的MQ，如RabbitMQ。 set sorted set 每个成员会有一个分数与之关联 可以实现延时队列。拿时间戳作为 score，消息内容作为key调用 zadd来生产消息，消费者用 zrangebyscore获取N秒之前的数据轮询进行处理。 ","date":"2023-08-08","objectID":"/redis/:0:1","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"🌟如何保证MySQL和Redis的数据一致性？ 强一致性方案-分布式锁 先拿 setnx来争抢锁，抢到之后，用expire给锁加一个过期时间防止忘记释放引发死锁。set指令有复杂的参数，是把setnx和expire合成一条指令来用。 任意时刻只能有一个客户端持有锁，**加锁和解锁必须是同一个客户端，客户端自己加的锁只能自己去解。**只要大多数 Redis 节点正常，客户端就能正常使用锁。 底层 setnx 通过 lua 脚本实现，保证原子性。 redisson 实现的分布式锁对同一线程可重入 RedLock 红锁：在多个实例上创建锁，性能差，违背了 Redis 的 AP 思想，不如使用 zookeeper 实现强一致性的 CP 系统 WatchDog 看门狗：给持有锁的线程续期（默认 10s） 高可用方案-异步通知：将binlog日志采集发送到MQ队列里面，然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性。 ","date":"2023-08-08","objectID":"/redis/:0:2","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"两种持久化方式？ RDB（Redis DataBase）：用数据集快照的方式半持久化（高效，持久化之间记录会丢失） AOF（Append-Only File）：所有的命令行记录以 redis 命令请求协议的格式完全持久化存储保存为 aof 文件（文件大、安全） 备份： save可以在 redis 数据目录生成数据文件 dump.rdb bgsave 用子线程异步执行，“save 300 10” 表示 300s 内有 10 个 key 被修改就执行 bgsave appendonly 默认关闭，若配置为 yes，则以AOF方式备份数据，在特定的时候执行追加命令 appendfsync everysec 每秒刷盘 bgrewriteaof 重写优化 aof 文件 恢复：重启服务，自动加载redis数据目录的 dump.rdb ","date":"2023-08-08","objectID":"/redis/:0:3","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"过期key的删除策略？ 两者配合使用： 定期：读取配置server.hz的值，默认为10 惰性：get key时，如果过期就删除，如果没过期就返回 ","date":"2023-08-08","objectID":"/redis/:0:4","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"哨兵Sentinel - 高可用 通过心跳机制，每秒发送 ping 命令，检查 master 和 slave 是否按期工作，在 master 故障时，Sentinel 将一个 slave 提升为 master。 自动选主 采用 Raft 共识算法，order by： 断线时间越低 优先级越高 slave-priority offset 越大 运行 id 越小 Sentinel（哨兵）不提供读写服务，默认运行在 26379 端口，建议多个 sentinel 节点（参数：quorum）协作运行，通过投票的方式来避免误判： 单个未响应，主观下线 多个未响应，客观下线 脑裂 Sentinel 与 master 失联，选出了新的 master’，于是覆盖了旧 master 用户写入的数据。 解决方案： min-replicas-to-write = 1：至少 1 个** slave 成功写入** min-replicas-max-lay = 5：同步延迟不超过 5s ","date":"2023-08-08","objectID":"/redis/:0:5","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"回收（淘汰）策略？如何保证Redis里存储的都是热点数据？ no-enviction：不淘汰，满了就报错（默认） volatile-：从已过期的数据集（server.db[i].expires）中挑选 ttl：剩余 ttl 越短的优先淘汰 random：随机 lru：最近最少使用【业务数据有冷热区分，且有置顶需求】 lfu：最少频率使用 allkeys-：针对全体 random【无明显冷热数据区分时】 lru【业务数据有冷热区分】 lfu【短时高频】 ","date":"2023-08-08","objectID":"/redis/:0:6","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"🌟什么是穿透、击穿与雪崩？ 穿透 Q：大量并发查询不存在的 key，导致压力传到数据库。 A：让缓存能够区分 key 不存在和查询到一个空值 缓存空值的（无效的） key，这样第一次不存在也会被加载并记录，下次拿到有这个key； Bloom过滤【判断一个元素是否在一个集合中】或RoaringBitmap 判断 key 是否存在；（RoaringBitMap和bloom filter本质上都是使用bitmap进行存储【bitmap的底层数据结构就是0/1bit 的二进制数值，最大长度512Mb = 512 _ 1024 _ 1024 * 8 = 2^32】。但bloom filter 使用的是多个hash函数对存储数据进行映射存储，如果两个游戏appId经过hash映射后得出的数据一致，则判定两者重复，这中间有一定的误判率，所以为满足在该业务场景其空间开销会非常的大。而RoaringBitMap是直接将元数据进行存储压缩，其准确率是100%的） 完全以缓存为准，使用延迟异步加载。 击穿 Q：某个key失效的时候，正好有大量并发请求访问这个key A：key的更新操作添加全局互斥锁（强一致性，只有第一个请求拿到数据） 或 使用延迟异步加载（高可用），或watchdog逻辑过期。 雪崩 Q：某一时刻发生大规模的缓存失效的情况，会有大量的请求直接打到数据库，导致压力过大甚至宕机。 A： 分散请求 热数据分散到不同机器上； 限流 控制过期时间：范围内随机； 多台机器做主从复制或多副本，实现高可用； 击穿和穿透的本质区别是什么？ 穿透：查询的** key 不在缓存中**，缓存没有起到缓冲的效果 击穿：查询的 key** 曾经有效**，在失效的瞬时打击了数据库 ","date":"2023-08-08","objectID":"/redis/:0:7","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"Redis 集群 - 主从复制？ 概念 主节点写，从节点读。 RDB同步有两个参数： replication id：标记数据集 offset：slave \u003c master 时需要更新 全量同步：主节点执行 **bgsave 生成 dump.rdb **文件给 slave 增量同步：slave 重启时，获取相同 replication id 的 offset 之后的数据 主从复制不要使用网状结构，使用单向链表结构，如 Master \u003c- slave1 \u003c- slave2 \u003c- slave3，更方便解决单点故障问题。 Redis 集群没有使用一致性 hash，而是引入了哈希槽概念，有 16384（2的14次）个哈希槽，每个key通过**CRC16（上限 2 的 16 次）**校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 使用 使用 Docker Compose 工具来创建 Redis 集群容器 version: '3' services: redis-7000: image: redis command: redis-server /usr/local/etc/redis/redis.conf ports: - \"7000:6379\" volumes: - ./conf/redis-7000.conf:/usr/local/etc/redis/redis.conf networks: - redis-net redis-7001: image: redis command: redis-server /usr/local/etc/redis/redis.conf ports: - \"7001:6379\" volumes: - ./conf/redis-7001.conf:/usr/local/etc/redis/redis.conf networks: - redis-net redis-7002: image: redis command: redis-server /usr/local/etc/redis/redis.conf ports: - \"7002:6379\" volumes: - ./conf/redis-7002.conf:/usr/local/etc/redis/redis.conf networks: - redis-net networks: redis-net: 该配置文件定义了三个 Redis 节点，每个节点监听不同的端口，使用不同的配置文件。可以使用 Docker Compose 工具来启动 Redis 集群容器： docker-compose up -d ","date":"2023-08-08","objectID":"/redis/:0:8","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"Redis 集群与高可用 Redis Sentinal 着眼于高可用，在 master 宕机时会自动将 slave 提升位 master，继续提供服务； Redis Cluster 着眼于扩展性，在单个 redis 内存不足时，使用 Cluster 进行分片存储。 # 从节点只读、异步复制 SLAVEOF 127.0.0.1 6379 # 主从切换基于 raft 协议,两种启动方式 redis-sentinel sentinel.conf redis-server redis.conf --sentinel sentinel.conf 配置： sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 # 数据分片 cluster-enabled yes 注意： 节点间使用 gossip 通信，规模\u003c1000 默认所有槽位可用，才提供服务 一般会配合主从模式使用 ","date":"2023-08-08","objectID":"/redis/:0:9","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"🌟Redis 单线程吗，为什么快？ Redis 网络模型 = IO 多路复用+事件派发（接收网络请求，Redis6 之后命令的转换采用多线程，执行采用单线程） 内存处理线程是单线程，纯内存操作，避免上下文切换，保证了高性能，也不会产生现成安全问题。 使用了 I/O 多路复用模型，是非阻塞 IO，利用单线程同时监听多个 socket，在某个 socket 可读、可写时得到通知，对就绪的 socket 调用 recvfrom，避免无效等待，充分利用 CPU。 select 和 poll 只通知有某个 socket 就绪（不知道是哪个），需要遍历确定具体的 socket，而 epoll 通知并把已就绪的 socket 写入用户空间。 ","date":"2023-08-08","objectID":"/redis/:0:10","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"Spring Cache 和 Redis 的区别？ Spring cache是代码级的缓存，一般是使用一个** ConcurrentMap**，也就是说实际上还是是使用JVM的内存来缓存对象的，这势必会造成大量的内存消耗。但好处是显然的：使用方便。 Redis 是内存级的缓存。它是使用单纯的内存来进行缓存。 集群环境下，每台服务器的spring cache是不同步的，这样会出问题的，spring cache只适合单机环境。 Redis是设置单独的缓存服务器，所有集群服务器统一访问redis，不会出现缓存不同步的情况。 ","date":"2023-08-08","objectID":"/redis/:0:11","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"Redis 的使用？ ❌Jedis：基于BIO，线程不安全，需要配置连接池管理连接 Lettuce：主流推荐的驱动，基于** Netty NIO**，API 线程安全 Redission：基于 Netty NIO，API 线程安全，大量丰富的分布式供能特性 JUC 的线程安全集合和工具的分布式版本 分布式的基本数据类型和锁 Redis如何设置密码和验证密码？ 设置密码：config set requirepass 123456 认证密码：auth 123456 Redis 事务 开启事务：multi 执行事务：exec 撤销事务：discard 实现乐观锁：watch Q：假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？ A：使用 keys 指令可以扫出指定模式的 key 列表。 Q2：如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？ A2：redis 内存处理是单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用** scan 指令**，scan 指令可以无阻塞的提取出指定模式的key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。 ","date":"2023-08-08","objectID":"/redis/:0:12","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["interview"],"content":"Pipeline有什么好处？ 将多次 IO 往返时间缩减为一次，前提是 pipeline 执行的指令之间没有因果相关性。 使用 redis-benchmark 进行压测的时候可以发现影响 redis 的 QPS 峰值的一个重要因素是 pipeline 批次指令的数目。 ","date":"2023-08-08","objectID":"/redis/:0:13","tags":["Redis"],"title":"🚩Redis","uri":"/redis/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-07","objectID":"/k8s_note/","tags":["k8s"],"title":"k8s 杂记","uri":"/k8s_note/"},{"categories":["draft"],"content":"基本概念 设计思想：从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。 Namespace 做隔离，容器的本质是进程 Cgroups 做限制 rootfs 做文件系统。 Pod Pod是一个逻辑概念，是 Kubernetes 项目中最基础的一个原子调度单位，Pod 里的容器共享同一个 Network Namespace、同一组数据卷 Volume，从而达到高效率交换信息的目的。 Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右 service：给 Pod 绑定一个 Service 服务，拥有不变的 IP 地址等信息，作为 pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。 这样，对于 Web 应用的 Pod 来说，它需要关心的就是数据库 Pod 的 Service 信息。不难想象，Service 后端真正代理的 Pod 的 IP 地址、端口等信息的自动更新、维护，则是 Kubernetes 项目的职责。 master 控制节点 apiserver：api服务，整个集群的持久化数据由其处理后保存的 etcd 中 controller manager：容器编排 scheduler：调度 node 计算节点 ","date":"2023-08-07","objectID":"/k8s_note/:0:1","tags":["k8s"],"title":"k8s 杂记","uri":"/k8s_note/"},{"categories":["draft"],"content":"kubeadm 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。 apt-get install kubeadm：安装 kubeadm、kubelet 和 kubectl 这三个二进制文件 kubeadm init 进行一系列的检查工作，以确定这台机器可以用来部署 Kubernetes 生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。 会为其他组件生成访问 kube-apiserver 所需的配置文件 会为 Master 组件生成 Pod 配置文件 kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。 在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 ConfigMap 的名字是 cluster-info。 就是安装默认插件： kube-proxy 和 DNS kubeadm join：，kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。 k8s 集群： 一个 Master 节点和多个 Worker 节点 Weave 容器网络插件 Rook 容器持久化存储插件 Dashboard 可视化的 Web 界面。 更新配置文件：kubectl apply -f nginx-deployment.yaml ","date":"2023-08-07","objectID":"/k8s_note/:0:2","tags":["k8s"],"title":"k8s 杂记","uri":"/k8s_note/"},{"categories":["draft"],"content":"Stateful 它把真实世界里的应用状态，抽象为了两种情况： 拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动，比如应用的主节点 A 要先于从节点 B 启动。而如果你把 A 和 B 两个 Pod 删除掉，它们再次被创建出来时也必须严格按照这个顺序才行。并且，新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。 存储状态。这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A 第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份，哪怕在此期间 Pod A 被重新创建过。这种情况最典型的例子，就是一个数据库应用的多个存储实例。 StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。 StatefulSet 其实就是一种特殊的Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重 要网络标识（即：在整个集群里唯一的、可被的访问身份）。有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。 ","date":"2023-08-07","objectID":"/k8s_note/:0:3","tags":["k8s"],"title":"k8s 杂记","uri":"/k8s_note/"},{"categories":["draft"],"content":"草稿太多，有空再整理吧。。","date":"2023-08-07","objectID":"/netty_note/","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"五种 IO 模型 通信模式：同步、异步 线程处理模式：阻塞、非阻塞 五种 IO 模型 BIO（阻塞式）：在while(true)中调用accept()方法等待客户端。一个线程只能同时处理一个连接请求 非阻塞式IO：内核会立即返回，已获得足够的CPU事件继续做其他事情，用户进程的第一阶段不阻塞，需要不断主动询问kernel数据是否准备好，第二阶段仍然阻塞。 多路复用IO/事件驱动IO：在单线程里同事监控多个套接字，通过select或poll轮询所负责的socket，当某个socket有数据到达了就通知用户进程。进程首先阻塞在select/poll上，再阻塞在读操作的第二阶段上。 信号驱动IO：在IO执行的数据准备阶段，不会阻塞用户进程，当用户进程收到信号后，才去查收数据。 异步IO：用户进程发出系统调用后立即返回，内核等待数据准备完成后，将数据拷贝到用户进程缓冲区，然后告诉用户进程IO操作执行完毕。 ","date":"2023-08-07","objectID":"/netty_note/:0:1","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"什么是 Netty？ Netty 是一个高性能的网络通信框架，基于 NIO 技术实现，广泛应用于互联网服务开发中，包括 Web 服务器、RPC、消息中间件等领域。 常规 IO NIO：少了一次缓冲区的复制 ","date":"2023-08-07","objectID":"/netty_note/:0:2","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"与 JDK 原生 NIO 相比，Netty 的优势在哪里？ Netty 相比于 JDK 原生 NIO 具有以下优势： 更好的性能：Netty 在 NIO 的基础上进行了优化，可以更好地利用系统资源，提高 I/O 处理的效率和吞吐量。 更易用的接口：Netty 提供了一套更加易用的 API 接口，简化了 NIO 的复杂性，使得开发者可以更加方便地使用。 更多的功能扩展：Netty 提供了丰富的扩展功能，如编解码器、心跳检测、SSL/TLS 加密等，可以帮助开发者更加快速地构建高效、安全的网络应用程序。 ","date":"2023-08-07","objectID":"/netty_note/:0:3","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"什么是 Channel 和 ChannelPipeline？ 在 Netty 中，Channel 是网络通信的基础，表示一个可靠的网络连接。ChannelPipeline 是一系列 ChannelHandler 的链表，用于处理 Channel 上的事件和数据，类似于 Servlet 中的 Filter。 ","date":"2023-08-07","objectID":"/netty_note/:0:4","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"什么是 EventLoop 和线程模型？ EventLoop 是 Netty 中的一个重要组件，用于处理 Channel 上的事件和任务。每个 Channel 都会绑定一个 EventLoop**，一个 EventLoop 可以处理多个 Channel 上的事件和任务**。Netty 采用了多种线程模型，如单线程模型、多线程模型、主从线程模型等，根据实际需求选择不同的线程模型可以更好地提高程序的性能和并发性。 EventLoop EventLoop定义了Netty的核心抽象，用于处理连接的生命周期中所发生的事件，负责监听网络事件并调用事件处理器进行相关I/O操作。 Channel 是 Netty 网络操作抽象类，EventLoop负责处理注册到其上的Channel 的 I/O操作。 EventloopGrup包含多个EventLoop（EventLoop：Thread = 1 : 1），它管理着所有的EventLoop的生命周期。 NioEventLoopGroup 默认的构造函数实际会起的线程数为 CPU核心数*2 核心概念 ServerBootstrap，服务器端程序的入口，这是 Netty 为简化网络程序配置和关闭等生命周期管理，所引入的 Bootstrapping 机制。我们通常要做的创建 Channel、绑定端口、注册 Handler 等，都可以通过这个统一的入口，以 Fluent API 等形式完成，相对简化了 API 使用。与之相对应， Bootstrap则是 Client 端的通常入口。 Channel：可以理解为读操作或写操作，因此可以被打开或被关闭 。作为一个基于 NIO 的扩展框架，Channel 和 Selector 等概念仍然是 Netty 的基础组件，但是针对应用开发具体需求，提供了相对易用的抽象。 EventLoop：只由一个线程驱动，处理一个 Channel中 的所有 I/O 事件。这是 Netty 处理事件的核心机制。例子中使用了 EventLoopGroup。我们在 NIO 中通常要做的几件事情，如注册感兴趣的事件、调度相应的 Handler 等，都是 EventLoop 负责。 ChannelFuture，这是 Netty 实现异步 IO 的基础之一，保证了同一个 Channel 操作的调用顺序。Netty 扩展了 Java 标准的 Future，提供了针对自己场景的特有Future定义。 ChannelHandler，这是应用开发者放置业务逻辑的主要地方，也是我上面提到的“Separation Of Concerns”原则的体现。每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法，实现业务逻辑与网络处理代码分离。 ChannelPipeline：它是 ChannelHandler 链条的容器，每个 Channel 在创建后，自动被分配一个 ChannelPipeline。 ","date":"2023-08-07","objectID":"/netty_note/:0:5","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"什么是 ByteBuf？ ByteBuf 是 Netty 中的一个重要组件，用于处理数据的缓冲区。与 JDK 中的 ByteBuffer 相比，ByteBuf 具有更好的可扩展性、更高的性能和更丰富的功能。ByteBuf 支持读写索引分离，内存池化和引用计数等功能，可以更好地管理内存和提高程序的性能。 ByteBuffer 的种类？ java.nio.HeapByteBuffer：java 堆内存，读写效率较低，受到 GC 的影响 java.nio.DirectByteBuffer：直接内存，读写效率高（少一次拷贝），不受 GC 影响，分配效率低 ","date":"2023-08-07","objectID":"/netty_note/:0:6","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"四种事件类型 accept：在有连接请求时触发 connect：客户端，连接建立后触发 read：可读事件 write：可写事件 在 select 后，事件要么处理 accept，要么取消 cancel（catch 异常之后）， ","date":"2023-08-07","objectID":"/netty_note/:0:7","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"如何处理粘包与拆包？ ByteToMessageDecoder 提供了一些常见的实现类： FixedLengthFrameDecoder：定长协议解码器，指定固定的字节数 LineBasedFrameDecoder：行分隔符解码器，识别 \\n或\\r\\n DelimiterBasedFrameDecoder：分隔符解码器 LengthFieldBasedFrameDecoder：长度编码解码器，将报文划分为header和body JsonObjectDecoder：监测到匹配的{}或[]就认为是完整的json对象或json数组 ","date":"2023-08-07","objectID":"/netty_note/:0:8","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"API 网关 API 网关的四大职能： 请求接入：作为所有API接口服务请求的接入点 业务聚合：作为所有后端业务服务的聚合点 中介策略：实现安全、验证、路由、过滤、流控等策略 统一管理：对所有API服务和策略进行统一管理 ","date":"2023-08-07","objectID":"/netty_note/:0:9","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["draft"],"content":"面试考点❓ Reactor 模式和 Netty 线程模型。 Pipelining、EventLoop 等部分的设计实现细节。 Netty 的内存管理机制、引用计数等特别手段。 有的时候面试官也喜欢对比 Java 标准 NIO API，例如，你是否知道 Java NIO 早期版本中的 Epoll空转问题，以及 Netty 的解决方式等。 ","date":"2023-08-07","objectID":"/netty_note/:0:10","tags":["Network"],"title":"Netty 杂记","uri":"/netty_note/"},{"categories":["ChatGPT"],"content":"在 gpt 面前就 stay foolish 吧","date":"2023-08-07","objectID":"/java_code_gpt/","tags":["Java"],"title":"ChatGPT 说 Java 源码应该这样读","uri":"/java_code_gpt/"},{"categories":["ChatGPT"],"content":"以下是 gpt3.5 推荐的 Java 源码掌握顺序（按照重要程度排序）： Java 基础类库（Java SE） - Java 基础类库是 Java 开发最基本的组成部分，包含了大量的核心类和接口，如集合框架、IO 操作、多线程、反射等。熟练掌握 Java 基础类库是入门 Java 开发的必要条件。 集合框架（Collection Framework） - 集合框架是 JavaSE 中最基本的组成部分之一，包含了众多的数据结构，如 List、Set、Map 等。熟练掌握集合框架的实现原理和使用方法，可以帮助我们更好地理解和应用集合框架。 并发编程（Concurrency） - 并发编程是 JavaSE 中一个重要的部分，涉及到线程、锁、并发集合等内容。熟练掌握并发编程可以帮助我们写出高效、可靠的多线程程序。 IO 操作（Input/Output） - IO 操作是 JavaSE 中常用的操作之一，包括文件读写、网络通信等。掌握 IO 操作的原理和实现可以帮助我们编写高效、可靠的 IO 程序。 反射（Reflection） - 反射是 JavaSE 中一个强大的特性，可以在程序运行时动态获取类的信息，创建对象、调用方法等。掌握反射的原理和实现可以帮助我们编写更加灵活的程序。 JVM（Java Virtual Machine） - JVM 是 JavaSE 中一个重要的组成部分，是 Java 语言的核心，也是 Java 程序运行的基础。掌握 JVM 的原理和实现可以帮助我们深入理解 Java 语言的特性和性能优化。 网络编程（Network Programming） - 网络编程是 JavaSE 中一个常用的部分，涉及到 Socket 编程、HTTP 编程等。掌握网络编程的原理和实现可以帮助我们编写高效、稳定的网络应用程序。 序列化（Serialization） - 序列化是 JavaSE 中一个重要的特性，可以将 Java 对象转换为字节流，便于网络传输或存储。掌握序列化的原理和实现可以帮助我们编写高效、可靠的序列化程序。 Java 虚拟机（JVM） - Java 虚拟机是 Java 语言的核心，也是 Java 程序运行的基础。掌握 Java 虚拟机的原理和实现可以帮助我们深入理解 Java 语言的特性和性能优化。 Java 并发编程（Java Concurrency） - Java 并发编程涉及到线程、锁、并发集合等内容，是 Java 程序开发中不可避免的一部分。掌握并发编程可以帮助我们写出高效、可靠的多线程程序。 Spring 框架 - Spring 框架是目前最流行的 Java 开发框架之一，包含了众多的模块，如 Spring Core、Spring MVC、Spring Boot 等。熟练掌握 Spring 框架可以提高我们的开发效率和代码质量。 Netty - Netty 是一个高性能的网络通信框架，广泛应用于互联网服务开发中。掌握 Netty 的原理和实现可以帮助我们编写高性能、稳定的网络应用程序。 Tomcat - Tomcat 是一个开源的 Web 服务器和 Servlet 容器，是 Java Web 应用程序开发的重要工具。掌握 Tomcat 的原理和实现可以帮助我们更好地理解 Web 应用程序的工作原理。 MySQL - MySQL 是目前最流行的关系型数据库之一，也是互联网应用程序开发中最常用的数据库之一。掌握 MySQL 的原理和实现可以帮助我们编写高效、安全的数据库应用程序。 通过按照以上顺序学习和掌握这些Java源码的知识，您将能够建立坚实的基础，并逐步扩展您的技能和理解，为Java编程提供更好的支持。 ","date":"2023-08-07","objectID":"/java_code_gpt/:0:0","tags":["Java"],"title":"ChatGPT 说 Java 源码应该这样读","uri":"/java_code_gpt/"},{"categories":["interview"],"content":"1、在 java 中守护线程和本地线程区别？ java 中的线程分为两种：守护线程（Daemon）和用户线程（User）。 任何线程都可以设置为守护线程和用户线程，通过方法 Thread.setDaemon(boolon)；true 则把该线程设置为守护线程，反之则为用户线程。Thread.setDaemon()必须在 Thread.start()之前调用，否则运行时会抛出异常。 两者的区别： 唯一的区别是判断虚拟机(JVM)何时离开，Daemon 是为其他线程提供服务，如果全部的 User Thread 已经撤离，Daemon 没有可服务的线程，JVM 撤离。也可以理解为守护线程是 JVM 自动创建的线程（但不一定），用户线程是程序创建的线程；比如 JVM 的垃圾回收线程是一个守护线程，当所有线程已经撤离，不再产生垃圾，守护线程自然就没事可干了，当垃圾回收线程是 Java 虚拟机上仅剩的线程时，Java 虚拟机会自动离开。 扩展：Thread Dump 打印出来的线程信息，含有 daemon 字样的线程即为守护 进程，可能会有：服务守护进程、编译守护进程、windows 下的监听 Ctrl+break的守护进程、Finalizer 守护进程、引用处理守护进程、GC 守护进程。 2、线程与进程的区别？ 进程是操作系统分配资源的最小单元，线程是操作系统调度的最小单元。 一个程序至少有一个进程,一个进程至少有一个线程。 3、什么是多线程中的上下文切换？ 多线程会共同使用一组计算机上的 CPU，而线程数大于给程序分配的 CPU 数量时，为了让各个线程都有执行的机会，就需要轮转使用 CPU。不同的线程切换使用 CPU发生的切换数据等就是上下文切换。 4、死锁与活锁的区别，死锁与饥饿的区别？ 死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。 产生死锁的必要条件： 互斥条件：所谓互斥就是进程在某一时间内独占资源。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的 “活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。 Java 中导致饥饿的原因： 高优先级线程吞噬所有的低优先级线程的 CPU 时间。 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。 线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。 5、Java 中用到的线程调度算法是什么？采用时间片轮转的方式。可以设置线程的优先级，会映射到下层的系统上面的优先级上，如非特别需要，尽量不要用，防止线程饥饿。 6、什么是线程组，为什么在 Java 中不推荐使用？ ThreadGroup 类，可以把线程归属到某一个线程组中，线程组中可以有线程对象，也可以有线程组，组中还可以有线程，这样的组织结构有点类似于树的形式。 为什么不推荐使用？因为使用有很多的安全隐患吧，没有具体追究，如果需要使用，推荐使用线程池。 7、为什么使用 Executor 框架？ 每次执行任务创建线程 new Thread()比较消耗性能，创建一个线程是比较耗时、耗资源的。 调用 new Thread()创建的线程缺乏管理，被称为野线程，而且可以无限制的创建，线程之间的相互竞争会导致过多占用系统资源而导致系统瘫痪，还有线程之间的频繁交替也会消耗很多系统资源。 接使用 new Thread() 启动的线程不利于扩展，比如定时执行、定期执行、定时定期执行、线程中断等都不便实现。 8、在 Java 中 Executor 和 Executors 的区别？ Executors 工具类的不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。 Executor 接口对象能执行我们的线程任务。 ExecutorService 接口继承了 Executor 接口并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。 使用 ThreadPoolExecutor 可以创建自定义线程池。 Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 get()方法获取计算的结果。 9、如何在 Windows 和 Linux 上查找哪个线程使用的 CPU 时间最长？ 10、什么是原子操作？在 Java Concurrency API 中有哪些原子类(atomic classes)？ 原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。 处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操 作。在 Java 中可以通过锁和循环 CAS 的方式来实现原子操作。 CAS 操作— —Compare \u0026 Set，或是 Compare \u0026 Swap，现在几乎所有的 CPU 指令都支持 CAS的原子操作。 原子操作是指一个不受其他操作影响的操作任务单元。原子操作是在多线程环境下避免数据不一致必须的手段。 int++并不是一个原子操作，所以当一个线程读取它的值并加 1 时，另外一个线程有可能会读到之前的值，这就会引发错误。 为了解决这个问题，必须保证增加操作是原子的，在 JDK1.5 之前我们可以使 用同步技术来做到这一点。到 JDK1.5，java.util.concurrent.atomic 包提供了 int 和long 类型的原子包装类，它们可以自动的保证对于他们的操作是原子的并且不需要使用同步。 java.util.concurrent 这个包里面提供了一组原子类。其基本的特性就是在多线程环境下，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到该方法执行完成，才由 JVM 从等待队列中选择一个另一个线程进入，这只是一种逻辑上的理解。 原子类：AtomicBoolean，AtomicInteger，AtomicLong，AtomicReference 原子数组：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray 原子属性更新器：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater， AtomicReferenceFieldUpdater 解决 ABA 问题的原子类：AtomicMarkableReference（通过引入一个 boolean 来反映中间有没有变过），AtomicStampedReference（通过引入一个 int 来累加来反映中间有没有变过） 11、Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？ Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。 他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。它的优势有：可以使锁更公平可以使线程在等待锁的时候响应中断可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间可以在不同的范围，以不同的顺序获取和释放锁 整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的(tryLock 方法)、定时的(tryLock 带参方法)、可中断的 (lockInterruptibly)、可多条件队列的(newCondition 方法)锁操作。另外 Lock 的实现类基本都支持非公平锁(默认)和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。 12、什么是 Executors 框架？ Executor 框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。 无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的 的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用 Executors 框架可以非常方便的创建一个线程池。 13、什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？ 阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。 这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。 当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 JDK7 提供了 7 个阻塞队列。分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 Java 5 之前实现同步存取时，可以使用普通的一个集合，然后在使用线程的协 作和线程同步可以实现生产者，消费者模式，主要的技术就是用好， wait ,notify,notifyAll,sychronized 这些关键字。而在 java 5 之后，可以使用阻塞队列来实现，此方式大大简少了代码量，使得多线程编程更加容易，安全方面也有保障。 BlockingQueue 接口是 Queue 的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性，当生产者线程试图向 BlockingQueue 放入元素时，如果队列已满，则线程被阻塞，当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞，正是因为它所具有这个特性，所以在程序中多个线程交替向 BlockingQueue 中放入元素，取出元素，它可以很好的控制线程之间的通信。 阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。 什么是 Callable 和 Future? Callable 接口类似于 Runnable，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说， Future 可以拿到异步执行任务的返回值。 可以认为是带有回调的 Runnable。 Future 接口表示异步任务，是还没有完成的任务给出的未来结果。所以说 Callable用于产生结果，Future 用于获取结果。 什么是 FutureTask?使用 ExecutorService 启动任务。 在 Java 并发程序中 FutureTask 表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是调用了 Runnable接口所以它可以提交给 Executor 来执行。 16、什么是并发容器的实现？ 何为同步容器：可以简单地理解为通过 synchronized 来实现同步的容器，如 果有多个线程调用同步容器的方法，它们将会串行执行。比如 Vector， Hashtable，以及 Collections.synchronizedSet，synchronizedList 等方法返回的容器。可以通过查看 Vector，Hashtable 等这些同步容器的实现代码，可以看到这些容器实现线程安全的方式就是将它们的状态封装起来，并在需要同步的方法上加上关键字 synchronized。 并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩 性，例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问 map，并且执行读操作的线程和写操作的线程也可以并发的访问 map，同时允许一定数量的写操作线程并发地修改 map，所以它可以在并发环境下实现更高的吞吐量。 17、多线程同步和互斥有几种实现方法，都是什么？ 线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。 线程间的同步方法大体可分为两类：用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式就是不需要切换到内核态，只在用户态完成操作。 用户模式下的方法有：原子操作（例如一个单一的全局变量），临界区。内核模式下的方法有：事件，信号量，互斥量。 18、什么是竞争条件？你怎样发现和解决竞争？ 当多个进程都企图对共享数据进行某种处理，而最后的结果又取决于进程运行的顺序时，则我们认为这发生了竞争条件（race condition）。 19、你将如何使用 thread dump？你将如何分析 Thread dump？ 新建状态（New）用 new 语句创建的线程处于新建状态，此时它和其他 Java 对象一样，仅仅在堆区中被分配了内存。就绪状态（Runnable） 当一个线程对象创建后，其他线程调用它的 start()方法，该线程就进入就绪状态，Java 虚拟机会为它创建方法调用栈和程序计数器。处于这个状态的线程位于可运行池中，等待获得 CPU 的使用权。 运行状态（Running） 处于这个状态的线程占用 CPU，执行程序代码。只有处于就绪状态的线程才有机会转到运行状态。 阻塞状态（Blocked） 阻塞状态是指线程因为某些原因放弃 CPU，暂时停止运行。当线程处于阻塞状态时，Java 虚拟机不会给线程分配 CPU。直到线程重新进入就绪状态，它才有机会转到运行状态。 阻塞状态可分为以下 3 种： 位于对象等待池中的阻塞状态（Blocked in object’s wait pool）： 当线程处于运行状态时，如果执行了某个对象的 wait()方法，Java 虚拟机就 会把线程放到这个对象的等待池中，这涉及到“线程通信”的内容。位于对象锁池中的阻塞状态（Blocked in object’s lock pool）： 当线程处于运行状态时，试图获得某个对象的同步锁时，如果该对象的同步锁已经被其他线程占用，Java 虚拟机就会把这个线程放到这个对象的锁池中，这涉及到“线程同步”的内容。 其他阻塞状态（Otherwise Blocked）： 当前线程执行了 sleep()方法，或者调用了其他线程的 join()方法，或者发出了 I/O请求时，就会进入这个状态。 死亡状态（Dead） 当线程退出 run()方法时，就进入死亡状态，该线程结束生命周期。 20、为什么我们调用 start()方法时会执行 run() 方法，为什么我们不能直接调用 run()方法？ 当你调用 start()方法时你将创建新的线程，并且执行在 run()方法里的代码。 但是如果你直接调用 run()方法，它不会创建新的线程也不会执行调用线程的代码，只会把 run 方法当作普通方法去执行。 21、Java 中你怎样唤醒一个阻塞的线程？ 在 Java 发展史上曾经使用 suspend()、resume()方法对于线程进行阻塞唤醒，但随之出现很多问题，比较典型的还是死锁问题。 解决方案可以使用以对象为目标的阻塞，即利用 Object 类的 wait()和 notify()方法实现线程阻塞。 首 先 ，wait、notify 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取改对象的锁，直到获取成功才能往下执行；其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。 22、在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？ CyclicBarrier 可以重复使用，而 CountdownLatch 不能重复使用。 Java 的 concurrent 包里面的 CountDownLatch 其实可以把它看作一个计数器，只不过这个计数器的操作是原子操作，同时只能有一个线程去操作这个计 数器，也就是同时只能有一个线程去减这个计数器里面的值。你可以向 CountDownLatch 对象设置一个初始的数字作为计数值，任何调用这个对象上的 await()方法都会阻塞，直到这个计数器的计数值被其他的线程减为 0 为止。所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 CountDownLatch 的一个非常典型的应用场景是：有一个任务想要往下执行，但必须要等到其他的任务执行完毕后才可以继续往下执行。假如我们这个想要继续往下执行的任务调用一个 CountDownLatch 对象的 await()方法，其他的任务执行完自己的任务后调用同一个 CountDownLatch 对象上的 countDown()方 法，这个调用 await()方法的任务将一直阻塞等待，直到这个 CountDownLatch 对象的计数值减到 0 为止。 CyclicBarrier 一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 23、什么是不可变对象，它对写并发应用有什么帮助？ 不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。 不可变对象的类即为不可变类(Immutable Class)。Java 平台类库中包含许多不可变类，如 String、基本类型的包装类、BigInteger 和 BigDecimal 等。不可变对象天生是线程安全的。它们的常量（域）是在构造函数中创建的。既然它们的状态无法修改，这些常量永远不会变。 不可变对象永远是线程安全的。 只有满足如下状态，一个对象才是不可变的； 它的状态不能在创建后再被修改；所有域都是 final 类型；并且，它被正确创建（创建期间没有发生 this 引用的逸出）。 24、什么是多线程中的上下文切换？ 在上下文切换过程中，CPU 会停止处理当前运行的程序，并保存当前程序运行的具体位置以便之后继续运行。从这个角度来看，上下文切换有点像我们同时阅读几本书，在来回切换书本的同时我们需要记住每本书当前读到的页码。在程序中，上下文切换过程中的“页码”信息是保存在进程控制块（PCB）中的。 PCB 还经常被称作“切换桢”（switchframe）。“页码”信息会一直保存到 CPU 的内存中，直到他们被再次使用。 上下文切换是存储和恢复 CPU 状态的过程，它使得线程执行能够从中断点恢复执行。上下文切换是多任务操作系统和多线程环境的基本特征。 25、Java 中用到的线程调度算法是什么？ 计算机通常只有一个 CPU,在任意时刻只能执行一条机器指令,每个线程只有获得CPU 的使用权才能执行指令.所谓多线程的并发运行,其实是指从宏观上看, 各个线程轮流获得 CPU 的使用权,分别执行各自的任务.在运行池中,会有多个处于就绪状态的线程在等待 CPU,JAVA 虚拟机的一项任务就是负责线程的调度, 线程调度是指按照特定机制为多个线程分配 CPU 的使用权. 有两种调度模型：分时调度模型和抢占式调度模型。 分时调度模型是指让所有的线程轮流获得 cpu 的使用权,并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。 Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用 CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用 CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。 26、什么是线程组，为什么在 Java 中不推荐使用？ 线程组和线程池是两个不同的概念，他们的作用完全不同，前者是为了方便线程的管理，后者是为了管理线程的生命周期，复用线程，减少创建销毁线程的开销。 27、为什么使用 Executor 框架比使用应用创建和管理线程好？ 为什么要使用 Executor 线程池框架 每次执行任务创建线程 new Thread()比较消耗性能，创建一个线程是比较耗时、耗资源的。 调用 new Thread()创建的线程缺乏管理，被称为野线程，而且可以无限制的创建，线程之间的相互竞争会导致过多占用系统资源而导致系统瘫痪，还有线程之间的频繁交替也会消耗很多系统资源。 直接使用 new Thread() 启动的线程不利于扩展，比如定时执行、定期执行、定时定期执行、线程中断等都不便实现。 使用 Executor 线程池框架的优点 能复用已存在并空闲的线程从而减少线程对象的创建从而减少了消亡线程的开销。 可有效控制最大并发线程数，提高系统资源使用率，同时避免过多资源竞争。 框架中已经有定时、定期、单线程、并发数控制等功能。 综上所述使用线程池框架 Executor 能更好的管理线程、提供系统资源使用率。 28、java 中有几种方法可以实现一个线程？ 继承 Thread 类实现 Runnable 接口 实现 Callable 接口，需要实现的是 call() 方法 29、如何停止一个正在运行的线程？ 使用共享变量的方式在这种方式中，之所以引入共享变量，是因为该变量可以被多个执行相同任务的线程用来作为是否中断的信号，通知中断线程的执行。 使用 interrupt 方法终止线程 如果一个线程由于等待某些事件的发生而被阻塞，又该怎样停止该线程呢？这种情况经常会发生，比如当一个线程由于需要等候键盘输入而被阻塞，或者调用Thread.join()方法，或者 Thread.sleep()方法，在网络中调用 ServerSocket.accept()方法，或者调用了 DatagramSocket.receive()方法 时，都有可能导致线程阻塞，使线程处于处于不可运行状态时，即使主程序中将该线程的共享变量设置为 true，但该线程此时根本无法检查循环标志，当然也就无法立即中断。这里我们给出的建议是，不要使用 stop()方法，而是使用 Thread 提供的interrupt()方法，因为该方法虽然不会中断一个正在运行的线程，但是它可以使一个被阻塞的线程抛出一个中断异常，从而使线程提前结束阻塞状态，退出堵塞代码。 30、notify()和 notifyAll()有什么区别？ 当一个线程进入 wait 之后，就必须等其他线程 notify/notifyall,使用 notifyall,可以唤醒所有处于 wait 状态的线程，使其重新进入锁的争夺队列中，而 notify 只能唤醒一个。 如果没把握，建议 notifyAll，防止 notigy 因为信号丢失而造成程序异常。 31、什么是 Daemon 线程？它有什么意义？ 所谓后台(daemon)线程，是指在程序运行的时候在后台提供一种通用服务的线程，并且这个线程并不属于程序中不可或缺的部分。因此，当所有的非后台线程结束时，程序也就终止了，同时会杀死进程中的所有后台线程。反过来说，只要有任何非后台线程还在运行，程序就不会终止。必须在线程启动之前调用 setDaemon()方法，才能把它设置为后台线程。注意：后台进程在不执行 finally子句的情况下就会终止其 run()方法。 比如：JVM 的垃圾回收线程就是 Daemon 线程，Finalizer 也是守护线程。 32、java 如何实现多线程之间的通讯和协作？ 中断 和 共享变量 33、什么是可重入锁（ReentrantLock）？ 举例来说明锁的可重入性 public class UnReentrant{ Lock lock = new Lock(); public void outer(){ lock.lock(); inner(); lock.unlock(); } public void inner(){ lock.lock(); //do something lock.unlock(); } }复制代码 outer 中调用了 inner，outer 先锁住了 lock，这样 inner 就不能再获取 lock。其实调用 outer 的线程已经获取了 lock 锁，但是不能在 inner 中重复利用已经获取的锁资源，这种锁即称之为 不可重入可重入就意味着：线程可以进入任何一个它已经拥有的锁所同步着的代码块。 synchronized、ReentrantLock 都是可重入的锁，可重入锁相对来说简化了并发编程的开发。 34、当一个线程进入某个对象的一个 synchronized 的实例方法后，其它线程是否可进入此对象的其它方法？ 如果其他方法没有 synchronized 的话，其他线程是可以进入的。 所以要开放一个线程安全的对象时，得保证每个方法都是线程安全的。 35、乐观锁和悲观锁的理解及如何实现，有哪些实现方式？ 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等， 读锁，写锁等，都是在做操作之前先上锁。再比如 Java 里面的同步原语 synchronized 关键字的实现也是悲观锁。 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 乐观锁的实现方式： 使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。 java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作中包含三个操作数 —— 需要读写的内存位置（V）、进行比较的预期原值 （A）和拟写入的新值(B)。如果内存位置 V 的值与预期原值 A 相匹配，那么 处理器会自动将该位置值更新为新值 B。否则处理器不做任何操作。 CAS 缺点： ABA 问题：比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操 作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。从 Java1.5 开始 JDK 的 atomic包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。 循环时间长开销大： 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 只能保证一个共享变量的原子操作：当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁。 36、SynchronizedMap 和 ConcurrentHashMap 有什么区别？ SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。 ConcurrentHashMap 使用分段锁来保证在多线程下的性能。 ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将hash 表分为 16 个桶，诸如 get,put,remove 等常用操作只锁当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有 16 个写线程执行，并发性能的提升是显而易见的。 另外 ConcurrentHashMap 使用了一种不同的迭代方式。在这种迭代方式中，当 iterator 被创建后集合再发生改变就不再是抛出 ConcurrentModificationException，取而代之的是在改变时 new 新的数据从而不影响原有的数据 ，iterator 完成后再将头指针替换为新的数据 ，这样 iterator线程可以使用原来老的数据，而写线程也可以并发的完成改变。 37、CopyOnWriteArrayList 可以用于什么应用场景？ CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出 ConcurrentModificationException。在 CopyOnWriteArrayList 中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 young gc 或者 full gc； 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set操作后，读取到数据可能还是旧的,虽然 CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求； CopyOnWriteArrayList 透露的思想 读写分离，读和写分开 最终一致性 使用另外开辟空间的思路，来解决并发冲突 38、什么叫线程安全？servlet 是线程安全吗? 线程安全是编程中的术语，指某个函数、函数库在多线程环境中被调用时，能 够正确地处理多个线程之间的共享变量，使程序功能正确完成。 Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。 Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。 SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。 Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。 39、volatile 有什么用？能否用一句话说明下 volatile 的应用场景？ volatile 保证内存可见性和禁止指令重排。 volatile 用于多线程环境下的单次操作(单次读或者单次写)。 40、为什么代码会重排序？ 在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但 是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：在单线程环境下不能改变程序运行的结果；存在数据依赖关系的不允许重排序需要注意的是：重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。 41、在 java 中 wait 和 sleep 方法的不同？ 最大的不同是在等待时 wait 会释放锁，而 sleep 一直持有锁。Wait 通常被用于线程间交互，sleep 通常被用于暂停执行。 42、用 Java 实现阻塞队列 43、一个线程运行时发生异常会怎样？ 如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler 是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候 JVM 会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给handler 的 uncaughtException()方法进行处理。 44、如何在两个线程间共享数据？ 在两个线程间共享变量即可实现共享。 一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。 45、Java 中 notify 和 notifyAll 有什么区别？ notify() 方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而 notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 46、为什么 wait, notify 和 notifyAll 这些方法不在 thread类里面？ 一个很明显的原因是 JAVA 提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。由于 wait，notify 和 notifyAll 都是锁级别的操作，所以把他们定义在 Object 类中因为锁属于对象。 ","date":"2023-08-07","objectID":"/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98/:0:0","tags":["draft"],"title":"多线程面试题","uri":"/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["interview"],"content":"47、什么是 ThreadLocal 变量？ ThreadLocal 是 Java 里一种特殊的变量。每个线程都有一个 ThreadLocal 就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用 ThreadLocal 让 SimpleDateFormat 变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。 48、Java 中 interrupted 和 isInterrupted 方法的区别？ interrupt interrupt 方法用于中断线程。调用该方法的线程的状态为将被置为”中断” 状态。 注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出 interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。 interrupted 查询当前线程的中断状态，并且清除原状态。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。 isInterrupted 仅仅是查询当前线程的中断状态 49、为什么 wait 和 notify 方法要在同步块中调用？ Java API 强制要求这样做，如果你不这么做，你的代码会抛出 IllegalMonitorStateException 异常。还有一个原因是为了避免 wait 和 notify之间产生竞态条件。 50、为什么你应该在循环中检查等待条件? 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。 51、Java 中的同步集合与并发集合有什么区别？ 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线 程并发的时候会导致争用，阻碍了系统的扩展性。Java5 介绍了并发集合像 ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。 52、什么是线程池？ 为什么要使用它？ 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的 时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线 程。从JDK1.5 开始，Java API 提供了 Executor 框架让你可以创建不同的线程池。 53、怎么检测一个线程是否拥有锁？ 在 java.lang.Thread 中有一个方法叫 holdsLock()，它返回 true 如果当且仅当当前线程拥有某个具体对象的锁。 54、你如何在 Java 中获取线程堆栈？ kill -3 [java pid] 不会在当前终端输出，它会输出到代码执行的或指定的地方去。比如，kill -3 tomcat pid, 输出堆栈到 log 目录下。 Jstack [java pid] 这个比较简单，在当前终端显示，也可以重定向到指定文件中。 -JvisualVM：Thread Dump 不做说明，打开 JvisualVM 后，都是界面操作，过程还是很简单的。 55、JVM 中哪个参数是用来控制线程的栈堆栈小的? -Xss 每个线程的栈大小 56、Thread 类中的 yield 方法有什么作用？ 使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。 当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。 57、Java 中 ConcurrentHashMap 的并发度是什么？ ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 在 JDK8 后，它摒弃了 Segment（锁段）的概念，而是启用了一种全新的方式实现,利用 CAS 算法。同时加入了更多的辅助变量来提高并发度，具体内容还是查看源码吧。 58、Java 中 Semaphore 是什么？ Java 中的 Semaphore 是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore 只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 59、Java 线程池中 submit() 和 execute()方法有什么区别？ 两个方法都可以向线程池提交任务，execute()方法的返回类型是 void，它定义在Executor 接口中。 而 submit()方法可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。 60、什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 61、Java 中的 ReadWriteLock 是什么？ 读写锁是用来提升并发程序性能的锁分离技术的成果。 62、volatile 变量和 atomic 变量有什么不同？ Volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量那么 count++ 操作就不是原子性的。 而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如 getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 63、可以直接调用 Thread 类的 run ()方法么？ 当然可以。但是如果我们调用了 Thread 的 run()方法，它的行为就会和普通的方法一样，会在当前线程中执行。为了在新的线程中执行我们的代码，必须使用Thread.start()方法。 64、如何让正在运行的线程暂停一段时间？ 我们可以使用 Thread 类的 Sleep()方法让线程暂停一段时间。需要注意的 是，这并不会让线程终止，一旦从休眠中唤醒线程，线程的状态将会被改变为 Runnable，并且根据线程调度，它将得到执行。 65、你对线程优先级的理解是什么？ 每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。 java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。 66、什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？ 线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。同上一个问题，线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制 它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。 时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU时间可以基于线程优先级或者线程等待的时间。 67、你如何确保 main()方法所在的线程是 Java 程序最后结束的线程？ 我们可以使用 Thread 类的 join()方法来确保所有程序创建的线程在 main() 方法退出前结束。 68、线程之间是如何通信的？ 当线程间是可以共享资源时，线程间通信是协调它们的重要的手段。Object 类中wait()\\notify()\\notifyAll()方法可以用于线程间通信关于资源的锁的状态。 69、为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？ Java 的每个对象中都有一个锁(monitor，也可以成为监视器) 并且 wait()， notify()等方法用于等待对象的锁或者通知其他线程对象的监视器可用。在 Java 的线程中并没有可供任何对象使用的锁和同步器。这就是为什么这些方法是 Object 类的一部分，这样 Java 的每一个类都有用于线程间通信的基本方法。 70、为什么 wait(), notify()和 notifyAll ()必须在同步方法或者同步块中被调用？ 当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。 71、为什么 Thread 类的 sleep()和 yield ()方法是静态的？ Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 72、如何确保线程安全？ 在 Java 中可以有很多方法来保证线程安全——同步，使用原子类(atomic concurrent classes)，实现并发锁，使用 volatile 关键字，使用不变类和线程安全类。 73、同步方法和同步块，哪个是更好的选择？ 同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。 同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。 74、如何创建守护线程？ 使用 Thread 类的 setDaemon(true)方法可以将线程设置为守护线程，需要注 意的是，需要在调用 start()方法前调用这个方法，否则会抛出 IllegalThreadStateException 异常。 75、什么是 Java Timer 类？如何创建一个有特定时间间隔的任务？ java.util.Timer 是一个工具类，可以用于安排一个线程在未来的某个特定时间执行。Timer 类可以用安排一次性任务或者周期任务。 java.util.TimerTask 是一个实现了 Runnable 接口的抽象类，我们需要去继承这个类来创建我们自己的定时任务并使用 Timer 去安排它的执行。 作者：程序员追风 链接：https://juejin.cn/post/6844904063687983111 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ","date":"2023-08-07","objectID":"/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98/:0:1","tags":["draft"],"title":"多线程面试题","uri":"/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["建站记录"],"content":"前前后后花了整整一周","date":"2023-08-06","objectID":"/blog_init/","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"选型 Jekyll：基于 Ruby，需要前端技能 Hexo：基于 Node.js ，主题好看，渲染慢 Hugo：基于 Go，速度快（一秒渲染 5000 篇文章） ☑️ JPress：商用比较多，付费 WordPress：基于 PHP，社区活跃，界面过于简约 Nest.js：技术较新，但生态不够完善 ","date":"2023-08-06","objectID":"/blog_init/:0:1","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"安装 # 下载 brew update brew install hugo # 验证 hugo version # 创建网页 hugo new site leni.fun # 主题 cd leni.fun git init -b main git submodule add https://github.com/hugo-fixit/FixIt.git themes/FixIt # 新建文章，注意要放在 posts 目录下（这是FixIt默认的，或配置其他目录名称） hugo new posts/xxx.md # 修改markdown 文件的 draft=false 和内容后，部署到本地 # --disableFastRender 即时渲染 # -D 草稿（draft=true）也部署 hugo server --disableFastRender ","date":"2023-08-06","objectID":"/blog_init/:0:2","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"关联 github page 创建 \u003cusername\u003e.github.io 仓库 # cd 到站点根目录,生成 public 文件夹 alias hugogo='hugo -F --cleanDestinationDir' # 以后每次就用它了 hugogo git remote add gitee git@gitee.com:dujianghui/blog.git git remote add github git@github.com:hihihiman/blog.git # 校验 git remote -v # push git commit -am \"init\" git push -u gitee master git push -u github main ","date":"2023-08-06","objectID":"/blog_init/:0:3","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"参考配置 baseURL: https://hihihiman.github.io languageCode: zh-CN title: 都将会的博客 theme: FixIt author: name: 都将会 params: author: 都将会 subtitle: 专注于 Java、MySQL、微服务等后端技术 keywords: Java,MySQL,微服务,都将会 description: 思绪来去如风，但愿有所停留。 ","date":"2023-08-06","objectID":"/blog_init/:0:4","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"tips https://transform.tools/toml-to-yaml 可以将 tomb 文件转换为 yml 文件，方便阅读和修改。 发文章：hugo new posts/xxx.md + 编辑 本地测试： hugogo +hugo serve -D，查看 localhost:1313 部署到Page：cd public + git commit -am \"message\"+git push gitee 默认主分支是 master，而 github 默认主分支是 main，需要注意 ","date":"2023-08-06","objectID":"/blog_init/:0:5","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":["建站记录"],"content":"进阶指南 https://www.wingoftime.cn/p/setup-blog-second/ 帮助你实现上传分支源码后自动部署到 Github Page ！ ","date":"2023-08-06","objectID":"/blog_init/:0:6","tags":["hugo"],"title":"初始化 Hogo 博客","uri":"/blog_init/"},{"categories":null,"content":"离线 - ","date":"0001-01-01","objectID":"/offline/","tags":null,"title":"","uri":"/offline/"}]